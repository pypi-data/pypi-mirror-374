"""This module provides the functions used to extract the behavior data from the .npz log archives generated by the
microcontrollers (hardware modules) used in the Sun lab."""

from typing import Any, TypedDict
from pathlib import Path
from collections.abc import Callable
from multiprocessing import cpu_count
from concurrent.futures import ProcessPoolExecutor, as_completed

from tqdm import tqdm
from numba import njit, prange  # type: ignore
import numpy as np
import polars as pl
from numpy.typing import NDArray
from sl_shared_assets import (
    SessionData,
    SessionLock,
    SessionTypes,
    TrackerFileNames,
    ProcessingTracker,
    AcquisitionSystems,
    MesoscopeHardwareState,
    generate_project_manifest,
)
from ataraxis_base_utilities import LogLevel, console, chunk_iterable
from ataraxis_communication_interface import ExtractedModuleData
from ataraxis_communication_interface.communication import SerialProtocols, SerialPrototypes

# Defines session types and acquisition systems that support extracting microcontroller data.
_supported_systems = {AcquisitionSystems.MESOSCOPE_VR}
_supported_sessions = {SessionTypes.LICK_TRAINING, SessionTypes.RUN_TRAINING, SessionTypes.MESOSCOPE_EXPERIMENT}


class _ParseTask(TypedDict):
    """An internal typing class used to enforce static typing while supporting processing microcontroller data in
    parallel."""

    func: Callable[..., None]
    output: Path
    kwargs: dict[str, Any]


def _process_module_message_batch(
    log_path: Path,
    file_names: list[str],
    onset_us: np.uint64,
    module_type_id: tuple[tuple[int, int], ...],
) -> dict[tuple[int, int], list[dict[str, Any]]]:
    """Processes the target batch of MicroControllerInterface-generated messages stored in the .npz log file.

    This worker function is used by the extract_logged_hardware_module_data() function to process multiple message
    batches in parallel to speed up the overall hardware module data extraction.

    Args:
        log_path: The path to the processed log file.
        file_names: The names of the individual message .npy files stored in the target archive.
        onset_us: The onset of the data acquisition, in microseconds elapsed since UTC epoch onset.
        module_type_id: The module type and ID codes to extract.

    Returns:
        A dictionary mapping module (type, id) tuples to lists of extracted data dictionaries. Each data dictionary
        contains 'event', 'timestamp', 'data', and 'command' keys.
    """
    # Precreates the dictionary to store the extracted data for this batch
    batch_data: dict[tuple[int, int], list[dict[str, Any]]] = {module: [] for module in module_type_id}

    # Opens the processed log archive using memory mapping. If module data processing is performed in parallel, all
    # processes interact with the archive concurrently.
    with np.load(log_path, allow_pickle=False, fix_imports=False, mmap_mode="r") as archive:
        # Loops over the batch of messages and extracts module data
        for item in file_names:
            message = archive[item]

            # Extracts the payload from each logged message.
            payload = message[9:]

            # Filters out the messages to exclusively process custom Data and State messages (event codes 51 and above).
            # The only exception to this rule is the CommandComplete state message, which uses the system-reserved code
            # '2'. In the future, if enough interest is shown, this list may be extended to also include outgoing
            # messages. For now, these messages need to be parsed manually by users that need this data.
            if (payload[0] != SerialProtocols.MODULE_STATE and payload[0] != SerialProtocols.MODULE_DATA) or (
                payload[4] != 2 and payload[4] < 51
            ):
                continue

            # Checks if this message comes from one of the processed modules
            current_module = None
            for module in module_type_id:
                if payload[1] == module[0] and payload[2] == module[1]:
                    current_module = module
                    break

            if current_module is None:
                continue

            # Extracts the elapsed microseconds since timestamp and uses it to calculate the global timestamp for the
            # message, in microseconds since epoch onset.
            elapsed_microseconds = np.uint64(message[1:9].view(np.uint64).item())
            timestamp = onset_us + elapsed_microseconds

            # Extracts command, event, and, if supported, data object from the message payload.
            command_code = np.uint8(payload[3])
            event = np.uint8(payload[4])

            # This section is executed only if the parsed payload is MessageData. MessageState payloads are only 5 bytes
            # in size. Extracts and formats the data object, included with the logged payload.
            data: Any = None
            if len(payload) > 5:
                # noinspection PyTypeChecker
                prototype = SerialPrototypes.get_prototype_for_code(code=payload[5])

                # Depending on the prototype, reads the data object as an array or scalar
                if isinstance(prototype, np.ndarray):
                    data = payload[6:].view(prototype.dtype)[:].copy()
                elif prototype is not None:
                    data = payload[6:].view(prototype.dtype)[0].copy()
                else:
                    data = None  # Marks as an error case

            # Adds the extracted data to the batch results
            batch_data[current_module].append(
                {"event": event, "timestamp": timestamp, "data": data, "command": command_code}
            )

    return batch_data


def _extract_logged_hardware_module_data(
    log_path: Path,
    module_type_id: tuple[tuple[int, int], ...],
    n_workers: int = -1,
) -> tuple[ExtractedModuleData, ...]:
    """Extracts the data for the requested hardware module instances running on an Ataraxis Micro Controller (AMC)
    device from the .npz log file generated by a DataLogger instance during runtime.

    This worker function was copied from the ataraxis-communication-interface library and optimized to use
    multiprocessing to achieve a measurable speedup while processing large log files.

    Notes:
        If the target .npz archive contains fewer than 2000 messages, the processing is carried out sequentially
        regardless of the specified worker-count.

    Args:
        log_path: The path to the .npz archive file that stores the logged data generated by the
            MicroControllerInterface instance during runtime.
        module_type_id: A tuple of tuples, where each inner tuple stores the type and ID codes of a specific hardware
            module, whose data should be extracted from the archive, e.g.: ((3, 1)).
        n_workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            below 1 uses all available CPU cores. Setting this to a value of 1 conducts the processing sequentially.

    Returns:
        A tuple of ExtractedModuleData instances. Each instance stores all data extracted from the log archive for one
        specific hardware module instance.

    Raises:
        ValueError: If the input path is not valid or does not point to an existing .npz archive. If the function is
            unable to properly extract a logged data object for the target hardware module.
    """
    # If the specified compressed log archive does not exist, raises an error
    if not log_path.exists() or log_path.suffix != ".npz" or not log_path.is_file():
        error_message = (
            f"Unable to extract hardware module data from the log file {log_path}, as it does not exist or does "
            f"not point to a valid .npz archive."
        )
        console.error(message=error_message, error=ValueError)

    # Memory-maps the processed archive to conserve RAM. The first processing pass is designed to find the onset
    # timestamp value and count the total number of messages.
    with np.load(log_path, allow_pickle=False, fix_imports=False, mmap_mode="r") as archive:
        # Locates the logging onset timestamp. The onset is used to convert the timestamps for logged module data into
        # absolute UTC timestamps. Originally, all timestamps other than onset are stored as elapsed time in
        # microseconds relative to the onset timestamp.
        timestamp_offset = 0
        onset_us = np.uint64(0)
        message_list = list(archive.files)

        for number, item in enumerate(message_list):
            message: NDArray[np.uint8] = archive[item]  # Extracts message payload from the compressed .npy file

            # Recovers the uint64 timestamp value from each message. The timestamp occupies 8 bytes of each logged
            # message starting at index 1. If the timestamp value is 0, the message contains the onset timestamp value
            # stored as an 8-byte payload. Index 0 stores the source ID (uint8 value)
            timestamp_value = message[1:9].view(np.uint64).item()
            if timestamp_value == 0:
                # Extracts the byte-serialized UTC timestamp stored as microseconds since epoch onset.
                onset_us = np.uint64(message[9:].view(np.int64).item())

                # Breaks the loop once the onset is found. Generally, the onset is expected to be found very early into
                # the loop
                timestamp_offset = number  # Records the item number at which the onset value was found.
                break

    # Builds the list of files to process after discovering the timestamp (the list of remaining messages)
    messages_to_process = message_list[timestamp_offset + 1 :]

    # If there are no leftover messages to process, return an empty tuple
    if not messages_to_process:
        return tuple()

    # Small archives are processed sequentially to avoid the unnecessary overhead of setting up the multiprocessing
    # runtime. This is also done for large files if the user explicitly requests to use a single worker process.
    if n_workers == 1 or len(messages_to_process) < 2000:
        # Processes all messages in a single batch sequentially
        batch_results = _process_module_message_batch(log_path, messages_to_process, onset_us, module_type_id)

        # Converts the batch results into the expected format
        module_event_data: dict[tuple[int, int], dict[Any, Any]] = {module: {} for module in module_type_id}

        for module, data_list in batch_results.items():
            for data_item in data_list:
                event = data_item["event"]
                # Iteratively fills the dictionary with extracted data. Uses event byte-codes as keys. For each event
                # code, creates a list of tuples. Each tuple inside the list contains the timestamp, data object
                # (or None) and the active command code.
                if event not in module_event_data[module]:
                    module_event_data[module][event] = [
                        {
                            "timestamp": data_item["timestamp"],
                            "data": data_item["data"],
                            "command": data_item["command"],
                        }
                    ]
                else:
                    module_event_data[module][event].append(
                        {
                            "timestamp": data_item["timestamp"],
                            "data": data_item["data"],
                            "command": data_item["command"],
                        }
                    )
    else:
        # If the user enabled using all available cores, configures the runtime to use all available CPUs
        if n_workers < 0:
            n_workers = cpu_count()

        # Creates batches of messages to process during runtime. Uses a fairly high batch multiplier to create many
        # smaller batches, which leads to a measurable increase in the processing speed, especially for large archives.
        # The optimal multiplier value (4) was determined experimentally.
        batches = []
        batch_indices = []  # Keeps track of batch order
        for i, batch in enumerate(chunk_iterable(messages_to_process, n_workers * 4)):
            if batch:
                batches.append((log_path, list(batch), onset_us, module_type_id))
                batch_indices.append(i)

        # Processes batches using ProcessPoolExecutor
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            # Submits all tasks
            future_to_index = {
                executor.submit(_process_module_message_batch, *batch_args): idx
                for idx, batch_args in zip(batch_indices, batches)
            }

            # Collects results while maintaining message order. This also propagates processing errors to the caller
            # process.
            results: list[dict[tuple[int, int], list[dict[str, Any]]] | None] = [None] * len(batches)

            # Creates a progress bar for batch processing
            with tqdm(total=len(batches), desc="Extracting microcontroller hardware module data", unit="batch") as pbar:
                for future in as_completed(future_to_index):
                    results[future_to_index[future]] = future.result()
                    pbar.update(1)  # Updates the progress bar after each batch completes

        # Combines processing results from all batches
        module_event_data = {module: {} for module in module_type_id}

        # Processes results from each batch to maintain chronological ordering
        for batch_result in results:
            if batch_result is not None:  # Skips None results
                for module, data_list in batch_result.items():
                    for data_item in data_list:
                        event = data_item["event"]
                        # Iteratively fills the dictionary with extracted data. Uses event byte-codes as keys. For each
                        # event code, creates a list of tuples. Each tuple inside the list contains the timestamp, data
                        # object (or None) and the active command code.
                        if event not in module_event_data[module]:
                            module_event_data[module][event] = [
                                {
                                    "timestamp": data_item["timestamp"],
                                    "data": data_item["data"],
                                    "command": data_item["command"],
                                }
                            ]
                        else:
                            module_event_data[module][event].append(
                                {
                                    "timestamp": data_item["timestamp"],
                                    "data": data_item["data"],
                                    "command": data_item["command"],
                                }
                            )

    # Creates ExtractedModuleData instances for each module and returns the tuple of created instances to caller
    result = tuple(
        ExtractedModuleData(module_type=module[0], module_id=module[1], data=module_event_data[module])
        for module in module_type_id
        if module_event_data[module]  # Only includes modules that have data
    )

    return result


def _interpolate_data(
    timestamps: NDArray[np.uint64],
    data: NDArray[Any],
    seed_timestamps: NDArray[np.uint64],
    is_discrete: bool,
) -> NDArray[Any]:
    """Interpolates data values for the provided seed timestamps.

    Primarily, this service function is used to time-align different datastreams from the same source.

    Notes:
        This function expects seed_timestamps and timestamps arrays to be monotonically increasing.

        Discrete interpolated data is returned as an array with the same datatype as the input data. Continuous
        interpolated data is always returned as float_64 datatype.

        This function is specifically designed to work with Sun lab time data, which uses the unsigned integer format.

    Args:
        timestamps: The one-dimensional numpy array that stores the timestamps for the source data.
        data: The one-dimensional numpy array that stores the source datapoints.
        seed_timestamps: The one-dimensional numpy array that stores the timestamps for which to interpolate the data
            values.
        is_discrete: A boolean flag that determines whether the data is discrete or continuous.

    Returns:
        A numpy array with the same dimension as the seed_timestamps array that stores the interpolated data values.
    """
    # Discrete data
    if is_discrete:
        # Preallocates the output array
        interpolated_data = np.empty(seed_timestamps.shape, dtype=data.dtype)

        # Handles boundary conditions in bulk using boolean masks. All seed timestamps below the minimum source
        # timestamp are statically set to data[0], and all seed timestamps above the maximum source timestamp are set
        # to data[-1].
        below_min = seed_timestamps < timestamps[0]
        above_max = seed_timestamps > timestamps[-1]
        within_bounds = ~(below_min | above_max)  # The portion of the seed that is within the source timestamp boundary

        # Assigns out-of-bounds values in-bulk
        interpolated_data[below_min] = data[0]
        interpolated_data[above_max] = data[-1]

        # Processes within-boundary timestamps by finding the last known certain value to the left of each seed
        # timestamp and setting each seed timestamp to that value.
        if np.any(within_bounds):
            indices = np.searchsorted(timestamps, seed_timestamps[within_bounds], side="right") - 1
            interpolated_data[within_bounds] = data[indices]

        return interpolated_data

    # Continuous data. Note, due to interpolation, continuous data is always returned using float_64 datatype.
    else:
        return np.interp(seed_timestamps, timestamps, data)  # type: ignore


def _parse_encoder_data(
    extracted_module_data: ExtractedModuleData, output_file: Path, cm_per_pulse: np.float64
) -> None:
    """Extracts and saves the data acquired by the EncoderModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        cm_per_pulse: The conversion factor to translate raw encoder pulses into distance in centimeters.
    """
    log_data = extracted_module_data.data

    # Looks for event-codes 51 (CCW displacement) and event-codes 52 (CW displacement).

    # Gets the data, defaulting to an empty list if the data is missing
    ccw_data = log_data.get(np.uint8(51), [])
    cw_data = log_data.get(np.uint8(52), [])

    # The way EncoderModule is implemented guarantees there is at least one CW code message with the displacement
    # of 0 that is received by the PC. In the worst case scenario, there will be no CCW codes and the parsing will
    # not work. To avoid that issue, generates an artificial zero-code CCW value at the same timestamp + 1
    # microsecond as the original CW zero-code value. This does not affect the accuracy of our data, just makes the
    # code work for edge-cases.
    if not ccw_data:
        first_timestamp = cw_data[0]["timestamp"]
        ccw_data = [{"timestamp": first_timestamp + 1, "data": 0}]
    elif not cw_data:
        first_timestamp = ccw_data[0]["timestamp"]
        cw_data = [{"timestamp": first_timestamp + 1, "data": 0}]

    # Precreates the output arrays, based on the number of recorded CW and CCW displacements.
    n_ccw = len(ccw_data)
    n_cw = len(cw_data)
    total_length = n_ccw + n_cw
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    displacements: NDArray[Any] = np.empty(total_length, dtype=np.float64)

    # Processes CCW rotations (Code 51). CCW rotation is interpreted as positive displacement
    timestamps[:n_ccw] = np.array([v["timestamp"] for v in ccw_data], dtype=np.uint64)
    displacements[:n_ccw] = np.array([v["data"] for v in ccw_data], dtype=np.float64)

    # Processes CW rotations (Code 52). CW rotation is interpreted as negative displacement
    timestamps[n_ccw:] = np.array([v["timestamp"] for v in cw_data], dtype=np.uint64)
    displacements[n_ccw:] = -np.array([v["data"] for v in cw_data], dtype=np.float64)

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    displacements = displacements[sort_indices]

    # Converts individual displacement vectors into aggregated absolute position of the animal in the task environment.
    # The position is also translated from encoder pulse counts into centimeters. The position is referenced to the
    # start of the experimental trial (beginning of the VR track) as 0-value. Positive positions mean moving forward
    # along the track, negative positions mean moving backward along the track.
    # noinspection PyTypeChecker
    positions = np.cumsum(displacements * cm_per_pulse)
    positions = np.round(positions, decimals=8)

    # Replaces -0.0 values with 0.0. This is a convenience adjustment to improve the visual appearance of the data.
    positions[np.isclose(positions, -0.0) & np.signbit(positions)] = 0.0

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "traveled_distance_cm": positions,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_ttl_data(extracted_module_data: ExtractedModuleData, output_file: Path) -> None:
    """Extracts and saves the data acquired by the TTLModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
    """
    log_data = extracted_module_data.data

    # Looks for event-codes 52 (InputON) and event-codes 53 (InputOFF).

    # Gets the data for both message types. The way the module is written guarantees that the PC receives code 53
    # at least once. No such guarantee is made for code 52, however.
    on_data = log_data.get(np.uint8(52), [])
    off_data = log_data.get(np.uint8(53), [])

    # Since this function ultimately looks for rising edges, it will not find any unless there is at least one ON and
    # one OFF message. Therefore, if any of the codes is actually missing, aborts data extraction early.
    if len(on_data) == 0 or len(off_data) == 0:
        return

    # Precreates the storage numpy arrays for both message types. Timestamps use uint64 datatype, and the trigger
    # values are boolean.
    n_on = len(on_data)
    n_off = len(off_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    triggers: NDArray[np.uint8] = np.empty(total_length, dtype=np.uint8)

    # Extracts ON (Code 52) trigger codes. Statically assigns the value '1' to denote ON signals.
    timestamps[:n_on] = np.array([v["timestamp"] for v in on_data], dtype=np.uint64)
    triggers[:n_on] = 1  # All ON signals

    # Extracts OFF (Code 53) trigger codes.
    timestamps[n_on:] = np.array([v["timestamp"] for v in off_data], dtype=np.uint64)
    triggers[n_on:] = 0  # All OFF signals

    # Sorts both arrays based on the timestamps, so that the data is in the chronological order.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    triggers = triggers[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if triggers[-1] != 0:
        timestamps = np.append(timestamps, timestamps[-1] + 1)
        triggers = np.append(triggers, 0)

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "ttl_state": triggers,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_break_data(
    extracted_module_data: ExtractedModuleData,
    output_file: Path,
    maximum_break_strength: np.float64,
    minimum_break_strength: np.float64,
) -> None:
    """Extracts and saves the data acquired by the BreakModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        maximum_break_strength: The maximum torque of the break in Newton centimeters.
        minimum_break_strength: The minimum torque of the break in Newton centimeters.

    Notes:
        This method assumes that the break was used in the absolute force mode. Currently, it does not support
        extracting variable break power data.
    """
    log_data = extracted_module_data.data

    # This function looks for event-codes 52 (Engaged) and event-codes 53 (Disengaged) as, currently, no experiment
    # requires variable breaking power. In the future, to add support for parsing variable breaking power, this function
    # needs to be expanded to parse code 54 events.

    # Gets the data, defaulting to an empty list if the data is missing
    engaged_data = log_data.get(np.uint8(52), [])
    disengaged_data = log_data.get(np.uint8(53), [])

    # Precreates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although trigger
    # values are boolean, they are translated into the actual torque applied by the break in Newton centimeters and
    # stored as float64 values.
    n_engaged = len(engaged_data)
    n_disengaged = len(disengaged_data)
    total_length = n_engaged + n_disengaged
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    torques: NDArray[np.float64] = np.empty(total_length, dtype=np.float64)

    # Processes Engaged (code 52) triggers. When the motor is engaged, it applies the maximum possible torque to
    # the break.
    timestamps[:n_engaged] = np.array([v["timestamp"] for v in engaged_data], dtype=np.uint64)
    torques[:n_engaged] = maximum_break_strength  # Broadcasting scalar value

    # Processes Disengaged (code 53) triggers. Contrary to naive expectation, the torque of a disengaged break is
    # NOT zero. Instead, it is at least the same as the minimum break strength, likely larger due to all mechanical
    # couplings in the system.
    timestamps[n_engaged:] = np.array([v["timestamp"] for v in disengaged_data], dtype=np.uint64)
    torques[n_engaged:] = minimum_break_strength  # Broadcasting scalar value

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    torques = torques[sort_indices]

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "break_torque_N_cm": torques,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_valve_data(
    extracted_module_data: ExtractedModuleData,
    output_file: Path,
    scale_coefficient: np.float64,
    nonlinearity_exponent: np.float64,
) -> None:
    """Extracts and saves the data acquired by the ValveModule during runtime as a .feather file.

    Notes:
        Unlike other processing methods, this method generates a .feather dataset with 3 columns: time, dispensed
        water volume, and the state of the tone buzzer.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        scale_coefficient: Stores the scale coefficient used in the fitted power law equation that translates valve
            pulses into dispensed water volumes.
        nonlinearity_exponent: Stores the nonlinearity exponent used in the fitted power law equation that
            translates valve pulses into dispensed water volumes.
    """
    log_data = extracted_module_data.data

    # This function looks for event-codes 52 (Valve Open) and event-codes 53 (Valve Closed). It also looks for codes 55
    # (ToneON) and 56 (ToneOFF), however, and these codes are parsed similar to the ttl state codes.

    # The way this module is implemented guarantees there is at least one code 53 message, but there may be no code
    # 52 messages.
    open_data = log_data.get(np.uint8(52), [])
    closed_data = log_data[np.uint8(53)]

    # If there were no valve open events, no water was dispensed. In this case, uses the first code 53 timestamp
    # to report a zero-volume reward and ends the runtime early. If the valve was never opened, there were no
    # tones, so aborts both tone-parsing and valve-parsing early.
    if not open_data:
        module_dataframe = pl.DataFrame(
            {
                "time_us": np.array([closed_data[0]["timestamp"]], dtype=np.uint64),
                "dispensed_water_volume_uL": np.array([0], dtype=np.float64),
                "tone_state": np.array([0], dtype=np.uint8),
            }
        )
        module_dataframe.write_ipc(file=output_file, compression="uncompressed")
        return

    # Precreates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although valve
    # trigger values are boolean, they are translated into the total volume of water, in microliters, dispensed to the
    # animal at each time-point and store that value as a float64.
    n_on = len(open_data)
    n_off = len(closed_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    volume: NDArray[np.float64] = np.empty(total_length, dtype=np.float64)

    # The water is dispensed gradually while the valve stays open. Therefore, the full reward volume is dispensed
    # when the valve goes from open to closed. Based on calibration data, uses a conversion factor to translate
    # the time the valve remains open into the fluid volume dispensed to the animal, which is then used to convert each
    # Open/Close cycle duration into the dispensed volume.

    # Extracts Open (Code 52) trigger codes. Statically assigns the value '1' to denote Open signals.
    timestamps[:n_on] = np.array([v["timestamp"] for v in open_data], dtype=np.uint64)
    volume[:n_on] = 1  # Open state

    # Extracts Closed (Code 53) trigger codes.
    timestamps[n_on:] = np.array([v["timestamp"] for v in closed_data], dtype=np.uint64)
    volume[n_on:] = 0  # Closed state

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    volume = volume[sort_indices]

    # Find falling and rising edges. Falling edges are valve-closing events, rising edges are valve-opening events.
    edges = np.diff(volume, prepend=volume[0])
    rising_edges = np.where(edges == 1)[0]
    falling_edges = np.where(edges == -1)[0]

    # Samples the timestamp array to only include timestamps for the falling edges. That is, when the valve has
    # finished delivering water
    reward_timestamps = timestamps[falling_edges]

    # Calculates pulse durations in microseconds for each open-close cycle. Since the original timestamp array
    # contains alternating HIGH / LOW edges, each falling edge has to match to a rising edge.
    pulse_durations: NDArray[np.float64] = (timestamps[falling_edges] - timestamps[rising_edges]).astype(np.float64)

    # Converts the time the Valve stayed open into the dispensed water volume, in microliters.
    # noinspection PyTypeChecker
    volumes = np.cumsum(scale_coefficient * np.power(pulse_durations, nonlinearity_exponent))
    volumes = np.round(volumes, decimals=8)

    # The processing logic above removes the initial water volume of 0. This re-adds the initial volume using the
    # first timestamp of the module data. That timestamp communicates the initial valve state, which should be 0.
    reward_timestamps = np.insert(reward_timestamps, 0, timestamps[0])
    volumes = np.insert(volumes, 0, 0.0)

    # Now carries out similar processing for the Tone signals
    # Same logic as with code 52 applies to code 55
    tone_on_data = log_data.get(np.uint8(55), [])
    tone_off_data = log_data.get(np.uint8(56), [])  # The empty default is to appease mypy

    tone_on_n = len(tone_on_data)
    tone_off_n = len(tone_off_data)
    tone_length = tone_on_n + tone_off_n
    tone_timestamps: NDArray[np.uint64] = np.empty(tone_length, dtype=np.uint64)
    tone_states: NDArray[np.uint8] = np.empty(tone_length, dtype=np.uint8)

    # Extracts ON (Code 55) Tone codes. Statically assigns the value '1' to denote On signals.
    tone_timestamps[:tone_on_n] = np.array([v["timestamp"] for v in tone_on_data], dtype=np.uint64)
    tone_states[:tone_on_n] = 1

    # Extracts Closed (Code 53) trigger codes.
    tone_timestamps[tone_on_n:] = np.array([v["timestamp"] for v in tone_off_data], dtype=np.uint64)
    tone_states[tone_on_n:] = 0

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(tone_timestamps)
    tone_timestamps = tone_timestamps[sort_indices]
    tone_states = tone_states[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if tone_states[-1] != 0:
        tone_timestamps = np.append(tone_timestamps, tone_timestamps[-1] + 1)
        tone_states = np.append(tone_states, 0)

    # Constructs a shared array that includes all reward and tone timestamps. This is used to interpolate tone
    # and timestamp values. Sorts the generated array to arrange all timestamps in monotonically ascending order
    shared_stamps = np.unique(np.concatenate([tone_timestamps, reward_timestamps]))

    # Interpolates the reward volumes for each tone state and tone states for each reward volume.
    out_reward = _interpolate_data(
        timestamps=reward_timestamps, data=volumes, seed_timestamps=shared_stamps, is_discrete=True
    )
    out_tones = _interpolate_data(
        timestamps=tone_timestamps, data=tone_states, seed_timestamps=shared_stamps, is_discrete=True
    )

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": shared_stamps,
            "dispensed_water_volume_uL": out_reward,
            "tone_state": out_tones,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_lick_data(extracted_module_data: ExtractedModuleData, output_file: Path, lick_threshold: np.uint16) -> None:
    """Extracts and saves the data acquired by the LickModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        lick_threshold: The voltage threshold for detecting the interaction with the sensor as a lick.

    Notes:
        The extraction classifies lick events based on the lick threshold used during runtime. The
        time-difference between consecutive ON and OFF event edges corresponds to the time, in microseconds, the
        tongue maintained contact with the lick tube. This may include both the time the tongue physically
        touched the tube and the time there was a conductive fluid bridge between the tongue and the lick tube.

        In addition to classifying the licks and providing binary lick state data, the extraction preserves the raw
        12-bit ADC voltages associated with each lick. This way, it is possible to spot issues with the lick detection
        system by applying a different lick threshold from the one used at runtime, potentially augmenting data
        analysis.
    """
    log_data = extracted_module_data.data

    # LickModule only sends messages with code 51 (Voltage level changed). Therefore, this extraction pipeline has
    # to apply the threshold filter, similar to how the real-time processing method.

    # Unlike the other parsing methods, this one will always work as expected since it only deals with one code and
    # that code is guaranteed to be received for each runtime.

    # Extract timestamps and voltage levels. Timestamps use uint64 datatype. Lick sensor
    # voltage levels come in as uint16, but they are later used to generate a binary uint8 lick classification mask.
    voltage_data = log_data[np.uint8(51)]
    timestamps = np.array([v["timestamp"] for v in voltage_data], dtype=np.uint64)
    voltages = np.array([v["data"] for v in voltage_data], dtype=np.uint16)

    # Sorts all arrays by timestamp. This is technically not needed as the extracted values are already sorted by
    # timestamp, but this is still done for additional safety.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    voltages = voltages[sort_indices]

    # Creates a lick binary classification column based on the class threshold. Note, the threshold is inclusive.
    licks = (voltages >= lick_threshold).astype(np.uint8)

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "voltage_12_bit_adc": voltages,
            "lick_state": licks,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_torque_data(
    extracted_module_data: ExtractedModuleData, output_file: Path, torque_per_adc_unit: np.float64
) -> None:
    """Extracts and saves the data acquired by the TorqueModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        torque_per_adc_unit: The conversion actor used to translate ADC units recorded by the torque sensor into
            the torque in Newton centimeter, applied by the animal to the wheel.

    Notes:
        Despite this method trying to translate the detected torque into Newton centimeters, it may not be accurate.
        Partially, the accuracy of the translation depends on the calibration of the interface class, which is very
        hard with our current setup. The accuracy also depends on the used hardware, and currently our hardware is
        not very well suited for working with millivolt differential voltage levels used by the sensor to report
        torque. Therefore, currently, it is best to treat the torque data extracted from this module as a very rough
        estimate of how active the animal is at a given point in time.
    """
    log_data = extracted_module_data.data

    # Looks for event-codes 51 (CCW Torque) and event-codes 52 (CW Torque). CCW torque is interpreted
    # as torque in the positive direction, and CW torque is interpreted as torque in the negative direction.

    # Gets the data, defaulting to an empty list if the data is missing
    ccw_data = log_data.get(np.uint8(51), [])
    cw_data = log_data.get(np.uint8(52), [])

    # The way TorqueModule is implemented guarantees there is at least one CW code message with the displacement
    # of 0 that is received by the PC. In the worst case scenario, there will be no CCW codes and the parsing will
    # not work. To avoid that issue, generates an artificial zero-code CCW value at the same timestamp + 1
    # microsecond as the original CW zero-code value. This does not affect the accuracy of our data, just makes the
    # code work for edge-cases.
    if not ccw_data:
        first_timestamp = cw_data[0]["timestamp"]
        ccw_data = [{"timestamp": first_timestamp + 1, "data": 0}]
    elif not cw_data:
        first_timestamp = ccw_data[0]["timestamp"]
        cw_data = [{"timestamp": first_timestamp + 1, "data": 0}]

    # Precreates the storage numpy arrays for both message types. Timestamps use uint64 datatype. Although torque
    # values are uint16, they are translated into the actual torque applied by the animal in Newton centimeters and
    # store them as float 64 values.
    n_ccw = len(ccw_data)
    n_cw = len(cw_data)
    total_length = n_ccw + n_cw
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)

    # Processes CCW torques (Code 51). CCW torque is interpreted as positive torque
    timestamps[:n_ccw] = np.array([v["timestamp"] for v in ccw_data], dtype=np.uint64)
    ccw_values = np.array([v["data"] for v in ccw_data], dtype=np.float64) * torque_per_adc_unit

    # Processes CW torques (Code 52). CW torque is interpreted as negative torque
    timestamps[n_ccw:] = np.array([v["timestamp"] for v in cw_data], dtype=np.uint64)
    cw_values = -np.array([v["data"] for v in cw_data], dtype=np.float64) * torque_per_adc_unit

    # Combine torques into a unified time-based array
    torques = np.concatenate([ccw_values, cw_values])
    torques = np.round(torques, decimals=8)

    # Sorts both arrays based on timestamps.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    torques = torques[sort_indices]

    # If the last value is not 0, adds a zero-value to the end of the data sequence, one microsecond
    # after the last readout. This is to properly mark the end of the monitoring sequence.
    if torques[-1] != 0:
        timestamps = np.append(timestamps, timestamps[-1] + 1)
        torques = np.append(torques, 0)

    # Replaces -0.0 values with 0.0. This is a convenience conversion used to improve the visual appearance of the data.
    torques[np.isclose(torques, -0.0) & np.signbit(torques)] = 0.0

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": timestamps,
            "torque_N_cm": torques,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_screen_data(extracted_module_data: ExtractedModuleData, output_file: Path, initially_on: bool) -> None:
    """Extracts and saves the data acquired by the ScreenModule during runtime as a .feather file.

    Args:
        extracted_module_data: The ExtractedModuleData instance that stores the data logged by the module during
            runtime.
        output_file: The path to the output .feather file where to save the extracted data.
        initially_on: Communicates the initial state of the screen at module interface initialization. This is used
            to determine the state of the screens after each processed screen toggle signal.

    Notes:
        This extraction method works similar to the TTLModule method. This is intentional, as ScreenInterface is
        essentially a group of 3 TTLModules.
    """
    log_data = extracted_module_data.data
    # Looks for event-codes 52 (pulse ON) and event-codes 53 (pulse OFF).

    # The way the module is implemented guarantees there is at least one code 53 message. However, if the screen state
    # is never toggled, there may be no code 52 messages.
    on_data = log_data.get(np.uint8(52), [])
    off_data = log_data[np.uint8(53)]

    # If there were no ON pulses, screens never changed state. In this case, shorts to returning the data for the
    # initial screen state using the initial Off timestamp. Otherwise, parses the data
    if not on_data:
        module_dataframe = pl.DataFrame(
            {
                "time_us": np.array([off_data[0]["timestamp"]], dtype=np.uint64),
                "screen_state": np.array([initially_on], dtype=np.uint8),
            }
        )
        module_dataframe.write_ipc(file=output_file, compression="uncompressed")
        return

    # Precreates the storage numpy arrays for both message types. Timestamps use uint64 datatype, and the trigger
    # values are boolean.
    n_on = len(on_data)
    n_off = len(off_data)
    total_length = n_on + n_off
    timestamps: NDArray[np.uint64] = np.empty(total_length, dtype=np.uint64)
    triggers: NDArray[np.uint8] = np.empty(total_length, dtype=np.uint8)

    # Extracts ON (Code 52) trigger codes. Statically assigns the value '1' to denote ON signals.
    timestamps[:n_on] = np.array([v["timestamp"] for v in on_data], dtype=np.uint64)
    triggers[:n_on] = 1

    # Extracts OFF (Code 53) trigger codes.
    timestamps[n_on:] = np.array([v["timestamp"] for v in off_data], dtype=np.uint64)
    triggers[n_on:] = 0

    # Sorts both arrays based on the timestamps, so that the data is in the chronological order.
    sort_indices = np.argsort(timestamps)
    timestamps = timestamps[sort_indices]
    triggers = triggers[sort_indices]

    # Finds rising edges (where the signal goes from 0 to 1). Then uses the indices for such events to extract the
    # timestamps associated with each rising edge, before returning them to the caller.
    edges = np.diff(triggers, prepend=0)
    rising_edges = np.where(edges == 1)[0]
    screen_timestamps = timestamps[rising_edges]

    # Adds the initial state of the screen using the first recorded timestamp. The module is configured to send the
    # initial state of the relay (Off) during Setup, so the first recorded timestamp will always be 0 and correspond
    # to the initial state of the screen.
    screen_timestamps = np.concatenate(([timestamps[0]], screen_timestamps))

    # Builds an array of screen states. Starts with the initial screen state and then flips the state for each
    # consecutive timestamp matching a rising edge of the toggle pulse.
    n_states = len(screen_timestamps)
    screen_states = np.empty(n_states, dtype=np.uint8)
    screen_states[0] = initially_on
    if n_states > 1:
        screen_states[1:] = (initially_on + np.arange(1, n_states)) % 2

    # Creates a Polars DataFrame with the processed data
    module_dataframe = pl.DataFrame(
        {
            "time_us": screen_timestamps,
            "screen_state": screen_states,
        }
    )

    # Saves extracted data using Feather format and no compression to support memory-mapping the file during processing.
    module_dataframe.write_ipc(file=output_file, compression="uncompressed")


def _parse_module_data(
    parse_func: Callable[..., None], extracted_data: ExtractedModuleData | None, output_file: Path, **kwargs: Any
) -> Exception | None:
    """Runs a hardware module data parsing function with error handling.

    This helper function is used to parse hardware module data in parallel.

    Args:
        parse_func: The parsing function to execute.
        extracted_data: The extracted module data to parse.
        output_file: The output file path.
        **kwargs: Additional arguments for the parsing function.

    Returns:
        None if successful, Exception if an error occurs during runtime.
    """
    if extracted_data is None:
        return None
    try:
        parse_func(extracted_data, output_file, **kwargs)
    except Exception as e:
        return e
    else:
        return None


def _extract_mesoscope_vr_actor_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Actor microcontroller hardware modules used during Mesoscope-VR acquisition
    system runtime and saves it as multiple .feather files.

    Args:
        log_path: The path to the .npz archive containing the Actor microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """

    # Resolves the modules for which to extract the data. Not all runtimes use all the modules supported by the AMC.
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Break
    index = 0
    if hardware_state.minimum_break_strength is not None and hardware_state.maximum_break_strength is not None:
        module_type_id.append((3, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_break_data,
                "output": output_directory.joinpath("break_data.feather"),
                "kwargs": {
                    "minimum_break_strength": np.float64(hardware_state.minimum_break_strength),
                    "maximum_break_strength": np.float64(hardware_state.maximum_break_strength),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Valve
    if hardware_state.valve_nonlinearity_exponent is not None and hardware_state.valve_scale_coefficient is not None:
        module_type_id.append((5, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_valve_data,
                "output": output_directory.joinpath("valve_data.feather"),
                "kwargs": {
                    "scale_coefficient": np.float64(hardware_state.valve_scale_coefficient),
                    "nonlinearity_exponent": np.float64(hardware_state.valve_nonlinearity_exponent),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Screens
    if hardware_state.screens_initially_on is not None:
        module_type_id.append((7, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_screen_data,
                "output": output_directory.joinpath("screen_data.feather"),
                "kwargs": {
                    "initially_on": hardware_state.screens_initially_on,
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts the data for the requested hardware modules from the log file.
    log_data_tuple = _extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Depending on configuration, executes the parsing tasks in-parallel or sequentially.
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        # Parallel execution
        n_workers = workers if workers > 0 else None  # None uses all available cores
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Actor microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def _extract_mesoscope_vr_sensor_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Sensor microcontroller modules used during Mesoscope-VR acquisition system
    runtime and saves it as multiple .feather files.

    Args:
        log_path: The path to the .npz archive containing the Sensor microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """

    # Resolves the modules for which to extract the data
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Lick Sensor
    index = 0
    if hardware_state.lick_threshold is not None:
        module_type_id.append((4, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_lick_data,
                "output": output_directory.joinpath("lick_data.feather"),
                "kwargs": {
                    "lick_threshold": np.uint16(hardware_state.lick_threshold),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Torque Sensor
    if hardware_state.torque_per_adc_unit is not None:
        module_type_id.append((6, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_torque_data,
                "output": output_directory.joinpath("torque_data.feather"),
                "kwargs": {
                    "torque_per_adc_unit": np.float64(hardware_state.torque_per_adc_unit),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Mesoscope Frame TTL module.
    if hardware_state.recorded_mesoscope_ttl:
        module_type_id.append((1, 1))
        data_indices.append(index)
        parse_tasks.append(
            {"func": _parse_ttl_data, "output": output_directory.joinpath("mesoscope_frame_data.feather"), "kwargs": {}}
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts the data for all requested modules in parallel
    log_data_tuple = _extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Execute module data parsing tasks in parallel
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        # Parallel execution
        n_workers = workers if workers > 0 else None  # None uses all available cores
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Sensor microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def _extract_mesoscope_vr_encoder_data(
    log_path: Path, output_directory: Path, hardware_state: MesoscopeHardwareState, workers: int
) -> None:
    """Extracts the data logged by the Encoder microcontroller modules used during Mesoscope-VR acquisition system
    runtime and saves it as multiple .feather files.

    Args:
        log_path: The path to the .npz archive containing the Encoder microcontroller data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        hardware_state: The HardwareState instance that stores the hardware configuration of the Mesoscope-VR
            acquisition system.
        workers: The number of parallel worker processes (CPU cores) to use for processing. Setting this to a value
            less than 1 uses all available CPU cores. Setting this to 1 conducts the processing sequentially.
    """

    # Resolves the module for which to extract the data
    module_type_id = []  # Determines the data to parse
    data_indices: list[int | None] = []  # Tracks the index under which module's data is returned
    parse_tasks: list[_ParseTask] = []  # Stores parsing tasks to execute in parallel

    # Encoder
    index = 0
    if hardware_state.cm_per_pulse is not None:
        module_type_id.append((2, 1))
        data_indices.append(index)
        parse_tasks.append(
            {
                "func": _parse_encoder_data,
                "output": output_directory.joinpath("encoder_data.feather"),
                "kwargs": {
                    "cm_per_pulse": np.float64(hardware_state.cm_per_pulse),
                },
            }
        )
        index += 1
    else:
        data_indices.append(None)

    # Aborts early if no module data needs to be parsed.
    if set(data_indices) == {None}:
        return

    # Extracts module data in parallel
    log_data_tuple = _extract_logged_hardware_module_data(
        log_path=log_path, module_type_id=tuple(module_type_id), n_workers=workers
    )

    # Parses extracted module data in parallel
    if workers == 1 or len(parse_tasks) == 1:
        # Sequential execution
        for i, task in enumerate(parse_tasks):
            idx = [d for d in data_indices if d is not None][i]
            task["func"](extracted_module_data=log_data_tuple[idx], output_file=task["output"], **task["kwargs"])
    else:
        n_workers = workers if workers > 0 else None
        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = []
            for i, task in enumerate(parse_tasks):
                idx = [d for d in data_indices if d is not None][i]
                future = executor.submit(
                    _parse_module_data, task["func"], log_data_tuple[idx], task["output"], **task["kwargs"]
                )
                futures.append(future)

            # Waits for all parsing tasks to complete and check for errors
            with tqdm(
                total=len(futures), desc="Parsing Encoder microcontroller hardware modules data", unit="module"
            ) as pbar:
                for future in as_completed(futures):
                    error = future.result()
                    if error is not None:
                        pbar.close()  # Closes progress bar before raising error
                        raise error
                    pbar.update(1)


def process_microcontroller_data(
    session_path: Path,
    log_id: int,
    manager_id: int,
    job_count: int,
    reset_tracker: bool = False,
    processed_data_root: Path | None = None,
    workers: int = -1,
) -> None:
    """Reads the specified microcontroller log .npz file and extracts the behavior data recorded by the hardware modules
    managed by the microcontroller as uncompressed .feather files.

    This function is used to process the log archives generated by any microcontroller used in the Sun lab. It assumes
    that the data was logged using the assets from the ataraxis-communication-interface library.

    Args:
        session_path: The path to the session directory for which to process the microcontroller log file.
        log_id: The name (ID) of the log archive to process, e.g. '101'.
        manager_id: The unique identifier of the manager process that manages the log processing runtime.
        job_count: The total number of jobs executed as part of the behavior processing pipeline that calls this
            function.
        reset_tracker: Determines whether to reset the tracker file before executing the runtime. This allows
            recovering from deadlocked runtimes, but otherwise should not be used to ensure runtime safety.
        processed_data_root: The absolute path to the directory where processed data from all projects is stored, if
            different from the root directory provided as part of the 'session_path' argument.
        workers: The number of worker processes to use for extracting the hardware module messages in parallel. Setting
            this argument to a value less than 1 uses all available CPU cores. Setting this to a value of 1 conducts
            the processing sequentially.
    """

    # Loads the target session's data hierarchy into memory
    session = SessionData.load(session_path=session_path, processed_data_root=processed_data_root)

    # Resolves the path to the processed log file
    log_path = session.source_data.behavior_data_path.joinpath(f"{log_id}_log.npz")

    # Ensures that runtime's manager process has exclusive access to the processed session's data
    lock = SessionLock(file_path=session.tracking_data.session_lock_path)
    lock.check_owner(manager_id=manager_id)

    # Initializes the processing tracker for this pipeline.
    tracker = ProcessingTracker(file_path=session.tracking_data.tracking_data_path.joinpath(TrackerFileNames.BEHAVIOR))

    # If requested, resets the processing tracker to the default state before running the processing
    if reset_tracker:
        tracker.abort()

    # Extracts the microcontroller data from the target log file
    tracker.start(manager_id=manager_id, job_count=job_count)
    hardware_state: MesoscopeHardwareState
    try:
        # Mesoscope-VR system
        if session.acquisition_system == AcquisitionSystems.MESOSCOPE_VR:
            # Ensures that the processed session supports this type of processing.
            if session.session_type not in _supported_sessions:
                message = (
                    f"Unable to extract the microcontroller data from the '{log_id}' log file of the session "
                    f"'{session.session_name}'. The processed session has an unsupported session type "
                    f"'{session.session_type}'. Currently, only the following Mesoscope-VR-acquired session types are "
                    f"supported: {', '.join(_supported_sessions)}."
                )
                console.error(message=message, error=NotImplementedError)

            hardware_state = MesoscopeHardwareState.from_yaml(session.source_data.hardware_state_path)  # type: ignore

            # Depending on the target log ID, calls the appropriate extraction function.
            if log_id == 101:  # ACTOR
                console.echo(f"Extracting microcontroller hardware module data from the '{log_id}' log file...")

                _extract_mesoscope_vr_actor_data(
                    log_path=log_path,
                    output_directory=session.processed_data.behavior_data_path,
                    hardware_state=hardware_state,
                    workers=workers,
                )
            elif log_id == 152:  # SENSOR
                console.echo(f"Extracting microcontroller hardware module data from the '{log_id}' log file...")

                _extract_mesoscope_vr_sensor_data(
                    log_path=log_path,
                    output_directory=session.processed_data.behavior_data_path,
                    hardware_state=hardware_state,
                    workers=workers,
                )
            elif log_id == 203:  # ENCODER
                console.echo(f"Extracting microcontroller hardware module data from the '{log_id}' log file...")

                _extract_mesoscope_vr_encoder_data(
                    log_path=log_path,
                    output_directory=session.processed_data.behavior_data_path,
                    hardware_state=hardware_state,
                    workers=workers,
                )
            else:
                message = (
                    f"Unable to extract the microcontroller data from the '{log_id}' log file of the session "
                    f"'{session.session_name}'. Encountered an unknown microcontroller log ID. Currently, the "
                    f"Mesoscope-VR system is expected to use the following log IDs: 101 (actor), 152 (sensor), and 203 "
                    f"(encoder)."
                )
                console.error(message=message, error=ValueError)
        else:
            message = (
                f"Unable to extract the microcontroller data from the '{log_id}' log file of the session "
                f"'{session.session_name}'. The processed session was acquired using an unsupported data acquisition "
                f"system '{session.acquisition_system}'. Currently, only the following acquisition systems are "
                f"supported: {', '.join(_supported_sessions)}."
            )
            console.error(message=message, error=NotImplementedError)

        # Configures the tracker to indicate that the processing was completed successfully.
        tracker.stop(manager_id=manager_id)
        console.echo(f"MicroController hardware module data processing: Complete.", level=LogLevel.SUCCESS)

    # If the runtime encounters an error, configures the tracker to indicate that the processing was interrupted.
    except Exception:
        tracker.error(manager_id=manager_id)
        raise

    # Updates the project manifest file to reflect the processing outcome.
    finally:
        generate_project_manifest(
            raw_project_directory=session.raw_data.root_path.joinpath(session.project_name),
            processed_data_root=processed_data_root,
            manager_id=manager_id,
        )
