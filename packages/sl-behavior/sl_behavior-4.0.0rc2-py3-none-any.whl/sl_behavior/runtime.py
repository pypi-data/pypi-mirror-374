"""This module provides the functions used to extract the acquisition-system and runtime task data from the .npz log
archives generated by data acquisition systems used in the Sun lab."""

from pathlib import Path

from numba import njit, prange  # type: ignore
import numpy as np
import polars as pl
from numpy.typing import NDArray
from numpy.lib.npyio import NpzFile
from sl_shared_assets import (
    SessionData,
    SessionLock,
    SessionTypes,
    ExperimentTrial,
    TrackerFileNames,
    ProcessingTracker,
    AcquisitionSystems,
    MesoscopeExperimentConfiguration,
    generate_project_manifest,
)
from ataraxis_base_utilities import LogLevel, console

# Defines session types and acquisition systems that support extracting runtime data.
_supported_systems = {AcquisitionSystems.MESOSCOPE_VR}
_supported_sessions = {SessionTypes.LICK_TRAINING, SessionTypes.RUN_TRAINING, SessionTypes.MESOSCOPE_EXPERIMENT}


def _prepare_motif_data(
    trial_motifs: list[NDArray[np.uint8]], trial_distances: list[float]
) -> tuple[NDArray[np.uint8], NDArray[np.int32], NDArray[np.int32], NDArray[np.int32], NDArray[np.float32]]:
    """Prepares the flattened trial motif data to speed up the cue-sequence-to-trial decomposition (conversion) process.

    Args:
        trial_motifs: A list of trial motifs (wall cue sequences) used by the processed session, where each sequence
            is stored as a numpy array.
        trial_distances: A list of trial motif distances in centimeters. Should match the order of items inside the
            trial_motifs list.

    Returns:
        A tuple containing five elements. The first element is a flattened array that stores all trial motifs. The
        second element is an array that stores the starting indices of each motif in the flat array. The third element
        is an array that stores the length of each motif. The fourth element is an array that stores the original
        indices of motifs before sorting. The fifth element is an array of trial distances in centimeters.
    """
    # Sorts motifs by length (longest first)
    motif_data: list[tuple[int, NDArray[np.uint8], int]] = [
        (i, motif, len(motif)) for i, motif in enumerate(trial_motifs)
    ]
    motif_data.sort(key=lambda x: x[2], reverse=True)

    # Calculates total size needed to represent all motifs in an array.
    total_size: int = sum(len(motif) for motif in trial_motifs)
    num_motifs: int = len(trial_motifs)

    # Creates arrays with specified dtypes.
    motifs_flat: NDArray[np.uint8] = np.zeros(total_size, dtype=np.uint8)
    motif_starts: NDArray[np.int32] = np.zeros(num_motifs, dtype=np.int32)
    motif_lengths: NDArray[np.int32] = np.zeros(num_motifs, dtype=np.int32)
    motif_indices: NDArray[np.int32] = np.zeros(num_motifs, dtype=np.int32)

    # Fills the arrays
    current_pos: int = 0
    for i, (orig_idx, motif, length) in enumerate(motif_data):
        # Ensures motifs are stored as uint8
        motif_uint8 = motif.astype(np.uint8) if motif.dtype != np.uint8 else motif
        motifs_flat[current_pos : current_pos + length] = motif_uint8
        motif_starts[i] = current_pos
        motif_lengths[i] = length
        motif_indices[i] = orig_idx
        current_pos += length

    # Converts distances to float32 type
    distances_array: NDArray[np.float32] = np.array(trial_distances, dtype=np.float32)

    return motifs_flat, motif_starts, motif_lengths, motif_indices, distances_array


@njit(cache=True)  # type: ignore
def _decompose_sequence_numba_flat(
    cue_sequence: NDArray[np.uint8],
    motifs_flat: NDArray[np.uint8],
    motif_starts: NDArray[np.int32],
    motif_lengths: NDArray[np.int32],
    motif_indices: NDArray[np.int32],
    max_trials: int,
) -> tuple[NDArray[np.int32], int]:
    """Decomposes a long sequence of Virtual Reality (VR) wall cues into individual trial motifs.

    This is a worker function used by the main _decompose_multiple_cue_sequences_into_trials() function to speed up
    sequence decomposition via numba-acceleration.

    Args:
        cue_sequence: The full cue sequence to decompose.
        motifs_flat: All motifs concatenated into a single 1D array.
        motif_starts: Starting index of each motif in motifs_flat.
        motif_lengths: The length of each motif.
        motif_indices: Original indices of motifs (before sorting).
        max_trials: The maximum number of trials that can make up the cue sequence.

    Returns:
        A tuple of two elements. The first element stores the array of trial type-indices (the sequence of trial
        type indices). The second element stores the total number of trials extracted from the cue sequence.
    """
    # Prepares runtime trackers
    trial_indices: NDArray[np.int32] = np.zeros(max_trials, dtype=np.int32)
    trial_count = 0
    sequence_pos = 0
    sequence_length = len(cue_sequence)
    num_motifs = len(motif_lengths)

    # Decomposes the sequence into trial motifs using greedy matching. Longer motifs are matched over shorter ones.
    # Pre-specifying the maximum number of trials serves as a safety feature to avoid processing errors.
    while sequence_pos < sequence_length and trial_count < max_trials:
        motif_found = False

        for i in range(num_motifs):
            motif_length = motif_lengths[i]

            # If the current sequence position is within the bounds of the motif, checks if it matches the motif.
            if sequence_pos + motif_length <= sequence_length:
                # Gets motif start position from the flat array
                motif_start = motif_starts[i]

                # Checks if the motif matches the evaluated sequence.
                match = True
                for j in range(motif_length):
                    if cue_sequence[sequence_pos + j] != motifs_flat[motif_start + j]:
                        match = False
                        break
                # If the motif matches, records the trial type index and moves to the next sequence position.
                if match:
                    trial_indices[trial_count] = motif_indices[i]
                    trial_count += 1
                    sequence_pos += motif_length
                    motif_found = True
                    break
        # If the function is not able to pair a part of the sequence with a motif, aborts with an error.
        if not motif_found:
            return trial_indices, -1

    return trial_indices[:trial_count], trial_count


def _decompose_multiple_cue_sequences_into_trials(
    experiment_configuration: MesoscopeExperimentConfiguration,
    cue_sequences: list[NDArray[np.uint8]],
    distance_breakpoints: list[np.float64],
) -> tuple[NDArray[np.int32], NDArray[np.float64]]:
    """Decomposes multiple Virtual Reality (VR) task wall cue sequences into a unified sequence of trials.

    This function handles cases where the original sequence was interrupted and a new sequence was generated. It uses
    distance breakpoints to stitch sequences together correctly.

    Args:
        experiment_configuration: The initialized ExperimentConfiguration instance for which to parse the trial data.
        cue_sequences: A list of cue sequences in the order they were used during runtime.
        distance_breakpoints: A list of cumulative distances (in centimeters) at which each sequence ends. Should have
            the same number of elements as the number of cue sequences - 1.

    Returns:
        A tuple of two elements. The first element is an array of trial type indices stored in the order encountered at
        runtime. The second element is an array of cumulative distances at the end of each trial.

    Raises:
        ValueError: If the number of breakpoints doesn't match the number of sequences - 1.
        RuntimeError: If the function is not able to fully decompose any of the cue sequences.
    """

    # Validates inputs
    if len(cue_sequences) == 0:
        message = (
            f"Unable to decompose input cue sequence(s) into trials. Expected at least one cue sequence as input, but "
            f"received none."
        )
        console.error(message=message, error=ValueError)

    if len(cue_sequences) > 1 and len(distance_breakpoints) != len(cue_sequences) - 1:
        message = (
            f"Unable to decompose input cue sequence(s) into trials. Expected the number of distance breakpoints "
            f"to be ({len(cue_sequences) - 1} (number of sequences - 1), but encountered ({len(distance_breakpoints)})."
        )
        console.error(message=message, error=ValueError)

    # Extracts the list of trial structures supported by the processed experiment runtime
    trials: list[ExperimentTrial] = [trial for trial in experiment_configuration.trial_structures.values()]

    # Extracts trial motif (cue sequences for each trial type) and their corresponding distances in cm
    trial_motifs: list[NDArray[np.uint8]] = [np.array(trial.cue_sequence, dtype=np.uint8) for trial in trials]
    trial_distances: list[float] = [float(trial.trial_length_cm) for trial in trials]

    # Prepares the flattened motif data
    motifs_flat, motif_starts, motif_lengths, motif_indices, distances_array = _prepare_motif_data(
        trial_motifs, trial_distances
    )

    # Estimates the maximum number of trials across all sequences
    min_motif_length = min(len(motif) for motif in trial_motifs)
    total_cue_length = sum(len(seq) for seq in cue_sequences)
    max_trials = total_cue_length // min_motif_length + 1

    # Processes each sequence and collects results
    all_trial_indices: list[int] = []
    all_trial_distances: list[float] = []
    cumulative_distance = 0.0

    for seq_idx, cue_sequence in enumerate(cue_sequences):
        # Decomposes the current sequence
        trial_indices_array, trial_count = _decompose_sequence_numba_flat(
            cue_sequence, motifs_flat, motif_starts, motif_lengths, motif_indices, max_trials
        )

        # Checks for decomposition errors
        if trial_count == -1:
            # Finds the position where decomposition failed
            sequence_pos = 0
            trial_indices_list = trial_indices_array[:max_trials].tolist()

            for idx in trial_indices_list:
                if idx == 0 and sequence_pos > 0:
                    break
                sequence_pos += len(trial_motifs[idx])

            remaining_sequence = cue_sequence[sequence_pos : sequence_pos + 20]
            message = (
                f"Unable to decompose VR wall cue sequence {seq_idx + 1} of {len(cue_sequences)} into a sequence of "
                f"trial distances. No trial motif matched at position {sequence_pos}. The next 20 cues: "
                f"{remaining_sequence.tolist()}"
            )
            console.error(message=message, error=RuntimeError)
            raise RuntimeError(message)

        # Extracts trial indices for this sequence
        sequence_trial_indices = trial_indices_array[:trial_count].tolist()

        for trial_idx in sequence_trial_indices:
            trial_distance = distances_array[trial_idx]
            new_cumulative_distance = cumulative_distance + trial_distance

            # If this is not the last sequence, checks if the loop has reached the breakpoint distance to
            # truncate the sequence
            if seq_idx < len(cue_sequences) - 1:
                breakpoint_distance = distance_breakpoints[seq_idx]

                # If the processed trial includes the breakpoint distance, truncates the sequence at the breakpoint
                if new_cumulative_distance > breakpoint_distance:
                    # This trial extends beyond the breakpoint. Adds a truncated version of this trial to the
                    # tracking list. The trial type is preserved, but later processing code is expected to catch the
                    # abrupt trial transition
                    truncated_distance = breakpoint_distance - cumulative_distance

                    # Once the truncated trial is found, modifies the trials' data to reflect the fact that the trial
                    # did not reach the final associated distance.
                    if truncated_distance > 0:
                        all_trial_indices.append(trial_idx)
                        all_trial_distances.append(breakpoint_distance)

                        # Notifies the user about the detected breakpoint
                        message = (
                            f"Sequence {seq_idx + 1}, Trial {trial_idx}: truncated. Full trial should have ended at "
                            f"{new_cumulative_distance:.1f} cm, but the sequence was interrupted at "
                            f"{breakpoint_distance:.1f} cm"
                        )
                        console.echo(message=message, level=LogLevel.WARNING)

                    # Stops processing trials (truncates the sequence) after breakpoint
                    cumulative_distance = breakpoint_distance
                    break
                else:
                    # If the trial was traversed fully, adds its data to the stitched sequence
                    all_trial_indices.append(trial_idx)
                    all_trial_distances.append(new_cumulative_distance)
                    cumulative_distance = new_cumulative_distance
            else:
                # For the last sequence, adds all available trials to the stitched sequence.
                all_trial_indices.append(trial_idx)
                all_trial_distances.append(new_cumulative_distance)
                cumulative_distance = new_cumulative_distance

    # Converts results to numpy arrays before returning them to the caller
    trial_type_sequence = np.array(all_trial_indices, dtype=np.int32)
    trial_distance_sequence = np.array(all_trial_distances, dtype=np.float64)

    return trial_type_sequence, trial_distance_sequence


def _decompose_cue_sequence_into_trials(
    experiment_configuration: MesoscopeExperimentConfiguration,
    cue_sequence: NDArray[np.uint8],
) -> tuple[NDArray[np.int32], NDArray[np.float64]]:
    """Decomposes a single Virtual Reality task wall cue sequence into a sequence of trials.

    This is a convenience wrapper around the _decompose_multiple_cue_sequences_into_trials() function to use when
    working with runtimes that only used a single wall cue sequence. Since multiple sequences are only present in
    runtimes that encountered issues at runtime, this function is typically used during most data processing runtimes.

    Args:
        experiment_configuration: The initialized ExperimentConfiguration instance for which to parse the trial data.
        cue_sequence: The cue sequence to decompose into trials.

    Returns:
        A tuple of two elements. The first element is an array of trial type indices stored in the order encountered at
        runtime. The second element is an array of cumulative distances at the end of each trial.

    Raises:
        RuntimeError: If the function is not able to fully decompose the cue sequence.
    """
    trial_indices, trial_distances = _decompose_multiple_cue_sequences_into_trials(
        experiment_configuration=experiment_configuration,
        cue_sequences=[cue_sequence],
        distance_breakpoints=[],
    )

    return trial_indices, trial_distances


def _process_trial_sequence(
    experiment_configuration: MesoscopeExperimentConfiguration,
    trial_types: NDArray[np.int32],
    trial_distances: NDArray[np.float64],
) -> tuple[NDArray[np.uint8], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:
    """Processes the sequence of trials experienced by the animal during runtime to extract trial metadata information.

    This function processes the trial sequences generated by the _decompose_cue_sequence_into_trials() and
    _decompose_multiple_cue_sequences_into_trials() function. The metadata extracted by this function is used to
    support trial-based data analysis in the sl-forgery library.

    Args:
        experiment_configuration: The initialized ExperimentConfiguration instance for which to process the trial
            sequence data.
        trial_types: A NumPy array that stores the indices used to query the trial data for each trial experienced by
            the animal at runtime. The indices are used to query each trial's ExperimentTrial instance from the
            ExperimentConfiguration instance.
        trial_distances: A NumPy array that stores the cumulative traveled distance, in centimeters, at which the animal
            fully completed the trial at runtime. The elements in this array use the same order as elements in the
            trial_types array.

    Returns:
        A tuple of five NumPy arrays. The first array stores the IDs of the cues experienced by the animal at runtime.
        The second array stores the total cumulative distance, in centimeters, traveled by the animal at the onset
        of each cue stored in the first array. The third array stores the cumulative distance traveled by the animal
        when it entered each trial's reward zone. The fourth array stores the cumulative distance traveled by the animal
        when it left each trial's reward zone. The fifth array stores the cumulative distance traveled by the animal
        during each trial when it collided with the invisible wall used to trigger water delivery in 'guided' mode.

    """
    # Extracts the list of trial type objects from experiment configuration data.
    trials: list[ExperimentTrial] = [entry for entry in experiment_configuration.trial_structures.values()]

    # Also extract the dictionary that maps wall cue IDs to the length of each cue, in centimeters, and a static
    # offset used to shift the animal's starting position on the VR track.
    cue_offset = experiment_configuration.cue_offset_cm
    cue_map = experiment_configuration.cue_map  # Maps cue_id -> length_cm

    # Pre-initializes output iterables as lists to efficiently support dynamic growth
    distances_list: list[np.float64] = []
    cues_list: list[np.uint8] = []
    reward_zone_starts_list: list[np.float64] = []
    reward_zone_ends_list: list[np.float64] = []
    trial_start_distances_list: list[np.float64] = []

    # Tracks the cumulative distance as the function essentially rebuilds the stitched cue sequence from trial
    # information
    cumulative_distance = np.float64(0)

    # Tracks whether to apply the cue position offset, which is done at the start and after each trial truncation
    # (sequence regeneration).
    apply_offset_to_next_cue = True

    # Loops over each trial in the sequence experienced by the animal and reconstructs the requested mappings
    previous_trial_end_distance = np.float64(0)
    index: int
    trial: np.int32
    for index, trial in enumerate(trial_types):
        # Uses the trial type to query the corresponding ExperimentTrial object for each trial. Note! This assumes that
        # the items stored in the experiment configuration class always follow the same order and that the order used
        # here and during sequence-to-trial decomposition is the same.
        trial_type = trials[trial]

        # Records the start distance for this trial
        trial_start_distances_list.append(previous_trial_end_distance)

        # Queries the actual distance traveled by the animal while running the processed trial
        actual_trial_distance = trial_distances[index] - previous_trial_end_distance

        # Queries the cue sequence for this trial
        trial_cue_sequence = trial_type.cue_sequence  # List of cue IDs

        # Tracks the distance within this trial to handle truncation
        distance_within_trial = np.float64(0)

        # Processes each cue in the trial
        for cue_idx, cue_id in enumerate(trial_cue_sequence):
            # Determines the length of each cue using the experiment cue_map
            cue_length = cue_map[int(cue_id)]

            # Applies offset if this is the first cue after a sequence start/restart
            if apply_offset_to_next_cue and cue_idx == 0:
                # This is the first cue of a new sequence (initial or after the sequence was regenerated). The animal
                # starts cue_offset cm into this cue
                effective_distance_to_next_cue = cue_length - cue_offset
                apply_offset_to_next_cue = False
            else:
                effective_distance_to_next_cue = cue_length

            # Checks if the trial is truncated (abruptly ended before completion). In this case, the actual trial
            # distance would be less than the end-distance of the currently processed cue
            if distance_within_trial + effective_distance_to_next_cue > actual_trial_distance:
                # In this case, stops processing the trial after this cue
                cues_list.append(np.uint8(cue_id))
                distances_list.append(cumulative_distance)  # Logs current cue onset distance as the end of previous cue

                # Updates the cumulative distance to the actual end of this trial. When the processing continues with
                # the next sequence, this would be used to accurately reflect the partial coverage of this abruptly
                # truncated cue.
                cumulative_distance = previous_trial_end_distance + actual_trial_distance

                # Since this trial was truncated, the next trial will start a new sequence, requiring the offset
                # to be applied again
                apply_offset_to_next_cue = True
                break

            else:
                # Otherwise, includes each cue and associated distance in the distance list.
                cues_list.append(np.uint8(cue_id))
                distances_list.append(cumulative_distance)

                # Updates distance trackers
                cumulative_distance += effective_distance_to_next_cue
                distance_within_trial += effective_distance_to_next_cue

        # Queries reward zone boundaries relative to trial start.
        reward_start_relative = trial_type.reward_zone_start_cm
        reward_end_relative = trial_type.reward_zone_end_cm

        # Converts to absolute positions given the global monotonically increasing traveled distance
        reward_start_absolute = previous_trial_end_distance + reward_start_relative
        reward_end_absolute = previous_trial_end_distance + reward_end_relative

        # Only adds reward zones if the start falls within the actual distance traveled
        if reward_start_absolute <= trial_distances[index]:
            reward_zone_starts_list.append(reward_start_absolute)

            # Clips the end if the trial was truncated before the animal reached the end of the reward zone
            if reward_end_absolute <= trial_distances[index]:
                reward_zone_ends_list.append(reward_end_absolute)
            else:
                # The reward zone was cut off by trial truncation, so 'ends' reward zone using the trial breakpoint
                # data
                # noinspection PyTypeChecker
                reward_zone_ends_list.append(trial_distances[index])

        # Updates previous trial end distance for next iteration
        previous_trial_end_distance = trial_distances[index]

    # Converts lists to numpy arrays and returns them to caller
    distances = np.array(distances_list, dtype=np.float64)
    cues = np.array(cues_list, dtype=np.uint8)
    reward_zone_starts = np.array(reward_zone_starts_list, dtype=np.float64)
    reward_zone_ends = np.array(reward_zone_ends_list, dtype=np.float64)
    trial_start_distances = np.array(trial_start_distances_list, dtype=np.float64)

    return cues, distances, reward_zone_starts, reward_zone_ends, trial_start_distances


def _extract_mesoscope_vr_data(
    log_path: Path, output_directory: Path, experiment_configuration: MesoscopeExperimentConfiguration | None = None
) -> None:
    """Reads the Mesoscope-VR acquisition system .npz log file and extracts acquisition system and runtime (task) data
    as uncompressed .feather files.

    This worker function is specifically designed to process the data logged by the Mesoscope-VR acquisition systems.
    It does not work for any other Sun lab data acquisition system.

    Args:
        log_path: The path to the .npz archive containing the Mesoscope-VR acquisition system data to parse.
        output_directory: The path to the directory where to save the extracted data as uncompressed .feather files.
        experiment_configuration: The ExperimentConfiguration class for the processed session, if the processed session
            is an experiment.
    """
    # Loads the archive into RAM
    archive: NpzFile = np.load(file=log_path)

    # Precreates the variables used to store extracted data
    system_states = []
    system_timestamps = []
    experiment_states = []
    experiment_timestamps = []
    guidance_states = []
    guidance_timestamps = []
    reward_visibility_states = []
    reward_visibility_timestamps = []
    cue_sequences: list[NDArray[np.uint8]] = []
    cue_sequence_breakpoints: list[np.float64] = []

    # Locates the logging onset timestamp. The onset is used to convert the timestamps for logged data into absolute
    # UTC timestamps. Originally, all timestamps other than onset are stored as elapsed time in microseconds
    # relative to the onset timestamp.
    timestamp_offset = 0
    onset_us = np.uint64(0)
    timestamp: np.uint64
    for number, item in enumerate(archive.files):
        message: NDArray[np.uint8] = archive[item]  # Extracts message payload from the compressed .npy file

        # Recovers the uint64 timestamp value from each message. The timestamp occupies 8 bytes of each logged
        # message starting at index 1. If the timestamp value is 0, the message contains the onset timestamp value
        # stored as an 8-byte payload. Index 0 stores the source ID (uint8 value)
        if np.uint64(message[1:9].view(np.uint64)[0]) == 0:
            # Extracts the byte-serialized UTC timestamp stored as microseconds since epoch onset.
            onset_us = np.uint64(message[9:].view("<i8")[0].copy())

            # Breaks the loop once the onset is found. Generally, the onset is expected to be found very early into
            # the loop
            timestamp_offset = number  # Records the item number at which the onset value was found.
            break

    # Once the onset has been discovered, loops over all remaining messages and extracts data stored in these
    # messages.
    for item in archive.files[timestamp_offset + 1 :]:
        message = archive[item]

        # Extracts the elapsed microseconds since timestamp and uses it to calculate the global timestamp for the
        # message, in microseconds since epoch onset.
        elapsed_microseconds = np.uint64(message[1:9].view(np.uint64)[0].copy())
        timestamp = onset_us + elapsed_microseconds

        payload = message[9:]  # Extracts the payload from the message

        # If the message is longer than 500 bytes, it is a sequence of wall cues.
        if len(payload) > 500 and experiment_configuration is not None:
            # Since some runtimes now support generating multiple cue sequences, each sequence is cached into a storage
            # list to be processed later
            cue_sequences.append(payload.view(np.uint8).copy())  # Keeps the original numpy uint8 format

        # If the first element is 1, the message communicates the VR state code.
        elif payload[0] == 1:
            # Extracts the VR state code from the second byte of the message.
            system_states.append(np.uint8(payload[1]))
            system_timestamps.append(timestamp)

        # If the starting code is 2, the message communicates the experiment state code.
        elif payload[0] == 2:
            # Extracts the experiment state code from the second byte of the message.
            experiment_states.append(np.uint8(payload[1]))
            experiment_timestamps.append(timestamp)

        # If the starting code is 3, the message communicates the current lick guidance state
        elif payload[0] == 3:
            guidance_states.append(np.uint8(payload[1]))
            guidance_timestamps.append(timestamp)

        # If the starting code is 4, the message communicates the current reward collision wall visibility state
        elif payload[0] == 4:
            reward_visibility_states.append(np.uint8(payload[1]))
            reward_visibility_timestamps.append(timestamp)

        # If the starting code is 5, the message communicates the breakpoint distance for the current cue sequence.
        elif payload[0] == 5:
            # Skips the first byte (message code) and gets the next 8 bytes storing the distance as a float64
            distance_bytes = payload[1:9]
            traveled_distance = distance_bytes.view(dtype="<f8")[0]  # Converts back to float64
            # noinspection PyTypeChecker
            cue_sequence_breakpoints.append(traveled_distance)

        # Otherwise, the payload cannot be attributed to a known type and is, therefore, ignored

    # Closes the archive to free up memory
    archive.close()

    # Converts extracted data into Polar Feather files:
    # System states
    system_dataframe = pl.DataFrame(
        {
            "time_us": system_timestamps,
            "system_state": system_states,
        }
    )
    system_dataframe.write_ipc(output_directory.joinpath("system_state_data.feather"), compression="uncompressed")

    # Experiment states
    exp_dataframe = pl.DataFrame(
        {
            "time_us": experiment_timestamps,
            "experiment_state": experiment_states,
        }
    )
    exp_dataframe.write_ipc(output_directory.joinpath("experiment_state_data.feather"), compression="uncompressed")

    # Note, although lick guidance and reward visibility are parsed for all sessions, this data only exists for
    # experiment sessions. Therefore, only attempt to export the data if the processed session is an experiment session.
    # The same holds with respect to the VR track data parsed below.
    if experiment_configuration is not None:
        # Lick guidance states
        system_dataframe = pl.DataFrame(
            {
                "time_us": guidance_timestamps,
                "lick_guidance_state": guidance_states,
            }
        )
        system_dataframe.write_ipc(output_directory.joinpath("guidance_state_data.feather"), compression="uncompressed")

        # Reward visibility states
        system_dataframe = pl.DataFrame(
            {
                "time_us": reward_visibility_timestamps,
                "reward_visibility_state": reward_visibility_states,
            }
        )
        system_dataframe.write_ipc(
            output_directory.joinpath("reward_visibility_state_data.feather"), compression="uncompressed"
        )

        # Cue sequence for Mesoscope-VR system

        # Most sessions should only have a single cue sequence. However, if necessary, the runtime also supports
        # stitching multiple abruptly terminated sequences to recover useful information from sessions that encountered
        # issues during runtime.
        if len(cue_sequences) > 1:
            trial_types, trial_distances = _decompose_multiple_cue_sequences_into_trials(
                experiment_configuration=experiment_configuration,
                cue_sequences=cue_sequences,
                distance_breakpoints=cue_sequence_breakpoints,
            )
        else:
            trial_types, trial_distances = _decompose_cue_sequence_into_trials(
                experiment_configuration=experiment_configuration, cue_sequence=cue_sequences.pop()
            )

        # Uses the computed sequence of trials and the information stored inside the ExperimentConfiguration class to
        # determine which VR cues were seen by the animal as it progressed through the experiment trials. Also computes
        # the traveled distance boundaries for reward zones.
        cue_sequence, distance_sequence, reward_start, reward_stop, trial_start = _process_trial_sequence(
            experiment_configuration, trial_types, trial_distances
        )

        # Exports the cue-distance mapping as a Polars dataframe
        cue_dataframe = pl.DataFrame(
            {
                "vr_cue": cue_sequence,
                "traveled_distance_cm": distance_sequence,
            }
        )
        cue_dataframe.write_ipc(output_directory.joinpath("vr_cue_data.feather"), compression="uncompressed")

        # Exports reward_zone-distance mapping as a Polars dataframe
        reward_dataframe = pl.DataFrame(
            {
                "reward_zone_start_cm": reward_start,
                "reward_zone_end_cm": reward_stop,
            }
        )
        reward_dataframe.write_ipc(output_directory.joinpath("vr_reward_zone_data.feather"), compression="uncompressed")

        # Exports trial-distance mapping as a Polars dataframe
        trial_dataframe = pl.DataFrame(
            {
                "trial_type_index": trial_types,
                "traveled_distance_cm": trial_start,
            }
        )
        trial_dataframe.write_ipc(output_directory.joinpath("trial_data.feather"), compression="uncompressed")


def process_runtime_data(
    session_path: Path,
    manager_id: int,
    job_count: int,
    reset_tracker: bool = False,
    processed_data_root: Path | None = None,
) -> None:
    """Reads the target session's data acquisition system .npz log file and extracts acquisition system and runtime
    (task) data as uncompressed .feather files.

    This function is used to process the log archives generated by any data acquisition system used in the Sun lab. It
    assumes that the data was logged using the assets from the sl-experiment library.

    Notes:
        This function statically assumes that the acquisition system log file uses the id '1'.

    Args:
        session_path: The path to the session directory for which to process the acquisition system log file.
        manager_id: The unique identifier of the manager process that manages the log processing runtime.
        job_count: The total number of jobs executed as part of the behavior processing pipeline that calls this
            function.
        reset_tracker: Determines whether to reset the tracker file before executing the runtime. This allows
            recovering from deadlocked runtimes, but otherwise should not be used to ensure runtime safety.
        processed_data_root: The absolute path to the directory where processed data from all projects is stored, if
            different from the root directory provided as part of the 'session_path' argument.
    """

    # Loads the target session's data hierarchy into memory
    session = SessionData.load(session_path=session_path, processed_data_root=processed_data_root)

    # Resolves the path to the processed log file. Statically assumes that all acquisition systems store their data
    # under the log file with source ID '1'.
    log_path = session.source_data.behavior_data_path.joinpath(f"1_log.npz")

    # Ensures that runtime's manager process has exclusive access to the processed session's data
    lock = SessionLock(file_path=session.tracking_data.session_lock_path)
    lock.check_owner(manager_id=manager_id)

    # Initializes the processing tracker for this pipeline.
    tracker = ProcessingTracker(file_path=session.tracking_data.tracking_data_path.joinpath(TrackerFileNames.BEHAVIOR))

    # If requested, resets the processing tracker to the default state before running the processing
    if reset_tracker:
        tracker.abort()

    # Extracts the acquisition system data from the target log file
    tracker.start(manager_id=manager_id, job_count=job_count)
    try:
        # Mesoscope-VR system
        experiment_configuration: MesoscopeExperimentConfiguration | None = None
        if session.acquisition_system == AcquisitionSystems.MESOSCOPE_VR:
            # Ensures that the processed session supports this type of processing.
            if session.session_type not in _supported_sessions:
                error_message = (
                    f"Unable to extract the acquisition system and runtime (task) data from the '{1}' log file of the "
                    f"session '{session.session_name}'. The processed session has an unsupported session type "
                    f"'{session.session_type}'. Currently, only the following Mesoscope-VR-acquired session types are "
                    f"supported: {', '.join(_supported_sessions)}."
                )
                console.error(message=error_message, error=NotImplementedError)

            # If the session is a Mesoscope-VR experiment, loads the experiment configuration data to memory
            if session.session_type == SessionTypes.MESOSCOPE_EXPERIMENT:
                experiment_configuration = MesoscopeExperimentConfiguration.from_yaml(  # type: ignore
                    session.source_data.experiment_configuration_path
                )

            console.echo(f"Extracting runtime data from the '1' log file...")

            # Extracts the runtime data
            _extract_mesoscope_vr_data(
                log_path=log_path,
                experiment_configuration=experiment_configuration,
                output_directory=session.processed_data.behavior_data_path,
            )

            tracker.stop(manager_id=manager_id)
            console.echo(f"Runtime data processing: Complete.", level=LogLevel.SUCCESS)

        else:
            error_message = (
                f"Unable to extract the acquisition system and runtime (task) data from the '{1}' log file of the "
                f"session '{session.session_name}'. The processed session was acquired using an unsupported data "
                f"acquisition system '{session.acquisition_system}'. Currently, only the following acquisition systems "
                f"are supported: {', '.join(_supported_sessions)}."
            )
            console.error(message=error_message, error=NotImplementedError)

    # If the runtime encounters an error, configures the tracker to indicate that the processing was interrupted.
    except Exception:
        tracker.error(manager_id=manager_id)
        raise

    # Updates the project manifest file to reflect the processing outcome.
    finally:
        generate_project_manifest(
            raw_project_directory=session.raw_data.root_path.joinpath(session.project_name),
            processed_data_root=processed_data_root,
            manager_id=manager_id,
        )
