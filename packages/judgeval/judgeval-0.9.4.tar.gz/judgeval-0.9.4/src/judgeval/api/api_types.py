# generated by datamodel-codegen:
#   filename:  .openapi.json
#   timestamp: 2025-08-29T04:49:39+00:00

from __future__ import annotations
from typing import Any, Dict, List, Optional, TypedDict, Union
from typing_extensions import NotRequired


TraceAndSpanId = List


class EvalResultsFetch(TypedDict):
    experiment_run_id: str
    project_name: str


class DatasetFetch(TypedDict):
    dataset_alias: str
    project_name: str


class TraceSave(TypedDict):
    project_name: str
    trace_id: str
    name: str
    created_at: str
    duration: float
    offline_mode: NotRequired[bool]
    has_notification: NotRequired[bool]
    customer_id: NotRequired[Optional[str]]
    tags: NotRequired[List[str]]
    metadata: NotRequired[Dict[str, Any]]
    update_id: NotRequired[int]


class TraceFetch(TypedDict):
    trace_id: str


class TraceAddToDataset(TypedDict):
    trace_id: str
    trace_span_id: str
    dataset_alias: str
    project_name: str


class EvaluationRunsBatchRequest(TypedDict):
    organization_id: str
    evaluation_entries: List[Dict[str, Any]]


class ProjectAdd(TypedDict):
    project_name: str


class ProjectAddResponse(TypedDict):
    project_id: str


class ProjectDeleteFromJudgevalResponse(TypedDict):
    project_name: str


class ProjectDeleteResponse(TypedDict):
    message: str


class ScorerExistsRequest(TypedDict):
    name: str


class ScorerExistsResponse(TypedDict):
    exists: bool


class SavePromptScorerRequest(TypedDict):
    name: str
    prompt: str
    threshold: float
    options: NotRequired[Optional[Dict[str, float]]]
    is_trace: NotRequired[Optional[bool]]


class SavePromptScorerResponse(TypedDict):
    message: str
    name: str


class FetchPromptScorerRequest(TypedDict):
    name: str


class CustomScorerUploadPayload(TypedDict):
    scorer_name: str
    scorer_code: str
    requirements_text: str


class CustomScorerTemplateResponse(TypedDict):
    scorer_name: str
    status: str
    message: str


class ResolveProjectNameRequest(TypedDict):
    project_name: str


class ResolveProjectNameResponse(TypedDict):
    project_id: str


class TraceIdRequest(TypedDict):
    trace_id: str


class SpanScoreRequest(TypedDict):
    span_id: str
    trace_id: str


class BaseScorer(TypedDict):
    score_type: str
    threshold: NotRequired[float]
    name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    score: NotRequired[Optional[float]]
    score_breakdown: NotRequired[Optional[Dict[str, Any]]]
    reason: NotRequired[Optional[str]]
    using_native_model: NotRequired[Optional[bool]]
    success: NotRequired[Optional[bool]]
    model: NotRequired[Optional[str]]
    model_client: NotRequired[Any]
    strict_mode: NotRequired[bool]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    user: NotRequired[Optional[str]]
    server_hosted: NotRequired[bool]


class ScorerConfig(TypedDict):
    score_type: str
    name: NotRequired[Optional[str]]
    threshold: NotRequired[float]
    strict_mode: NotRequired[bool]
    required_params: NotRequired[List[str]]
    kwargs: NotRequired[Optional[Dict[str, Any]]]


class Example(TypedDict):
    example_id: str
    created_at: str
    name: NotRequired[Optional[str]]


class ValidationError(TypedDict):
    loc: List[Union[str, int]]
    msg: str
    type: str


class SpanBatchItem(TypedDict):
    span_id: str
    trace_id: str
    function: str
    created_at: NotRequired[Any]
    parent_span_id: NotRequired[Optional[str]]
    span_type: NotRequired[Optional[str]]
    inputs: NotRequired[Optional[Dict[str, Any]]]
    output: NotRequired[Any]
    error: NotRequired[Optional[Dict[str, Any]]]
    usage: NotRequired[Optional[Dict[str, Any]]]
    duration: NotRequired[Optional[float]]
    expected_tools: NotRequired[Optional[List[Dict[str, Any]]]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    has_evaluation: NotRequired[Optional[bool]]
    agent_name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    state_before: NotRequired[Optional[Dict[str, Any]]]
    state_after: NotRequired[Optional[Dict[str, Any]]]
    span_state: str
    update_id: NotRequired[int]
    queued_at: float


class PromptScorer(TypedDict):
    name: str
    prompt: str
    threshold: float
    options: NotRequired[Optional[Dict[str, float]]]
    created_at: NotRequired[Optional[str]]
    updated_at: NotRequired[Optional[str]]
    is_trace: NotRequired[Optional[bool]]


class ScorerData(TypedDict):
    name: str
    threshold: float
    success: bool
    score: NotRequired[Optional[float]]
    reason: NotRequired[Optional[str]]
    strict_mode: NotRequired[Optional[bool]]
    evaluation_model: NotRequired[Union[List[str], str]]
    error: NotRequired[Optional[str]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]


class TraceUsage(TypedDict):
    prompt_tokens: NotRequired[Optional[int]]
    completion_tokens: NotRequired[Optional[int]]
    cache_creation_input_tokens: NotRequired[Optional[int]]
    cache_read_input_tokens: NotRequired[Optional[int]]
    total_tokens: NotRequired[Optional[int]]
    prompt_tokens_cost_usd: NotRequired[Optional[float]]
    completion_tokens_cost_usd: NotRequired[Optional[float]]
    total_cost_usd: NotRequired[Optional[float]]
    model_name: NotRequired[Optional[str]]


class Tool(TypedDict):
    tool_name: str
    parameters: NotRequired[Optional[Dict[str, Any]]]
    agent_name: NotRequired[Optional[str]]
    result_dependencies: NotRequired[Optional[List[Dict[str, Any]]]]
    action_dependencies: NotRequired[Optional[List[Dict[str, Any]]]]
    require_all: NotRequired[Optional[bool]]


class ExampleEvaluationRun(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: NotRequired[Optional[str]]
    eval_name: NotRequired[Optional[str]]
    custom_scorers: NotRequired[List[BaseScorer]]
    judgment_scorers: NotRequired[List[ScorerConfig]]
    model: str
    created_at: NotRequired[Optional[str]]
    examples: List[Example]
    trace_span_id: NotRequired[Optional[str]]
    trace_id: NotRequired[Optional[str]]


class HTTPValidationError(TypedDict):
    detail: NotRequired[List[ValidationError]]


class TraceEvaluationRun(TypedDict):
    id: NotRequired[Optional[str]]
    project_name: NotRequired[Optional[str]]
    eval_name: NotRequired[Optional[str]]
    custom_scorers: NotRequired[List[BaseScorer]]
    judgment_scorers: NotRequired[List[ScorerConfig]]
    model: str
    created_at: NotRequired[Optional[str]]
    trace_and_span_ids: List[TraceAndSpanId]
    is_offline: NotRequired[bool]


class DatasetInsertExamples(TypedDict):
    dataset_alias: str
    examples: List[Example]
    project_name: str


class SpansBatchRequest(TypedDict):
    spans: List[SpanBatchItem]
    organization_id: str


class FetchPromptScorerResponse(TypedDict):
    scorer: PromptScorer


class TraceSpan(TypedDict):
    span_id: str
    trace_id: str
    function: str
    created_at: NotRequired[Any]
    parent_span_id: NotRequired[Optional[str]]
    span_type: NotRequired[Optional[str]]
    inputs: NotRequired[Optional[Dict[str, Any]]]
    error: NotRequired[Optional[Dict[str, Any]]]
    output: NotRequired[Any]
    usage: NotRequired[Optional[TraceUsage]]
    duration: NotRequired[Optional[float]]
    expected_tools: NotRequired[Optional[List[Tool]]]
    additional_metadata: NotRequired[Optional[Dict[str, Any]]]
    has_evaluation: NotRequired[Optional[bool]]
    agent_name: NotRequired[Optional[str]]
    class_name: NotRequired[Optional[str]]
    state_before: NotRequired[Optional[Dict[str, Any]]]
    state_after: NotRequired[Optional[Dict[str, Any]]]
    update_id: NotRequired[int]


class Trace(TypedDict):
    trace_id: str
    name: str
    created_at: str
    duration: float
    trace_spans: List[TraceSpan]
    offline_mode: NotRequired[bool]
    rules: NotRequired[Dict[str, Any]]
    has_notification: NotRequired[bool]
    customer_id: NotRequired[Optional[str]]
    tags: NotRequired[List[str]]
    metadata: NotRequired[Dict[str, Any]]
    update_id: NotRequired[int]


class ScoringResult(TypedDict):
    success: bool
    scorers_data: Optional[List[ScorerData]]
    name: NotRequired[Optional[str]]
    data_object: NotRequired[Optional[Union[TraceSpan, Example]]]
    trace_id: NotRequired[Optional[str]]
    run_duration: NotRequired[Optional[float]]
    evaluation_cost: NotRequired[Optional[float]]


class TraceRun(TypedDict):
    project_name: NotRequired[Optional[str]]
    eval_name: NotRequired[Optional[str]]
    traces: List[Trace]
    scorers: List[ScorerConfig]
    model: str
    trace_span_id: NotRequired[Optional[str]]
    tools: NotRequired[Optional[List[Dict[str, Any]]]]


class EvalResults(TypedDict):
    results: List[ScoringResult]
    run: Union[ExampleEvaluationRun, TraceEvaluationRun]


class DatasetPush(TypedDict):
    dataset_alias: str
    comments: NotRequired[Optional[str]]
    source_file: NotRequired[Optional[str]]
    examples: NotRequired[Optional[List[Example]]]
    traces: NotRequired[Optional[List[Trace]]]
    is_trace: NotRequired[bool]
    project_name: str
    overwrite: NotRequired[Optional[bool]]
