---
description: Tests
globs: tests/*,tests/**/*.py
alwaysApply: false
---
# Testing Guide 

## Directory Structure & Organization

```
tests/
‚îú‚îÄ‚îÄ unit/                    # Unit tests with heavy mocking
‚îÇ   ‚îî‚îÄ‚îÄ somedirectory/           # Mirror source code structure
‚îÇ       ‚îú‚îÄ‚îÄ langchain_related/
‚îÇ       ‚îú‚îÄ‚îÄ user_client/
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ integration/            # Integration tests with mocking of external dependencies / APIs only
```

**File Naming**: Test files prefixed with `test_` and mirror source structure:
- Source: `somedirectory/langchain_related/tools/planer.py`
- Test: `tests/unit/somedirectory/langchain_related/tools/test_planer.py`

## üö® **CRITICAL: AVOID OVER-TESTING**

*Preventing excessive test code and maintaining focus on behavior*

### **The Over-Testing Problem**

**WARNING SIGNS:**
- Test code exceeds business code by 2:1 ratio
- Testing every possible internal error scenario
- Multiple tests for the same behavior
- Testing implementation details instead of user outcomes
- Complex test setup for simple functions

### **Test-to-Code Ratio Guidelines**

**‚úÖ HEALTHY RATIOS:**
```
Simple Functions (< 50 lines):    1:1 to 1.5:1 test-to-code ratio
Complex Business Logic:           1.5:1 to 2:1 test-to-code ratio
Critical System Components:       2:1 to 2.5:1 test-to-code ratio
```

**‚ùå UNHEALTHY RATIOS:**
```
> 3:1 ratio = Over-testing likely
> 5:1 ratio = Definitely over-testing
```

### **Simple Function Testing Strategy**

**For a simple function like `display_image()`:**

**‚úÖ GOOD - Behavior-Focused (5-6 tests max):**
```python
class TestDisplayImage:
    """Tests for display_image function - behavior focused."""

    def test_successfully_displays_image(self):
        """Test that image is displayed when called."""
        # Test the core behavior

    def test_clears_screen_when_requested(self):
        """Test clear_first=True behavior."""
        # Test parameter behavior

    def test_skips_clearing_when_not_requested(self):
        """Test clear_first=False behavior."""
        # Test parameter behavior

    def test_handles_errors_gracefully(self):
        """Test error handling behavior."""
        # Test error scenarios

    @pytest.mark.parametrize("image_name", [...])
    def test_works_with_different_images(self):
        """Test with various inputs."""
        # Test input variations
```

**‚ùå BAD - Implementation-Focused (10+ tests):**
```python
# DON'T DO THIS - Over-testing implementation details
def test_display_image_handles_clear_error_during_error_recovery(self):
    """Testing internal error recovery mechanisms."""

def test_display_image_calls_correct_internal_methods_in_order(self):
    """Testing internal method call patterns."""

def test_display_image_prints_exact_messages_in_sequence(self):
    """Testing internal print statement details."""
```

### **When to Stop Testing**

**STOP adding tests when:**
- You're testing the same behavior in different ways
- You're testing internal implementation details
- Your test setup is more complex than the function being tested
- You have more test code than business logic code for simple functions
- Tests are breaking when you refactor internals (not behavior)

**ASK YOURSELF:**
- "Does this test verify something the user cares about?"
- "Would this test catch a real bug that affects end users?"
- "Am I testing behavior or implementation?"

## Running Tests

### Direct UV Execution
```bash
uv run pytest tests/unit/ -v
uv run pytest tests/unit/path/to/test.py::TestClass::test_method -v
```

## Test Structure Patterns

### Test Organization
```python
class TestPlanModel:
    """Tests for PlanModel class."""
    
    def test_valid_model(self):
        """Test creating a valid PlanModel."""
        # given
        data = {"selected_tool_names": ["Tool1"], "main_goal": "Test"}
        
        # when
        model = PlanModel(**data)
        
        # then
        assert model.selected_tool_names == ["Tool1"]
        assert model.main_goal == "Test"

    @pytest.mark.parametrize("input_data, expected", [
        pytest.param("input1", "output1", id="scenario1"),
        pytest.param("input2", "output2", id="scenario2"),
    ])
    def test_multiple_scenarios(self, input_data, expected):
        """Test multiple scenarios with different inputs."""
        # given
        component = ComponentUnderTest()
        
        # when
        result = component.process(input_data)
        
        # then
        assert result == expected
```

### Given-When-Then Pattern (Required)
```python
def test_feature_functionality(self):
    """Test description of what this validates."""
    # given: setup test conditions
    mock_data = {"key": "value"}
    component = ComponentUnderTest()
    
    # when: execute the action being tested
    result = component.method_under_test(mock_data)
    
    # then: verify expected outcomes
    assert result == "expected_result"
    assert component.state == "expected_state"
```

### Async Testing
```python
@pytest.mark.asyncio
async def test_async_functionality(self):
    """Test async operations."""
    # given
    mock_service = AsyncMock()
    mock_service.process.return_value = "async_result"
    
    # when
    result = await component.async_method()
    
    # then
    assert result == "async_result"
    mock_service.process.assert_called_once()
```

## Common Test Patterns

### Unit Test Template
```python
class TestMyComponent:
    """Tests for MyComponent class."""
    
    @pytest.fixture
    def component(self):
        """Create component instance for testing."""
        return MyComponent()
    
    def test_basic_functionality(self, component):
        """Test basic component functionality."""
        # given
        input_data = "test input"
        
        # when
        result = component.process(input_data)
        
        # then
        assert result == "expected output"
```


## Best Practices

1. **Use Given-When-Then structure** - Always structure tests with these comments
2. **Group tests in classes** - Organize related tests together
3. **Use descriptive test names** - Test name should explain what is being tested
4. **Leverage autouse fixtures** - Don't manually mock what's auto-mocked
5. **Test both success and error paths** - Include error handling tests
6. **Use parametrization** - Test multiple scenarios efficiently
7. **Keep tests focused** - One assertion per test concept
8. **Use appropriate markers** - `@pytest.mark.asyncio`, `@pytest.mark.essential`
9. **üö® AVOID OVER-TESTING** - Don't test implementation details or internal mechanics
10. **Monitor test-to-code ratios** - Keep ratios reasonable for function complexity

## üéØ **CRITICAL: Test BEHAVIOR, Not IMPLEMENTATION**

*The most important testing principle for maintainable, refactoring-safe tests*

### **Why This Matters**

**‚úÖ GOOD - Behavior-Focused Tests:**
```python
@pytest.mark.asyncio
async def test_execute_llm_task_structured_mode(self, mock_kernel):
    """Test executing LLM task in structured mode (with schema)."""
    # given
    kernel = mock_kernel
    chat_history = [{"role": "user", "content": "Hello"}]
    schema_file = "mock_schema_file"
    
    # when - Testing PUBLIC interface
    result = await execute_llm_task(kernel, chat_history, schema_file)
    
    # then - Verifying BEHAVIOR/OUTCOMES
    assert result["mode"] == "structured"
    assert result["schema_enforced"] == True
    # ‚úÖ Tests WHAT the function does, not HOW
```

**‚ùå BAD - Implementation-Focused Tests:**
```python
async def test_execute_llm_task_internal_details(self):
    """DON'T DO THIS - Testing internal implementation."""
    # ‚ùå Testing internal function calls
    with patch("module._internal_helper_function") as mock_helper:
        # ‚ùå Testing how schema loading works internally
        with patch("module._load_schema_internal") as mock_schema:
            result = await execute_llm_task(...)
            # ‚ùå Verifying internal call patterns
            mock_helper.assert_called_with_specific_params()
            mock_schema.assert_called_once()
```

### **Real-World Success Example**

**Phase 2 Refactoring Proof**: During our major architecture refactoring from parameter pollution to class-based design:

**BEFORE (Internal Complexity):**
```python
async def execute_llm_task(...):
    schema_model = None
    schema_dict = None
    if schema_file:
        schema_result = load_schema_file(...)
        # 50+ lines of complex internal logic
    enhanced_history = _enhance_prompt_with_one_shot_example(...)
    result = await _execute_semantic_kernel_with_schema(...)
```

**AFTER (Clean Class Architecture):**
```python
async def execute_llm_task(...):
    executor = LLMExecutor(kernel=kernel, schema_file=schema_file, output_file=output_file)
    return await executor.execute(chat_history)
```

**Test Results**: **209/209 tests passed** with **ZERO test changes** because tests focused on behavior, not implementation!

### **Behavior vs Implementation Guidelines**

| ‚úÖ **Test This (BEHAVIOR)**    | ‚ùå **Don't Test This (IMPLEMENTATION)** |
| ----------------------------- | -------------------------------------- |
| Function return values        | Internal function calls                |
| Error conditions and messages | How errors are generated internally    |
| State changes in the system   | Which internal methods are called      |
| Public API contracts          | Private method implementations         |
| Input validation rules        | How validation is implemented          |
| Business logic outcomes       | Code organization patterns             |

### **Benefits of Behavior-Focused Testing**

1. **üîÑ Refactoring Safety** - Tests survive major internal changes
2. **üìà Maintenance** - Tests remain stable during code evolution  
3. **üéØ Focus** - Tests verify what users/consumers actually care about
4. **‚ö° Speed** - Less brittle tests mean fewer test maintenance cycles
5. **üß† Clarity** - Tests document actual requirements, not implementation details

### **Practical Application**

**When writing tests, ask:**
- ‚ùì "If I completely rewrote this function's internals, should this test still pass?"
- ‚ùì "Does this test verify what a user of this function cares about?"
- ‚ùì "Would this test help me catch a bug that affects the end user?"

**If the answer is YES ‚Üí You're testing behavior ‚úÖ**  
**If the answer is NO ‚Üí You're testing implementation ‚ùå**

This principle is the foundation of maintainable test suites that survive architectural changes.

## Adding New Tests

1. **Create test file** mirroring source structure with `test_` prefix
2. **Import necessary modules** and fixtures
3. **Organize tests in classes** by functionality
4. **Use Given-When-Then pattern** for all tests
5. **Add fixtures** in local `conftest.py` if needed
6. **Run tests locally** before committing: `./run_tests.sh unit/path/to/test.py`
7. **Check test-to-code ratio** - Avoid over-testing simple functions

## üõ†Ô∏è **SYSTEMATIC TEST FAILURE RESOLUTION FOR CURSOR**

*Advanced debugging methodology for large-scale test failures during refactoring*

### **When to Use Systematic Approach**

Use this methodology when facing:
- **Large refactoring impacts** (20+ failing tests)
- **Architectural changes** (module moves, domain model evolution)
- **Infrastructure updates** (Pydantic adoption, API changes)
- **Multiple failure categories** (imports, types, business logic)

### **Phase-Based Debugging Strategy**

#### **üö® Phase 1: Import/Module Errors (Fix First)**
```python
# Common pattern: Module refactoring breaks imports
# ‚ùå OLD: from handlers.google_calendar_handler import Handler
# ‚úÖ NEW: from langchain_related.tools.google_calendar_tool import Tool

# Strategy: Update import paths systematically
# 1. Fix conftest.py fixtures first (prevents cascade failures)
# 2. Update patch targets to match new module structure
# 3. Create import mapping for large refactors
```

#### **üîß Phase 2: Domain Model Evolution (High Impact)**
```python
# Common pattern: Business logic changes break tests
# ‚ùå OLD: consent.category == ConsentCategory.BASIC_TERMS
# ‚úÖ NEW: consent.category == ConsentCategory.BASIC_DATA_PROCESSING

# Strategy: Understand business changes before fixing
# 1. Analyze current domain model structure
# 2. Add delegation methods for backward compatibility
# 3. Update test expectations to match new business rules
```

#### **üîí Phase 3: Type Safety Migration (Pydantic Focus)**
```python
# Common pattern: Dict access ‚Üí Pydantic attribute access
# ‚ùå OLD: result['allowed'], limits['minute_limit']
# ‚úÖ NEW: result.allowed, limits.minute_limit

# Strategy: Systematic type safety conversion
# 1. Find all dictionary access patterns
# 2. Convert to attribute access systematically
# 3. Update all related assertions
```