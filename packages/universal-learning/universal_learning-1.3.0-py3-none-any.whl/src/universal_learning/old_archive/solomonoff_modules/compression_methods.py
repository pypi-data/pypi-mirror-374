#!/usr/bin/env python3
"""
📋 Compression Methods
=======================

🎯 ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

🧪 Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

📋 Component Integration:
========================
    ┌──────────┐
    │   This   │
    │Component │ ←→ Other Components
    └──────────┘
         ↑↓
    System Integration

"""
"""
🗜️ COMPRESSION-BASED KOLMOGOROV COMPLEXITY APPROXIMATION
========================================================

Author: Benedict Chen (PayPal)
Based on: Li & Vitányi (2019) "An Introduction to Kolmogorov Complexity and Its Applications"

📚 THEORETICAL FOUNDATION
========================
This module implements the compression-based paradigm for approximating Kolmogorov complexity:

    K(x) ≈ |compress(x)|

Where K(x) is the Kolmogorov complexity (length of shortest program generating x),
and |compress(x)| is the compressed size using various algorithms.

🧠 ALGORITHMIC INFORMATION THEORY BACKGROUND  
===========================================
Kolmogorov complexity K(x) is the length of the shortest program that outputs string x.
While K(x) is uncomputable, compression algorithms provide practical approximations by
exploiting regularities that correspond to algorithmic structure:

• REPETITIONS → Dictionary methods (LZ77, LZ78)
• SYMBOL FREQUENCIES → Entropy coding (Huffman, Arithmetic)  
• CONTEXT DEPENDENCIES → Prediction by Partial Matching (PPM)
• LONG-RANGE CORRELATIONS → Burrows-Wheeler Transform (BWT)

Key Insight: If a string compresses well, it likely has low Kolmogorov complexity.
This connection forms the theoretical basis for compression-based complexity estimation.

🔬 MATHEMATICAL FOUNDATIONS
===========================
For any compression algorithm C and string x:

    K(x) ≤ |C(x)| + |Decompress(C)|

Where |Decompress(C)| is the size of the decompression program (constant for fixed C).
This gives us the inequality:

    K(x) ≲ |C(x)|

Different algorithms capture different types of regularities:
• LZ77: K_LZ77(x) ≈ γ log|x| where γ is the number of distinct factors
• RLE: K_RLE(x) ≈ number of runs × log(alphabet size)
• Arithmetic: K_AC(x) ≈ -∑ p_i log p_i (Shannon entropy)

🎯 COMPRESSION ALGORITHMS IMPLEMENTED
====================================
✅ LZ77 (Lempel-Ziv 1977): Sliding window dictionary compression
   - Theoretical basis: Exploits local repetitions and self-similarity
   - Complexity: O(n²) time, good for sequences with nearby repetitions
   
✅ Run-Length Encoding (RLE): Simple repetition compression  
   - Theoretical basis: Exploits runs of identical symbols
   - Optimal for: Sequences with long runs of repeated elements
   
✅ ZLIB/Deflate: LZ77 + Huffman coding
   - Combines dictionary compression with entropy coding
   - Industry standard, good general-purpose approximation
   
✅ LZMA: Advanced dictionary compression
   - Uses range coding and sophisticated matching
   - Excellent compression ratios for structured data
   
✅ BZIP2: Block-sorting compression
   - Burrows-Wheeler Transform + Move-to-Front + RLE + Huffman
   - Captures long-range dependencies and context

🌟 KEY THEORETICAL RESULTS
==========================
1. Universal Compression (Li & Vitányi): No single algorithm dominates all others
2. Normalized Information Distance: d(x,y) = max{K(x|y), K(y|x)} / max{K(x), K(y)}
3. Compression-based similarity: Objects are similar if they compress each other well
4. MDL Principle: Best model minimizes description length (related to compression)

📊 PRACTICAL APPLICATIONS
=========================
• Sequence Analysis: Estimate complexity of genomic/protein sequences
• Anomaly Detection: High compression ratio → likely regular pattern
• Similarity Measurement: Use compression distance for clustering
• Model Selection: Choose model that best compresses the data
• Universal Prediction: Weight hypotheses by compression-based complexity

🔗 REFERENCES
=============
[1] Li, M. & Vitányi, P. (2019). "An Introduction to Kolmogorov Complexity and Its Applications" (4th ed.)
[2] Lempel, A. & Ziv, J. (1977). "A Universal Algorithm for Sequential Data Compression"
[3] Rissanen, J. (1978). "Modeling by Shortest Data Description"  
[4] Salomon, D. (2007). "Data Compression: The Complete Reference" (4th ed.)
[5] Grünwald, P. (2007). "The Minimum Description Length Principle"
[6] Cilibrasi, R. & Vitányi, P. (2005). "Clustering by Compression"
[7] Bennett, C. et al. (1998). "Information Distance"
[8] Chaitin, G. (1987). "Algorithmic Information Theory"

⚠️ COMPUTATIONAL CONSIDERATIONS
===============================
• Time Complexity: Varies by algorithm (O(n) to O(n²))
• Space Complexity: Typically O(n) with sliding windows
• Approximation Quality: Better compression → better complexity estimate
• Algorithm Selection: Different algorithms optimal for different data types

🧪 EXPERIMENTAL VALIDATION
==========================
Extensive empirical studies show compression-based complexity estimates correlate
strongly with:
• True Kolmogorov complexity (when computable)
• Human intuitions about pattern complexity  
• Performance of universal prediction algorithms
• Similarity judgments in various domains

This provides strong evidence for the practical validity of the compression paradigm.
"""

import numpy as np
import zlib
import lzma
import bz2
from typing import Dict, List, Tuple, Any, Optional, Union
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
import heapq
import math
import statistics


class CompressionAlgorithm(Enum):
    """
    🗜️ Compression algorithms for Kolmogorov complexity approximation
    
    Each algorithm captures different types of regularities in data:
    - LZ77: Local repetitions and self-similarity  
    - ZLIB: General-purpose with entropy coding
    - LZMA: Advanced dictionary compression
    - BZIP2: Long-range dependencies via BWT
    - RLE: Simple run-length patterns
    - ALL: Ensemble approach using multiple algorithms
    """
    LZ77 = "lz77"
    ZLIB = "zlib" 
    LZMA = "lzma"
    BZIP2 = "bzip2"
    RLE = "rle"
    ALL = "all"


@dataclass
class CompressionResult:
    """
    📊 Result of compression-based complexity estimation
    
    Encapsulates all information about compression performance including
    theoretical metrics and practical considerations.
    """
    algorithm: CompressionAlgorithm
    original_size: int
    compressed_size: int
    compression_ratio: float  # compressed_size / original_size
    compression_gain: float   # 1 - compression_ratio
    complexity_estimate: float  # Primary K(x) approximation
    bits_per_symbol: float    # Information content per symbol
    entropy_estimate: float   # Shannon entropy approximation
    execution_time: float     # Time taken for compression
    memory_usage: int        # Peak memory usage (if available)
    error_occurred: bool     # Whether compression failed
    error_message: str       # Error details if any
    
    def __post_init__(self):
        """Calculate derived metrics"""
        if self.original_size > 0:
            self.compression_ratio = self.compressed_size / self.original_size
            self.compression_gain = 1.0 - self.compression_ratio
            self.bits_per_symbol = (self.compressed_size * 8) / self.original_size
            # Shannon entropy approximation based on compression
            self.entropy_estimate = -math.log2(self.compression_ratio) if self.compression_ratio > 0 else 0
        else:
            self.compression_ratio = 1.0
            self.compression_gain = 0.0
            self.bits_per_symbol = 0.0
            self.entropy_estimate = 0.0


class CompressionMethodsMixin:
    """
    🧠 COMPRESSION-BASED KOLMOGOROV COMPLEXITY APPROXIMATION MIXIN
    ==============================================================
    
    This mixin implements the fundamental connection between data compression
    and Kolmogorov complexity, providing multiple algorithms for complexity
    estimation and pattern analysis.
    
    THEORETICAL BASIS:
    The core insight is that compressible data has low Kolmogorov complexity.
    If a string x compresses to size C(x), then:
    
        K(x) ≤ |C(x)| + |Decompressor|
    
    Where |Decompressor| is constant for a fixed compression algorithm.
    
    ALGORITHMIC APPROACH:
    We implement multiple compression algorithms, each capturing different
    types of regularities:
    
    1. LZ77: Exploits local repetitions via sliding window dictionary
    2. RLE: Captures run-length patterns efficiently  
    3. ZLIB: Combines LZ77 with Huffman entropy coding
    4. LZMA: Advanced dictionary compression with range coding
    5. BZIP2: Uses Burrows-Wheeler transform for long-range dependencies
    
    ENSEMBLE METHOD:
    Since no single algorithm dominates all others (Universal Compression
    theorem), we provide ensemble methods that combine multiple approaches.
    
    APPLICATIONS:
    • Sequence complexity estimation for Solomonoff induction
    • Pattern discovery and anomaly detection
    • Universal similarity measurement
    • Model selection via Minimum Description Length
    • Data type classification and clustering
    
    RESEARCH CONTEXT:
    This implementation is grounded in decades of research in algorithmic
    information theory, particularly the work of Li & Vitányi on practical
    applications of Kolmogorov complexity via compression.
    """
    
    def __init__(self):
        """Initialize compression methods with configurable parameters"""
        # LZ77 parameters
        self.lz77_window_size = 4096  # Sliding window size
        self.lz77_max_match = 258     # Maximum match length
        self.lz77_min_match = 3       # Minimum match length for efficiency
        
        # RLE parameters  
        self.rle_alphabet_size = 256  # Default byte alphabet
        
        # Compression ensemble weights (tunable based on data type)
        self.compression_weights = {
            CompressionAlgorithm.LZ77: 0.3,
            CompressionAlgorithm.ZLIB: 0.25,
            CompressionAlgorithm.LZMA: 0.25,
            CompressionAlgorithm.BZIP2: 0.1,
            CompressionAlgorithm.RLE: 0.1
        }
        
        # Complexity estimation parameters
        self.complexity_normalization = True    # Normalize by sequence length
        self.use_ensemble_median = False       # Use median instead of weighted average
        self.penalize_overhead = True          # Account for algorithm overhead
        
        # Performance optimization
        self.compression_cache = {}            # Cache for repeated sequences
        self.max_cache_size = 1000            # Maximum cache entries
        self.enable_parallel = False          # Parallel compression (future)
        
    def compression_approximation(self, sequence: List[int], 
                                algorithms: Optional[List[CompressionAlgorithm]] = None) -> Dict[str, Any]:
        """
        🎯 MAIN COMPRESSION-BASED COMPLEXITY APPROXIMATION
        ==================================================
        
        Estimates Kolmogorov complexity using multiple compression algorithms
        and provides comprehensive analysis of the sequence's algorithmic structure.
        
        THEORETICAL FOUNDATION:
        For each compression algorithm C, we estimate:
        
            K(x) ≈ |C(x)| + O(1)
        
        The ensemble approach provides robustness against algorithm-specific biases:
        
            K(x) ≈ Σᵢ wᵢ |Cᵢ(x)|
        
        Where wᵢ are weights reflecting each algorithm's effectiveness.
        
        ALGORITHMIC PROCESS:
        1. Convert sequence to compressible format
        2. Apply each compression algorithm
        3. Measure compressed sizes and ratios  
        4. Calculate ensemble complexity estimate
        5. Provide detailed breakdown and insights
        
        Args:
            sequence: Input sequence of integers/symbols to analyze
            algorithms: Specific algorithms to use (None = use all)
            
        Returns:
            Comprehensive dictionary containing:
            - complexity_estimate: Primary K(x) approximation
            - algorithm_results: Per-algorithm compression metrics
            - ensemble_metrics: Combined analysis
            - theoretical_insights: Pattern analysis and interpretation
            - performance_data: Execution time and resource usage
            
        INTERPRETATION GUIDE:
        • Low complexity (< 0.3): Highly regular, simple pattern
        • Medium complexity (0.3-0.7): Some structure but not trivial  
        • High complexity (> 0.7): Random-like, minimal compression
        """
        
        if not sequence:
            return self._empty_sequence_result()
            
        # Use specified algorithms or default ensemble
        if algorithms is None:
            algorithms = list(self.compression_weights.keys())
            
        # Check cache for this sequence
        cache_key = (tuple(sequence), tuple(algorithms))
        if cache_key in self.compression_cache:
            return self.compression_cache[cache_key]
            
        # Convert sequence to bytes for compression
        sequence_bytes, conversion_info = self._sequence_to_bytes(sequence)
        original_size = len(sequence_bytes)
        
        # Apply each compression algorithm
        algorithm_results = {}
        compression_times = {}
        
        for algorithm in algorithms:
            try:
                import time
                start_time = time.time()
                
                if algorithm == CompressionAlgorithm.LZ77:
                    compressed_data, compressed_size = self._lz77_compress(sequence)
                elif algorithm == CompressionAlgorithm.RLE:
                    compressed_data, compressed_size = self._run_length_encode(sequence)
                elif algorithm == CompressionAlgorithm.ZLIB:
                    compressed_data = zlib.compress(sequence_bytes, level=9)
                    compressed_size = len(compressed_data)
                elif algorithm == CompressionAlgorithm.LZMA:
                    compressed_data = lzma.compress(sequence_bytes, preset=9)
                    compressed_size = len(compressed_data)
                elif algorithm == CompressionAlgorithm.BZIP2:
                    compressed_data = bz2.compress(sequence_bytes, compresslevel=9)
                    compressed_size = len(compressed_data)
                else:
                    continue
                    
                execution_time = time.time() - start_time
                compression_times[algorithm] = execution_time
                
                # Create compression result
                result = CompressionResult(
                    algorithm=algorithm,
                    original_size=original_size,
                    compressed_size=compressed_size,
                    compression_ratio=0,  # Will be calculated in __post_init__
                    compression_gain=0,   # Will be calculated in __post_init__
                    complexity_estimate=compressed_size,
                    bits_per_symbol=0,    # Will be calculated in __post_init__
                    entropy_estimate=0,   # Will be calculated in __post_init__
                    execution_time=execution_time,
                    memory_usage=0,       # TODO: Implement memory tracking
                    error_occurred=False,
                    error_message=""
                )
                
                algorithm_results[algorithm] = result
                
            except Exception as e:
                # Handle compression failures gracefully
                algorithm_results[algorithm] = CompressionResult(
                    algorithm=algorithm,
                    original_size=original_size,
                    compressed_size=original_size,  # No compression achieved
                    compression_ratio=1.0,
                    compression_gain=0.0,
                    complexity_estimate=original_size,
                    bits_per_symbol=8.0,  # Assume 8 bits per byte
                    entropy_estimate=0.0,
                    execution_time=0.0,
                    memory_usage=0,
                    error_occurred=True,
                    error_message=str(e)
                )
        
        # Calculate ensemble complexity estimate
        ensemble_complexity = self._calculate_ensemble_complexity(algorithm_results)
        
        # Perform theoretical analysis
        theoretical_insights = self._analyze_compression_patterns(algorithm_results, sequence)
        
        # Prepare comprehensive result
        result = {
            'complexity_estimate': ensemble_complexity,
            'normalized_complexity': ensemble_complexity / original_size if original_size > 0 else 0,
            'algorithm_results': algorithm_results,
            'ensemble_metrics': self._calculate_ensemble_metrics(algorithm_results),
            'theoretical_insights': theoretical_insights,
            'performance_data': {
                'total_time': sum(compression_times.values()),
                'fastest_algorithm': min(compression_times.keys(), key=lambda k: compression_times[k]),
                'best_compression': min(algorithm_results.keys(), 
                                      key=lambda k: algorithm_results[k].compressed_size),
                'compression_times': compression_times
            },
            'sequence_properties': {
                'length': len(sequence),
                'alphabet_size': len(set(sequence)),
                'entropy_estimate': self._calculate_shannon_entropy(sequence),
                'repetition_structure': self._analyze_repetitions(sequence)
            }
        }
        
        # Cache result if cache not full
        if len(self.compression_cache) < self.max_cache_size:
            self.compression_cache[cache_key] = result
            
        return result
    
    def _lz77_compress(self, sequence: List[int]) -> Tuple[List[Tuple], int]:
        """
        🔍 LZ77 COMPRESSION IMPLEMENTATION
        ==================================
        
        Implements the classic Lempel-Ziv 1977 algorithm for dictionary-based
        compression. This algorithm is fundamental to understanding how local
        repetitions contribute to compressibility and low Kolmogorov complexity.
        
        ALGORITHM DESCRIPTION:
        LZ77 uses a sliding window approach to find repeated substrings:
        1. Maintain a "search buffer" of recently processed symbols
        2. Look ahead into unprocessed data for matches
        3. Encode matches as (distance, length) pairs
        4. Encode non-matching symbols literally
        
        THEORETICAL SIGNIFICANCE:
        LZ77 compression ratio relates to the number of distinct substrings,
        which connects to pattern complexity. For a sequence with γ distinct
        factors of length ≤ k:
        
            |LZ77(x)| ≈ γ * log(n) + O(k)
            
        This provides a direct link to substring complexity measures.
        
        COMPLEXITY ANALYSIS:
        • Time: O(n²) worst case, O(n log n) average
        • Space: O(window_size) 
        • Compression effectiveness: Excellent for repetitive local patterns
        
        Args:
            sequence: Input sequence of integers to compress
            
        Returns:
            Tuple containing:
            - compressed_data: List of (type, *args) tuples
            - complexity: Estimated compressed size in bits
        """
        
        if not sequence:
            return [], 0
            
        compressed = []
        position = 0
        
        while position < len(sequence):
            # Search for longest match in sliding window
            match_distance = 0
            match_length = 0
            
            # Define search window boundaries
            window_start = max(0, position - self.lz77_window_size)
            search_end = min(len(sequence), position + self.lz77_max_match)
            
            # Look for matches in the search buffer
            for distance in range(1, min(position - window_start + 1, self.lz77_window_size)):
                if position - distance < 0:
                    break
                    
                # Find longest match at this distance
                length = 0
                while (position + length < search_end and 
                       position - distance + length < position and
                       sequence[position + length] == sequence[position - distance + (length % distance)]):
                    length += 1
                    
                # Update best match if this is longer and worth encoding
                if length >= self.lz77_min_match and length > match_length:
                    match_distance = distance
                    match_length = length
            
            # Output either match or literal
            if match_length >= self.lz77_min_match:
                compressed.append(('match', match_distance, match_length))
                position += match_length
            else:
                compressed.append(('literal', sequence[position]))
                position += 1
        
        # Estimate compressed size in bits
        # Each literal: log2(alphabet_size) bits
        # Each match: log2(window_size) + log2(max_match) bits
        alphabet_size = len(set(sequence)) if sequence else 2
        literal_bits = math.log2(alphabet_size) if alphabet_size > 1 else 1
        match_bits = math.log2(self.lz77_window_size) + math.log2(self.lz77_max_match)
        
        complexity = 0
        for item in compressed:
            if item[0] == 'literal':
                complexity += literal_bits + 1  # +1 for type bit
            else:  # match
                complexity += match_bits + 1    # +1 for type bit
                
        return compressed, int(complexity / 8) + 1  # Convert to bytes
    
    def _run_length_encode(self, sequence: List[int]) -> Tuple[List[Tuple], int]:
        """
        📏 RUN-LENGTH ENCODING IMPLEMENTATION  
        =====================================
        
        Implements Run-Length Encoding (RLE), one of the simplest compression
        algorithms that captures repetitive patterns effectively.
        
        ALGORITHM DESCRIPTION:
        RLE replaces runs of identical symbols with (symbol, count) pairs:
        - Input: [1,1,1,2,2,3,3,3,3] 
        - Output: [(1,3), (2,2), (3,4)]
        
        THEORETICAL ANALYSIS:
        For a sequence with r runs, RLE complexity is:
        
            |RLE(x)| = r * (log₂|Σ| + log₂n)
            
        Where |Σ| is alphabet size and n is sequence length.
        This makes RLE optimal for sequences with few but long runs.
        
        KOLMOGOROV COMPLEXITY CONNECTION:
        Sequences with good RLE compression have structure that can be
        described by:
        • Number of distinct runs (r)
        • Run lengths (which may follow patterns)
        • Symbol distribution
        
        If run lengths follow a simple pattern, the Kolmogorov complexity
        can be much lower than the RLE estimate suggests.
        
        Args:
            sequence: Input sequence to compress
            
        Returns:
            Tuple of (compressed_data, estimated_size_bytes)
        """
        
        if not sequence:
            return [], 0
        
        compressed = []
        current_symbol = sequence[0]
        run_length = 1
        
        for symbol in sequence[1:]:
            if symbol == current_symbol:
                run_length += 1
            else:
                compressed.append((current_symbol, run_length))
                current_symbol = symbol
                run_length = 1
                
        # Don't forget the last run
        compressed.append((current_symbol, run_length))
        
        # Estimate compressed size
        alphabet_size = len(set(sequence))
        symbol_bits = math.log2(alphabet_size) if alphabet_size > 1 else 1
        count_bits = math.log2(len(sequence)) if len(sequence) > 1 else 1
        
        complexity = len(compressed) * (symbol_bits + count_bits)
        
        return compressed, int(complexity / 8) + 1  # Convert to bytes
        
    def _sequence_to_bytes(self, sequence: List[int]) -> Tuple[bytes, Dict]:
        """
        🔄 SEQUENCE TO BYTES CONVERSION
        ===============================
        
        Converts integer sequences to byte arrays for compression while
        preserving information content and handling various data ranges.
        
        CONVERSION STRATEGIES:
        1. Direct byte mapping (0-255 range)
        2. Multi-byte encoding for larger integers
        3. String-based encoding for very large values
        4. Optimal encoding based on value distribution
        
        INFORMATION PRESERVATION:
        The conversion must be lossless and efficient. Different strategies
        have different compression implications:
        • Byte mapping: Preserves symbol-level patterns
        • String encoding: May introduce artifacts but handles any range
        • Optimal encoding: Minimizes representation overhead
        
        Args:
            sequence: Input integer sequence
            
        Returns:
            Tuple of (bytes_data, conversion_info_dict)
        """
        
        if not sequence:
            return b'', {'method': 'empty', 'overhead': 0}
            
        min_val = min(sequence)
        max_val = max(sequence)
        
        conversion_info = {
            'min_value': min_val,
            'max_value': max_val,
            'range': max_val - min_val,
            'original_length': len(sequence)
        }
        
        try:
            # Strategy 1: Direct byte mapping if all values in [0, 255]
            if 0 <= min_val <= max_val <= 255:
                sequence_bytes = bytes(sequence)
                conversion_info.update({
                    'method': 'direct_bytes',
                    'overhead': 0,
                    'efficiency': 1.0
                })
                return sequence_bytes, conversion_info
                
            # Strategy 2: Shifted byte mapping if range fits in byte
            elif max_val - min_val <= 255:
                shifted_sequence = [x - min_val for x in sequence]
                sequence_bytes = bytes(shifted_sequence)
                conversion_info.update({
                    'method': 'shifted_bytes', 
                    'shift_value': min_val,
                    'overhead': 4,  # Store shift value
                    'efficiency': 1.0
                })
                return sequence_bytes, conversion_info
                
            # Strategy 3: Multi-byte encoding for larger ranges
            elif max_val < 65536:  # Fits in 2 bytes
                byte_array = bytearray()
                for val in sequence:
                    byte_array.extend(val.to_bytes(2, 'big'))
                conversion_info.update({
                    'method': 'multibyte_2',
                    'bytes_per_value': 2,
                    'overhead': 1,
                    'efficiency': math.log2(max_val + 1) / 16 if max_val > 0 else 0
                })
                return bytes(byte_array), conversion_info
                
            # Strategy 4: String-based encoding for very large values
            else:
                sequence_str = ','.join(map(str, sequence))
                sequence_bytes = sequence_str.encode('utf-8')
                conversion_info.update({
                    'method': 'string_encoding',
                    'overhead': 10,  # Estimated overhead for delimiters
                    'efficiency': 0.5  # Lower efficiency due to decimal representation
                })
                return sequence_bytes, conversion_info
                
        except (ValueError, OverflowError) as e:
            # Fallback: String encoding
            sequence_str = str(sequence)
            sequence_bytes = sequence_str.encode('utf-8') 
            conversion_info.update({
                'method': 'fallback_string',
                'error': str(e),
                'overhead': 20,
                'efficiency': 0.3
            })
            return sequence_bytes, conversion_info
    
    def _calculate_ensemble_complexity(self, algorithm_results: Dict[CompressionAlgorithm, CompressionResult]) -> float:
        """
        🎯 ENSEMBLE COMPLEXITY CALCULATION
        ==================================
        
        Combines complexity estimates from multiple compression algorithms
        to provide a robust approximation of Kolmogorov complexity.
        
        THEORETICAL JUSTIFICATION:
        No single compression algorithm dominates all others (Universal
        Compression theorem). An ensemble approach provides:
        1. Robustness against algorithm-specific biases
        2. Better coverage of different pattern types
        3. Improved accuracy through diversification
        
        COMBINATION METHODS:
        • Weighted average: Σᵢ wᵢ Kᵢ(x) 
        • Geometric mean: (∏ᵢ Kᵢ(x)^wᵢ)
        • Median: Robust to outliers
        • Minimum: Most optimistic estimate (best compression)
        
        Args:
            algorithm_results: Dictionary of compression results by algorithm
            
        Returns:
            Combined complexity estimate
        """
        
        if not algorithm_results:
            return float('inf')
            
        # Filter out failed compressions
        valid_results = {alg: result for alg, result in algorithm_results.items() 
                        if not result.error_occurred}
                        
        if not valid_results:
            return float('inf')
        
        # Method 1: Weighted average (default)
        if not self.use_ensemble_median:
            total_weight = 0
            weighted_sum = 0
            
            for algorithm, result in valid_results.items():
                weight = self.compression_weights.get(algorithm, 0.2)
                complexity = result.complexity_estimate
                
                # Apply normalization if enabled
                if self.complexity_normalization and result.original_size > 0:
                    complexity = complexity / result.original_size
                    
                # Apply overhead penalty if enabled
                if self.penalize_overhead:
                    overhead_penalty = self._get_algorithm_overhead(algorithm)
                    complexity += overhead_penalty
                    
                weighted_sum += weight * complexity
                total_weight += weight
                
            return weighted_sum / total_weight if total_weight > 0 else float('inf')
            
        # Method 2: Median (robust alternative)
        else:
            complexities = []
            for algorithm, result in valid_results.items():
                complexity = result.complexity_estimate
                
                if self.complexity_normalization and result.original_size > 0:
                    complexity = complexity / result.original_size
                    
                if self.penalize_overhead:
                    overhead_penalty = self._get_algorithm_overhead(algorithm)
                    complexity += overhead_penalty
                    
                complexities.append(complexity)
                
            return statistics.median(complexities) if complexities else float('inf')
    
    def _calculate_ensemble_metrics(self, algorithm_results: Dict[CompressionAlgorithm, CompressionResult]) -> Dict[str, Any]:
        """
        📊 ENSEMBLE ANALYSIS METRICS
        ============================
        
        Provides comprehensive analysis of compression results across algorithms,
        including statistical measures and algorithmic insights.
        
        METRICS CALCULATED:
        • Compression statistics (mean, median, std dev)
        • Algorithm agreement (consistency across methods)  
        • Best/worst performers for this specific sequence
        • Diversity measures (how much algorithms disagree)
        • Confidence intervals and reliability estimates
        
        THEORETICAL INTERPRETATION:
        • High agreement → Robust complexity estimate
        • Low agreement → Sequence may have mixed patterns
        • Best algorithm → Reveals dominant pattern type
        • Diversity → Indicates algorithmic uncertainty
        """
        
        valid_results = {alg: result for alg, result in algorithm_results.items() 
                        if not result.error_occurred}
        
        if not valid_results:
            return {'status': 'no_valid_results'}
            
        # Extract key metrics
        compression_ratios = [result.compression_ratio for result in valid_results.values()]
        complexities = [result.complexity_estimate for result in valid_results.values()]
        execution_times = [result.execution_time for result in valid_results.values()]
        
        # Statistical analysis
        metrics = {
            'num_algorithms': len(valid_results),
            'compression_ratio_stats': {
                'mean': statistics.mean(compression_ratios),
                'median': statistics.median(compression_ratios),
                'stdev': statistics.stdev(compression_ratios) if len(compression_ratios) > 1 else 0,
                'min': min(compression_ratios),
                'max': max(compression_ratios)
            },
            'complexity_stats': {
                'mean': statistics.mean(complexities),
                'median': statistics.median(complexities),
                'stdev': statistics.stdev(complexities) if len(complexities) > 1 else 0,
                'min': min(complexities),
                'max': max(complexities)
            },
            'performance_stats': {
                'total_time': sum(execution_times),
                'mean_time': statistics.mean(execution_times),
                'fastest_time': min(execution_times),
                'slowest_time': max(execution_times)
            }
        }
        
        # Algorithm rankings
        best_compression = min(valid_results.keys(), 
                              key=lambda k: valid_results[k].compression_ratio)
        worst_compression = max(valid_results.keys(),
                               key=lambda k: valid_results[k].compression_ratio)
        fastest_algorithm = min(valid_results.keys(),
                               key=lambda k: valid_results[k].execution_time)
        
        metrics.update({
            'best_compression_algorithm': best_compression,
            'worst_compression_algorithm': worst_compression, 
            'fastest_algorithm': fastest_algorithm,
            'compression_efficiency': {
                alg.value: result.compression_gain for alg, result in valid_results.items()
            }
        })
        
        # Diversity and agreement measures
        if len(compression_ratios) > 1:
            cv = statistics.stdev(compression_ratios) / statistics.mean(compression_ratios)
            metrics['coefficient_of_variation'] = cv
            metrics['algorithm_agreement'] = max(0, 1 - cv)  # Higher = more agreement
            
        # Confidence assessment
        metrics['confidence_level'] = self._assess_confidence(valid_results)
        
        return metrics
    
    def _analyze_compression_patterns(self, algorithm_results: Dict[CompressionAlgorithm, CompressionResult], 
                                     sequence: List[int]) -> Dict[str, Any]:
        """
        🔬 THEORETICAL PATTERN ANALYSIS
        ===============================
        
        Analyzes compression results to infer theoretical properties about
        the sequence's algorithmic structure and complexity characteristics.
        
        ANALYSIS DIMENSIONS:
        1. Pattern Type Identification: What kind of structure is present?
        2. Complexity Classification: Simple, structured, or random-like?
        3. Algorithmic Insights: Which theoretical models apply?
        4. Predictive Implications: What does this mean for forecasting?
        
        THEORETICAL FRAMEWORK:
        Different compression ratios indicate different types of structure:
        • Excellent LZ77 → Local repetitions, self-similarity
        • Good RLE → Run-length patterns, state persistence  
        • Strong LZMA → Complex but structured patterns
        • Poor all → Random-like, high complexity
        
        This analysis connects compression performance to fundamental
        questions about the sequence's algorithmic nature.
        """
        
        insights = {
            'pattern_classification': 'unknown',
            'complexity_level': 'medium',
            'dominant_structures': [],
            'algorithmic_properties': {},
            'theoretical_interpretation': '',
            'solomonoff_implications': {},
            'recommendations': []
        }
        
        valid_results = {alg: result for alg, result in algorithm_results.items() 
                        if not result.error_occurred}
                        
        if not valid_results:
            return insights
            
        # Analyze compression performance patterns
        best_ratios = {}
        for alg, result in valid_results.items():
            best_ratios[alg] = result.compression_gain
            
        # Pattern classification based on which algorithms work best
        max_gain = max(best_ratios.values()) if best_ratios else 0
        
        if max_gain > 0.8:  # Excellent compression
            insights['complexity_level'] = 'low'
            insights['pattern_classification'] = 'highly_regular'
            insights['theoretical_interpretation'] = (
                "Sequence exhibits strong algorithmic regularity. "
                "Low Kolmogorov complexity indicates simple underlying pattern."
            )
        elif max_gain > 0.5:  # Good compression  
            insights['complexity_level'] = 'medium-low'
            insights['pattern_classification'] = 'structured'
            insights['theoretical_interpretation'] = (
                "Sequence contains significant structure but is not trivial. "
                "Moderate Kolmogorov complexity with identifiable patterns."
            )
        elif max_gain > 0.2:  # Some compression
            insights['complexity_level'] = 'medium-high' 
            insights['pattern_classification'] = 'weakly_structured'
            insights['theoretical_interpretation'] = (
                "Sequence shows limited structure. May contain patterns "
                "mixed with noise or irregular components."
            )
        else:  # Poor compression
            insights['complexity_level'] = 'high'
            insights['pattern_classification'] = 'random_like'
            insights['theoretical_interpretation'] = (
                "Sequence appears random-like with high Kolmogorov complexity. "
                "Limited algorithmic structure detectable."
            )
            
        # Identify dominant structures based on algorithm performance
        if CompressionAlgorithm.RLE in best_ratios and best_ratios[CompressionAlgorithm.RLE] > 0.3:
            insights['dominant_structures'].append('run_length_patterns')
            
        if CompressionAlgorithm.LZ77 in best_ratios and best_ratios[CompressionAlgorithm.LZ77] > 0.3:
            insights['dominant_structures'].append('local_repetitions')
            
        if CompressionAlgorithm.BZIP2 in best_ratios and best_ratios[CompressionAlgorithm.BZIP2] > 0.4:
            insights['dominant_structures'].append('long_range_correlations')
            
        # Solomonoff Induction implications
        insights['solomonoff_implications'] = {
            'predictability': 'high' if max_gain > 0.6 else 'medium' if max_gain > 0.3 else 'low',
            'pattern_confidence': max_gain,
            'universal_prior_weight': 2**(-insights.get('complexity_estimate', len(sequence))),
            'explanation_quality': 'excellent' if max_gain > 0.7 else 'good' if max_gain > 0.4 else 'poor'
        }
        
        # Generate recommendations
        if insights['complexity_level'] == 'low':
            insights['recommendations'].extend([
                'Sequence is highly predictable - simple models should work well',
                'Look for mathematical patterns (arithmetic, geometric, periodic)',
                'Strong candidate for rule-based forecasting'
            ])
        elif insights['complexity_level'] == 'high':
            insights['recommendations'].extend([
                'Sequence may be random - use statistical models',
                'Consider stochastic processes or noise models', 
                'Prediction accuracy likely limited by inherent randomness'
            ])
        else:
            insights['recommendations'].extend([
                'Mixed structure suggests hybrid modeling approaches',
                'Try both pattern-based and statistical methods',
                'Consider ensemble prediction techniques'
            ])
            
        return insights
    
    def _calculate_shannon_entropy(self, sequence: List[int]) -> float:
        """Calculate Shannon entropy of sequence for comparison with compression"""
        if not sequence:
            return 0.0
            
        # Count symbol frequencies
        symbol_counts = {}
        for symbol in sequence:
            symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1
            
        # Calculate entropy
        entropy = 0.0
        total_count = len(sequence)
        
        for count in symbol_counts.values():
            probability = count / total_count
            if probability > 0:
                entropy -= probability * math.log2(probability)
                
        return entropy
    
    def _analyze_repetitions(self, sequence: List[int]) -> Dict[str, Any]:
        """Analyze repetition structure in sequence"""
        if not sequence:
            return {}
            
        # Find all repeated subsequences
        repetitions = {}
        max_length = min(len(sequence) // 2, 20)  # Limit search for efficiency
        
        for length in range(2, max_length + 1):
            for start in range(len(sequence) - length + 1):
                subseq = tuple(sequence[start:start + length])
                if subseq in repetitions:
                    repetitions[subseq] += 1
                else:
                    repetitions[subseq] = 1
                    
        # Filter to only actual repetitions (count > 1)
        actual_repetitions = {k: v for k, v in repetitions.items() if v > 1}
        
        return {
            'num_unique_subsequences': len(repetitions),
            'num_repeated_subsequences': len(actual_repetitions),
            'most_frequent_repetition': max(actual_repetitions, key=actual_repetitions.get) if actual_repetitions else None,
            'max_repetition_count': max(actual_repetitions.values()) if actual_repetitions else 0,
            'repetition_ratio': len(actual_repetitions) / len(repetitions) if repetitions else 0
        }
    
    def _get_algorithm_overhead(self, algorithm: CompressionAlgorithm) -> float:
        """Estimate overhead penalty for each algorithm"""
        overhead_estimates = {
            CompressionAlgorithm.RLE: 0.1,      # Minimal overhead
            CompressionAlgorithm.LZ77: 0.2,     # Dictionary overhead
            CompressionAlgorithm.ZLIB: 0.3,     # LZ77 + Huffman tables
            CompressionAlgorithm.LZMA: 0.4,     # Complex encoder
            CompressionAlgorithm.BZIP2: 0.5     # BWT + multiple stages
        }
        return overhead_estimates.get(algorithm, 0.2)
    
    def _assess_confidence(self, algorithm_results: Dict[CompressionAlgorithm, CompressionResult]) -> str:
        """Assess confidence level in complexity estimate based on algorithm agreement"""
        if len(algorithm_results) < 2:
            return 'low'
            
        compression_ratios = [result.compression_ratio for result in algorithm_results.values()]
        cv = statistics.stdev(compression_ratios) / statistics.mean(compression_ratios)
        
        if cv < 0.1:
            return 'very_high'
        elif cv < 0.2:
            return 'high'  
        elif cv < 0.4:
            return 'medium'
        else:
            return 'low'
    
    def _empty_sequence_result(self) -> Dict[str, Any]:
        """Return appropriate result for empty sequence"""
        return {
            'complexity_estimate': 0,
            'normalized_complexity': 0,
            'algorithm_results': {},
            'ensemble_metrics': {'status': 'empty_sequence'},
            'theoretical_insights': {
                'pattern_classification': 'empty',
                'complexity_level': 'minimal',
                'theoretical_interpretation': 'Empty sequence has zero complexity by definition.'
            },
            'performance_data': {'total_time': 0},
            'sequence_properties': {
                'length': 0,
                'alphabet_size': 0,
                'entropy_estimate': 0,
                'repetition_structure': {}
            }
        }
    
    def get_compression_summary(self, sequence: List[int]) -> str:
        """
        📋 COMPRESSION ANALYSIS SUMMARY
        ===============================
        
        Provides a human-readable summary of compression-based complexity analysis,
        suitable for research reports and practical interpretation.
        
        Returns formatted summary string with key insights and recommendations.
        """
        
        results = self.compression_approximation(sequence)
        
        summary_lines = [
            "🗜️ COMPRESSION-BASED COMPLEXITY ANALYSIS",
            "=" * 50,
            "",
            f"📏 Sequence Length: {len(sequence)}",
            f"🎯 Complexity Estimate: {results['complexity_estimate']:.2f}",
            f"📊 Normalized Complexity: {results['normalized_complexity']:.3f}",
            f"🔬 Pattern Classification: {results['theoretical_insights']['pattern_classification']}",
            f"⚡ Complexity Level: {results['theoretical_insights']['complexity_level']}",
            "",
            "🏆 ALGORITHM PERFORMANCE:",
        ]
        
        # Add algorithm results
        for alg, result in results['algorithm_results'].items():
            if not result.error_occurred:
                summary_lines.append(
                    f"  • {alg.value}: {result.compression_gain:.1%} compression gain, "
                    f"{result.execution_time*1000:.1f}ms"
                )
        
        summary_lines.extend([
            "",
            "🧠 THEORETICAL INTERPRETATION:",
            f"  {results['theoretical_insights']['theoretical_interpretation']}",
            "",
            "💡 RECOMMENDATIONS:"
        ])
        
        for rec in results['theoretical_insights']['recommendations']:
            summary_lines.append(f"  • {rec}")
            
        return "\n".join(summary_lines)