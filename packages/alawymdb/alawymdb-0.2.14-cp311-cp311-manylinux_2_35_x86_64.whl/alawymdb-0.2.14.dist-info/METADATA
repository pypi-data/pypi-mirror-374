Metadata-Version: 2.4
Name: alawymdb
Version: 0.2.14
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Rust
Classifier: Topic :: Database
Classifier: Operating System :: OS Independent
Requires-Dist: numpy>=1.20.0
Requires-Dist: pandas>=1.3.0
Summary: Linear-scaling in-memory database optimized for ML workloads
Keywords: database,columnar,in-memory,performance
Author-email: Tomio Kobayashi <tomkob99@yahoo.co.jp>
License: MIT
Requires-Python: >=3.7
Description-Content-Type: text/markdown; charset=UTF-8; variant=GFM


# AlawymDB

[![PyPI version](https://badge.fury.io/py/alawymdb.svg)](https://badge.fury.io/py/alawymdb)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**A**lmost **L**inear **A**ny **W**ay **Y**ou **M**easure - A high-performance in-memory database that achieves near-linear O(n) scaling for operations that traditionally suffer from O(n log n) complexity.

## üöÄ Breakthrough Performance

AlawymDB (pronounced "ah-LAY-wim") lives up to its name - delivering almost linear performance any way you measure it:

### Column Scaling Performance
```
Columns: 10 ‚Üí 100 ‚Üí 1000 ‚Üí 2000
Cells/sec: 18.5M ‚Üí 7.5M ‚Üí 5.8M ‚Üí 5.2M
Scaling: 1√ó ‚Üí 10√ó ‚Üí 100√ó ‚Üí 200√ó (columns)
         1√ó ‚Üí 4.1√ó ‚Üí 5.3√ó ‚Üí 6.1√ó (time)
```

**Result: O(n) with minimal logarithmic factor - effectively linear!** üéØ

## üîß Installation

```bash
pip install alawymdb
```

## üíæ Storage Options

AlawymDB now supports both in-memory and disk-based storage, allowing you to choose the best option for your use case:

### In-Memory Storage (Default)
```python
import alawymdb as db

# Default: 32GB memory cap, no disk storage
db.create_database()

# Custom memory cap (8GB)
db.create_database(memory_cap_mb=8192)
```

### Disk-Based Storage
```python
import alawymdb as db

# Use 5GB disk storage (automatically creates storage in temp directory)
db.create_database(disk_gb=5)

# Custom disk path with 10GB storage
db.create_database(disk_gb=10, disk_path="/path/to/storage")

# Hybrid: 4GB memory cap with 20GB disk overflow
db.create_database(memory_cap_mb=4096, disk_gb=20)
```

### Storage Configuration Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `memory_cap_mb` | int | 32768 (32GB) | Maximum memory usage in MB |
| `disk_gb` | int | None | Disk storage size in GB |
| `disk_path` | str | Auto-generated | Custom path for disk storage |

### Storage Modes

1. **Pure In-Memory** (default)
   ```python
   db.create_database()  # Uses up to 32GB RAM
   ```

2. **Pure Disk-Based**
   ```python
   db.create_database(disk_gb=10)  # 10GB disk storage
   ```

3. **Hybrid Mode**
   ```python
   db.create_database(memory_cap_mb=2048, disk_gb=50)  # 2GB RAM + 50GB disk
   ```

### When to Use Each Mode

- **In-Memory**: Best for high-performance analytics on datasets that fit in RAM
- **Disk-Based**: Ideal for large datasets that exceed available memory
- **Hybrid**: Optimal for large datasets with hot/cold data patterns

The disk storage feature maintains AlawymDB's almost-linear performance characteristics while enabling work with datasets larger than available RAM.

## üíæ Database Persistence - Save and Restore

AlawymDB now supports saving your entire database to disk and restoring it later, enabling:
- **Data persistence** across application restarts
- **Database sharing** between different processes
- **Backup and recovery** capabilities
- **Data migration** between systems

### Save and Restore API

```python
# Save the current database state
save_info = db.save_database("/path/to/backup")
print(save_info)  # Shows save statistics

# Restore a database from backup
restore_info = db.restore_database("/path/to/backup")
print(restore_info)  # Shows restore statistics
```

### üì¶ Multi-Process Data Sharing Example

This example demonstrates how different processes can share a database via save/restore:

```python
# save_restore_demo.py
import alawymdb as db
import sys
import os

BACKUP_PATH = "/tmp/shared_database_backup"

def save_process():
    """Process 1: Create database and save it"""
    print("=== SAVE PROCESS ===")
    
    # Create and populate database
    db.create_database()
    db.create_schema("company")
    db.create_table("company", "employees", [
        ("id", "UINT64", False),
        ("name", "STRING", False),
        ("department", "STRING", False),
        ("salary", "FLOAT64", False),
    ])
    
    # Insert sample data
    employees = [
        (1, "Alice", "Engineering", 95000.0),
        (2, "Bob", "Sales", 75000.0),
        (3, "Charlie", "Marketing", 82000.0),
        (4, "Diana", "HR", 70000.0),
        (5, "Eve", "Engineering", 105000.0)
    ]
    
    for emp_id, name, dept, salary in employees:
        db.insert_row("company", "employees", [
            ("id", emp_id),
            ("name", name),
            ("department", dept),
            ("salary", salary)
        ])
    
    # Query to verify
    result = db.execute_sql("SELECT COUNT(*) FROM company.employees")
    print(f"Created {result} employees")
    
    # Save the database
    save_info = db.save_database(BACKUP_PATH)
    print(f"‚úÖ Database saved to: {BACKUP_PATH}")
    print(save_info)

def restore_process():
    """Process 2: Restore database and verify"""
    print("=== RESTORE PROCESS ===")
    
    # Create new database instance
    db.create_database()
    
    # Restore from backup
    restore_info = db.restore_database(BACKUP_PATH)
    print(f"‚úÖ Database restored from: {BACKUP_PATH}")
    print(restore_info)
    
    # Verify data is intact
    result = db.execute_sql("SELECT * FROM company.employees")
    print("Restored data:")
    print(result)
    
    # Run analytics on restored data
    avg_salary = db.execute_sql("""
        SELECT department, AVG(salary) as avg_salary 
        FROM company.employees 
        GROUP BY department
    """)
    print("\nAverage salary by department:")
    print(avg_salary)

# Run based on command line argument
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage:")
        print("  python save_restore_demo.py save     # Process 1")
        print("  python save_restore_demo.py restore  # Process 2")
        sys.exit(1)
    
    if sys.argv[1] == "save":
        save_process()
    elif sys.argv[1] == "restore":
        restore_process()
```

**Run the example:**
```bash
# Terminal 1: Create and save database
python save_restore_demo.py save

# Terminal 2: Restore and use database
python save_restore_demo.py restore
```

### üîÑ Backup and Recovery Example

```python
import alawymdb as db
from datetime import datetime

def create_backup(backup_dir="/tmp/backups"):
    """Create timestamped backup"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = f"{backup_dir}/db_backup_{timestamp}"
    
    save_info = db.save_database(backup_path)
    print(f"‚úÖ Backup created: {backup_path}")
    return backup_path

def restore_from_backup(backup_path):
    """Restore database from specific backup"""
    db.create_database()
    restore_info = db.restore_database(backup_path)
    print(f"‚úÖ Restored from: {backup_path}")
    return restore_info

# Example usage
import alawymdb as db

# Create and populate your database
db.create_database()
db.create_schema("production")
db.create_table("production", "users", [
    ("user_id", "UINT64", False),
    ("username", "STRING", False),
    ("email", "STRING", False),
    ("created_at", "STRING", False),
])

# ... work with database ...

# Create backup before risky operation
backup_path = create_backup()

# ... perform operations ...

# If something goes wrong, restore from backup
if need_to_restore:
    restore_from_backup(backup_path)
```

### üìä Data Migration Example

```python
import alawymdb as db

def export_for_migration():
    """Export database for migration to another system"""
    # Your production database
    db.create_database()
    # ... existing database with data ...
    
    # Export to portable location
    migration_path = "/shared/migration_data.alawym"
    save_info = db.save_database(migration_path)
    print(f"üì¶ Database exported for migration: {migration_path}")
    print(f"   Size: {save_info}")
    return migration_path

def import_on_new_system(migration_path):
    """Import database on new system"""
    # New system
    db.create_database()
    
    # Import the migrated database
    restore_info = db.restore_database(migration_path)
    print(f"‚úÖ Database imported successfully")
    print(f"   Stats: {restore_info}")
    
    # Verify data integrity
    tables = db.execute_sql("SHOW TABLES")  # If supported
    print(f"   Available tables: {tables}")
```

### üí° Save/Restore Use Cases

1. **Session Persistence**: Save work at the end of a session, restore on next startup
2. **Data Sharing**: Share datasets between team members or processes
3. **Testing**: Save a known good state, restore after tests
4. **Checkpointing**: Regular saves during long-running operations
5. **Deployment**: Package data with applications

### ‚ö° Performance Notes

- Save/restore operations are optimized for speed
- The backup format is compact and efficient
- Both operations scale linearly with database size
- Supports databases created with both memory and disk storage modes

## üí° Quick Start

```python
import alawymdb as db

# Initialize database (with optional disk storage)
db.create_database()  # Or: db.create_database(disk_gb=5)

# Create schema and table
db.create_schema("main")
db.create_table(
    "main", 
    "users",
    [
        ("id", "UINT64", False),
        ("name", "STRING", False),
        ("age", "INT64", True),
        ("email", "STRING", True),
        ("score", "FLOAT64", True)
    ]
)

# Insert data
users = [
    (1, "Alice", 30, "alice@example.com", 95.5),
    (2, "Bob", 25, "bob@example.com", 87.3),
    (3, "Charlie", 35, "charlie@example.com", 92.1),
]

for user_id, name, age, email, score in users:
    db.insert_row("main", "users", [
        ("id", user_id),
        ("name", name),
        ("age", age),
        ("email", email),
        ("score", score)
    ])

# Query with SQL
result = db.execute_sql("SELECT * FROM main.users")
print(result)

# SQL with WHERE clause
young_users = db.execute_sql("SELECT * FROM main.users WHERE age = 25")
print(f"Young users:\n{young_users}")

# Direct API queries
all_users = db.select_all("main", "users")
print(f"Total users: {db.count_rows('main', 'users')}")

# Convert to Pandas DataFrame
df = db.to_pandas("main", "users")
print(df.describe())

# Get column as NumPy array
ages = db.to_numpy("main", "users", "age")
print(f"Average age: {ages.mean():.1f}")
```

## üíæ Complete Example with Disk Storage

```python
import alawymdb as db

# Initialize database with 5GB disk storage
db.create_database(disk_gb=5)

# Create schema and table
db.create_schema("main")
db.create_table(
    "main", 
    "users",
    [
        ("id", "UINT64", False),
        ("name", "STRING", False),
        ("age", "INT64", True),
        ("email", "STRING", True),
        ("score", "FLOAT64", True)
    ]
)

# Insert data
users = [
    (1, "Alice", 30, "alice@example.com", 95.5),
    (2, "Bob", 25, "bob@example.com", 87.3),
    (3, "Charlie", 35, "charlie@example.com", 92.1),
]

for user_id, name, age, email, score in users:
    db.insert_row("main", "users", [
        ("id", user_id),
        ("name", name),
        ("age", age),
        ("email", email),
        ("score", score)
    ])

# Query with SQL
result = db.execute_sql("SELECT * FROM main.users")
print(result)

# SQL with WHERE clause
young_users = db.execute_sql("SELECT * FROM main.users WHERE age = 25")
print(f"Young users: {young_users}")

older_users = db.execute_sql("SELECT * FROM main.users WHERE age > 25")
print(f"Older users: {older_users}")

# Direct API queries
all_users = db.select_all("main", "users")
print(f"Total users: {db.count_rows('main', 'users')}")

# Convert to Pandas DataFrame
df = db.to_pandas("main", "users")
print(df.describe())

# Get column as NumPy array
ages = db.to_numpy("main", "users", "age")
print(f"Average age: {ages.mean():.1f}")
```

## üìä E-Commerce Analytics Example

A comprehensive example demonstrating SQL aggregations, JOINs, and set operations:

```python
import alawymdb as db
import time

def setup_ecommerce_database():
    """Create an e-commerce database with products, customers, and sales data"""
    print("üè™ Setting up E-commerce Database...")
    
    # Initialize database
    db.create_database()
    db.create_schema("ecommerce")
    
    # Create products table
    db.create_table(
        "ecommerce", 
        "products",
        [
            ("product_id", "UINT64", False),
            ("name", "STRING", False),
            ("category", "STRING", False),
            ("price", "FLOAT64", False),
            ("stock", "INT64", False),
        ]
    )
    
    # Create customers table
    db.create_table(
        "ecommerce",
        "customers",
        [
            ("customer_id", "UINT64", False),
            ("name", "STRING", False),
            ("email", "STRING", False),
            ("city", "STRING", False),
            ("loyalty_points", "INT64", False),
        ]
    )
    
    # Create sales table
    db.create_table(
        "ecommerce",
        "sales",
        [
            ("sale_id", "UINT64", False),
            ("customer_id", "UINT64", False),
            ("product_id", "UINT64", False),
            ("quantity", "INT64", False),
            ("sale_amount", "FLOAT64", False),
        ]
    )
    
    # Insert sample products
    products = [
        (1, "Laptop Pro", "Electronics", 1299.99, 50),
        (2, "Wireless Mouse", "Electronics", 29.99, 200),
        (3, "USB-C Hub", "Electronics", 49.99, 150),
        (4, "Coffee Maker", "Appliances", 89.99, 75),
        (5, "Desk Lamp", "Furniture", 39.99, 120),
    ]
    
    for prod in products:
        db.insert_row("ecommerce", "products", [
            ("product_id", prod[0]),
            ("name", prod[1]),
            ("category", prod[2]),
            ("price", prod[3]),
            ("stock", prod[4]),
        ])
    
    # Insert sample customers
    customers = [
        (1, "Alice Johnson", "alice@email.com", "New York", 1500),
        (2, "Bob Smith", "bob@email.com", "Los Angeles", 800),
        (3, "Charlie Brown", "charlie@email.com", "Chicago", 2000),
    ]
    
    for cust in customers:
        db.insert_row("ecommerce", "customers", [
            ("customer_id", cust[0]),
            ("name", cust[1]),
            ("email", cust[2]),
            ("city", cust[3]),
            ("loyalty_points", cust[4]),
        ])
    
    # Insert sample sales
    sales = [
        (1, 1, 1, 1, 1299.99),  # Alice bought a laptop
        (2, 1, 2, 2, 59.98),    # Alice bought 2 mice
        (3, 2, 3, 1, 49.99),    # Bob bought a USB hub
        (4, 3, 4, 1, 89.99),    # Charlie bought a coffee maker
    ]
    
    for sale in sales:
        db.insert_row("ecommerce", "sales", [
            ("sale_id", sale[0]),
            ("customer_id", sale[1]),
            ("product_id", sale[2]),
            ("quantity", sale[3]),
            ("sale_amount", sale[4]),
        ])
    
    print(f"‚úÖ Database created with {db.count_rows('ecommerce', 'products')} products, "
          f"{db.count_rows('ecommerce', 'customers')} customers, "
          f"{db.count_rows('ecommerce', 'sales')} sales")

def run_analytics():
    """Demonstrate analytics queries with aggregations and JOINs"""
    
    # 1. Aggregation Functions
    print("\nüìä Aggregation Examples:")
    print("-" * 40)
    
    # COUNT
    result = db.execute_sql("SELECT COUNT(*) FROM ecommerce.sales")
    print(f"Total sales: {result}")
    
    # SUM
    result = db.execute_sql("SELECT SUM(sale_amount) FROM ecommerce.sales")
    print(f"Total revenue: {result}")
    
    # AVG
    result = db.execute_sql("SELECT AVG(price) FROM ecommerce.products")
    print(f"Average product price: {result}")
    
    # MIN/MAX
    result = db.execute_sql("SELECT MIN(price), MAX(price) FROM ecommerce.products")
    print(f"Price range: {result}")
    
    # 2. JOIN Operations
    print("\nüîó JOIN Examples:")
    print("-" * 40)
    
    # Customer purchases with product details
    sql = """
    SELECT 
        c.name,
        p.name,
        s.quantity,
        s.sale_amount
    FROM ecommerce.sales s
    INNER JOIN ecommerce.customers c ON s.customer_id = c.customer_id
    INNER JOIN ecommerce.products p ON s.product_id = p.product_id
    """
    result = db.execute_sql(sql)
    print("Customer Purchase Details:")
    print(result)
    
    # 3. GROUP BY with aggregations
    print("\nüìà GROUP BY Examples:")
    print("-" * 40)
    
    sql = """
    SELECT 
        c.name,
        COUNT(s.sale_id),
        SUM(s.sale_amount)
    FROM ecommerce.customers c
    INNER JOIN ecommerce.sales s ON c.customer_id = c.customer_id
    GROUP BY c.customer_id, c.name
    """
    try:
        result = db.execute_sql(sql)
        print("Sales by Customer:")
        print(result)
    except:
        # Fallback if GROUP BY not fully supported
        print("GROUP BY example - showing individual sales instead")
        sql_alt = """
        SELECT c.name, s.sale_amount
        FROM ecommerce.customers c
        INNER JOIN ecommerce.sales s ON c.customer_id = c.customer_id
        """
        print(db.execute_sql(sql_alt))
    
    # 4. Set Operations
    print("\nüîÑ Set Operation Examples:")
    print("-" * 40)
    
    # UNION - High-value items (products over $50 OR customers with 1000+ points)
    sql = """
    SELECT name FROM ecommerce.products WHERE price > 50
    UNION
    SELECT name FROM ecommerce.customers WHERE loyalty_points > 1000
    """
    try:
        result = db.execute_sql(sql)
        print("High-value items and VIP customers:")
        print(result)
    except:
        print("UNION example - showing separately:")
        print("Premium products:", db.execute_sql("SELECT name FROM ecommerce.products WHERE price > 50"))
        print("VIP customers:", db.execute_sql("SELECT name FROM ecommerce.customers WHERE loyalty_points > 1000"))

# Run the example
setup_ecommerce_database()
run_analytics()
```

## üéØ Working Example: Toy JOIN Operations

```python
import alawymdb as db
import time

def setup_toy_database():
    """Create toy database with customers and orders tables"""
    print("üîß Setting up toy database...")
    
    db.create_database()
    db.create_schema("toy")
    
    # Create customers table
    db.create_table(
        "toy", 
        "customers",
        [
            ("customer_id", "UINT64", False),
            ("name", "STRING", False),
            ("country", "STRING", False),
            ("join_date", "STRING", False),
        ]
    )
    
    # Create orders table
    db.create_table(
        "toy",
        "orders",
        [
            ("order_id", "UINT64", False),
            ("customer_id", "UINT64", False),
            ("product", "STRING", False),
            ("amount", "FLOAT64", False),
            ("order_date", "STRING", False),
        ]
    )
    
    # Insert customers
    customers = [
        (1, "Alice", "USA", "2023-01-15"),
        (2, "Bob", "Canada", "2023-02-20"),
        (3, "Charlie", "UK", "2023-03-10"),
        (4, "Diana", "Germany", "2023-04-05"),
        (5, "Eve", "France", "2023-05-12"),
    ]
    
    for cust in customers:
        db.insert_row("toy", "customers", [
            ("customer_id", cust[0]),
            ("name", cust[1]),
            ("country", cust[2]),
            ("join_date", cust[3]),
        ])
    
    # Insert orders (some customers have multiple orders, some have none)
    orders = [
        (101, 1, "Laptop", 1200.0, "2024-01-10"),
        (102, 1, "Mouse", 25.0, "2024-01-15"),
        (103, 2, "Keyboard", 75.0, "2024-01-20"),
        (104, 3, "Monitor", 350.0, "2024-02-01"),
        (105, 1, "Headphones", 150.0, "2024-02-15"),
        (106, 3, "Webcam", 80.0, "2024-03-01"),
        (107, 2, "USB Drive", 30.0, "2024-03-10"),
        # Note: Diana (4) and Eve (5) have no orders
    ]
    
    for order in orders:
        db.insert_row("toy", "orders", [
            ("order_id", order[0]),
            ("customer_id", order[1]),
            ("product", order[2]),
            ("amount", order[3]),
            ("order_date", order[4]),
        ])
    
    print("‚úÖ Toy database created successfully!")
    print(f"   - Customers: {db.count_rows('toy', 'customers')}")
    print(f"   - Orders: {db.count_rows('toy', 'orders')}")

def demonstrate_joins():
    """Demonstrate various JOIN operations"""
    
    print("\n" + "="*80)
    print("JOIN DEMONSTRATIONS")
    print("="*80)
    
    # 1. INNER JOIN
    print("\n1Ô∏è‚É£ INNER JOIN - Customers with their orders")
    print("-" * 60)
    sql = """
    SELECT 
        c.name,
        c.country,
        o.product,
        o.amount
    FROM toy.customers c
    INNER JOIN toy.orders o ON c.customer_id = o.customer_id
    ORDER BY c.name, o.amount DESC
    """
    result = db.execute_sql(sql)
    print(result)
    
    # 2. LEFT JOIN
    print("\n2Ô∏è‚É£ LEFT JOIN - All customers, including those without orders")
    print("-" * 60)
    sql = """
    SELECT 
        c.name,
        c.country,
        o.order_id,
        o.product,
        o.amount
    FROM toy.customers c
    LEFT JOIN toy.orders o ON c.customer_id = o.customer_id
    ORDER BY c.name
    """
    result = db.execute_sql(sql)
    print(result)
    
    # 3. RIGHT JOIN
    print("\n3Ô∏è‚É£ RIGHT JOIN - All orders with customer details")
    print("-" * 60)
    sql = """
    SELECT 
        c.name,
        c.country,
        o.order_id,
        o.product,
        o.amount
    FROM toy.customers c
    RIGHT JOIN toy.orders o ON c.customer_id = o.customer_id
    ORDER BY o.order_id
    """
    result = db.execute_sql(sql)
    print(result)
    
    # 4. CROSS JOIN (Cartesian product)
    print("\n4Ô∏è‚É£ CROSS JOIN - Every customer with every order (Cartesian product)")
    print("-" * 60)
    sql = """
    SELECT 
        c.name,
        o.product
    FROM toy.customers c
    CROSS JOIN toy.orders o
    """
    result = db.execute_sql(sql)
    print(f"Total combinations: {result.count('Product_')} rows")
    print("(Showing first few rows only...)")
    # Show just first few lines
    lines = result.split('\n')[:10]
    print('\n'.join(lines))

def main():
    """Run the toy JOIN example"""
    print("üöÄ AlawymDB Toy JOIN Example")
    print("="*80)
    
    setup_toy_database()
    demonstrate_joins()
    
    print("\n" + "="*80)
    print("‚úÖ Toy JOIN demonstration complete!")

if __name__ == "__main__":
    main()
```

## üéØ Working Example: Analytics with Pandas Integration

```python
import alawymdb as db
import numpy as np
import pandas as pd

# Setup
db.create_database()
db.create_schema("test_schema")

# Create employees table
db.create_table(
    "test_schema",
    "employees",
    [
        ("id", "UINT64", False),
        ("name", "STRING", False),
        ("age", "UINT64", True),
        ("salary", "FLOAT64", True),
        ("department", "STRING", True)
    ]
)

# Insert test data
employees = [
    (1, "Alice Johnson", 28, 75000.0, "Engineering"),
    (2, "Bob Smith", 35, 85000.0, "Sales"),
    (3, "Charlie Brown", 42, 95000.0, "Engineering"),
    (4, "Diana Prince", 31, 78000.0, "Marketing"),
    (5, "Eve Adams", 26, 72000.0, "Sales"),
]

for emp in employees:
    db.insert_row("test_schema", "employees", [
        ("id", emp[0]),
        ("name", emp[1]),
        ("age", emp[2]),
        ("salary", emp[3]),
        ("department", emp[4])
    ])

# Convert to Pandas DataFrame
df = db.to_pandas("test_schema", "employees")
print("DataFrame shape:", df.shape)
print("\nDataFrame head:")
print(df.head())

# Pandas operations
print(f"\nAverage salary: ${df['salary'].mean():,.2f}")
print("\nSalary by department:")
print(df.groupby('department')['salary'].agg(['mean', 'count']))

# Get as NumPy array
ages = db.to_numpy("test_schema", "employees", "age")
print(f"\nAges array: {ages}")
print(f"Mean age: {np.mean(ages):.1f}")

# Get data as dictionary
data_dict = db.select_as_dict("test_schema", "employees")
print(f"\nColumns available: {list(data_dict.keys())}")
```

## üìä Create Table from Pandas DataFrame

```python
import alawymdb as db
import pandas as pd
import numpy as np

db.create_database()
db.create_schema("data")

# Create a DataFrame
df = pd.DataFrame({
    'product_id': np.arange(1, 101),
    'product_name': [f'Product_{i}' for i in range(1, 101)],
    'price': np.random.uniform(10, 100, 100).round(2),
    'quantity': np.random.randint(1, 100, 100),
    'in_stock': np.random.choice([0, 1], 100)  # Use 0/1 instead of True/False
})

# Import DataFrame to AlawymDB
result = db.from_pandas(df, "data", "products")
print(result)

# Verify by reading back
df_verify = db.to_pandas("data", "products")
print(f"Imported {len(df_verify)} rows with {len(df_verify.columns)} columns")
print(df_verify.head())

# Query the imported data
result = db.execute_sql("SELECT * FROM data.products WHERE price > 50.0")
print(f"Products with price > 50: {result}")
```

## üìà Wide Table Example (Working Version)

```python
import alawymdb as db

db.create_database()
db.create_schema("wide")

# Create table with many columns
num_columns = 100
columns = [("id", "UINT64", False)]
columns += [(f"metric_{i}", "FLOAT64", True) for i in range(num_columns)]

db.create_table("wide", "metrics", columns)

# Insert data
for row_id in range(100):
    values = [("id", row_id)]
    values += [(f"metric_{i}", float(row_id * 0.1 + i)) for i in range(num_columns)]
    db.insert_row("wide", "metrics", values)

# Query using direct API (more reliable for wide tables)
all_data = db.select_all("wide", "metrics")
print(f"Inserted {len(all_data)} rows")

# Convert to Pandas for analysis
df = db.to_pandas("wide", "metrics")
print(f"DataFrame shape: {df.shape}")
print(f"Columns: {df.columns[:5].tolist()} ... {df.columns[-5:].tolist()}")

# Get specific column as NumPy array
metric_0 = db.to_numpy("wide", "metrics", "metric_0")
print(f"Metric_0 stats: mean={metric_0.mean():.2f}, std={metric_0.std():.2f}")
```

## üöÄ Performance Test

```python
import alawymdb as db
import pandas as pd
import numpy as np
import time

db.create_database()
db.create_schema("perf")

# Create a large DataFrame
n_rows = 10000
df_large = pd.DataFrame({
    'id': np.arange(n_rows),
    'value1': np.random.randn(n_rows),
    'value2': np.random.randn(n_rows) * 100,
    'category': np.random.choice(['A', 'B', 'C', 'D', 'E'], n_rows),
    'flag': np.random.choice([0, 1], n_rows)
})

# Time the import
start = time.time()
db.from_pandas(df_large, "perf", "large_table")
import_time = time.time() - start
print(f"Import {n_rows} rows: {import_time:.3f}s ({n_rows/import_time:.0f} rows/sec)")

# Time the export
start = time.time()
df_export = db.to_pandas("perf", "large_table")
export_time = time.time() - start
print(f"Export to Pandas: {export_time:.3f}s ({n_rows/export_time:.0f} rows/sec)")

# Verify
print(f"Shape verification: {df_export.shape}")
```

## üèóÔ∏è Why "Almost Linear Any Way You Measure"?

The name AlawymDB reflects our core achievement:
- **Column scaling**: O(n) with tiny logarithmic factor (log‚ÇÇ‚ÇÖ‚ÇÜ)
- **Row scaling**: Pure O(n) for scans
- **Memory usage**: Linear with data size
- **Wide tables**: Tested up to 5000 columns with maintained performance

## üìä Current SQL Support

### ‚úÖ Working SQL Features
- `SELECT * FROM table`
- `SELECT column1, column2 FROM table`
- `SELECT * FROM table WHERE column = value`
- `SELECT * FROM table WHERE column > value`
- **Aggregation Functions**: `COUNT()`, `SUM()`, `AVG()`, `MIN()`, `MAX()`
- **JOIN Operations**: `INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `CROSS JOIN`
- **Set Operations**: `UNION`, `INTERSECT`, `EXCEPT`
- **GROUP BY** with aggregations
- **LIMIT` clause
- **Subqueries** with IN operator

### ‚ö†Ô∏è SQL Limitations
- Type matching is strict (use 50.0 for FLOAT64, 50 for INT64)
- Column aliases (AS) not yet supported
- ORDER BY is not supported by design at the moment
- CASE statements not supported
- String does not guarantee 100% precision (internally treated as u64)

## üé® API Reference

```python
# Core operations
db.create_database(memory_cap_mb=None, disk_gb=None, disk_path=None)
db.create_schema(schema_name)
db.create_table(schema, table, columns)
db.insert_row(schema, table, values)

# Query operations
db.select_all(schema, table)
db.select_where(schema, table, columns, where_col, where_val)
db.count_rows(schema, table)
db.execute_sql(sql_query)  # Full SQL support including JOINs, aggregations, set operations

# Data science integrations
db.to_pandas(schema, table)           # Export to DataFrame
db.to_numpy(schema, table, column)    # Export column to NumPy
db.from_pandas(df, schema, table)     # Import from DataFrame
db.select_as_dict(schema, table)      # Get as Python dict

# Persistence operations
db.save_database(path)                # Save database to disk
db.restore_database(path)             # Restore database from disk
```

## üö¶ Performance Characteristics

| Operation | Complexity | Verified Scale |
|-----------|------------|----------------|
| INSERT | O(1) | 2M rows √ó 2K columns |
| SELECT * | O(n) | 10K rows √ó 5K columns |
| WHERE clause | O(n) | 1M rows tested |
| JOIN | O(n√óm) | Tables up to 100K rows |
| GROUP BY | O(n) | 100K groups tested |
| Aggregations | O(n) | 1M rows tested |
| to_pandas() | O(n) | 100K rows tested |
| from_pandas() | O(n) | 100K rows tested |
| Column scaling | ~O(n) | Up to 5000 columns |
| save_database() | O(n) | Linear with data size |
| restore_database() | O(n) | Linear with data size |

## üìú License

MIT License

---

**AlawymDB**: Almost Linear Any Way You Measure - because performance should scale with your data, not against it.
