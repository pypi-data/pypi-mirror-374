---
title: "run_simulations"
description: "Run multiple AI agent simulations with automated testing and evaluation"
---

## run_simulations

Run multiple AI agent simulations to test and evaluate your agent's performance across various conversation scenarios.

### Function Signature

```python
def run_simulations(
    *,
    target_agent: Callable[[], Callable[[str], Awaitable[str] | str]],
    api_key: str,
    num_simulations: int,
    max_turns: int,
    base_url: Optional[str] = None,
    context: Optional[str] = None,
    goal: Optional[str] = None,
    rules: Optional[Sequence[str]] = None,
    persona_kwargs: Optional[Dict[str, str]] = None,
) -> List[dict]
```



### Basic Usage

```python
import asyncio
import janus_sdk as janus

class MyAgent:
    async def chat(self, prompt: str) -> str:
        # Your agent logic here
        return "Hello! I'm your AI assistant."

async def main():
    results = await janus.run_simulations(
        num_simulations=10,
        max_turns=5,
        target_agent=lambda: MyAgent().chat,
        api_key="your_janus_api_key"  # Contact team@withjanus.com to get your API key
    )
    
    print(f"Completed {len(results)} simulations")

asyncio.run(main())
```

### Advanced Usage with Rules

```python
# Define evaluation rules
rules = [
    "The agent should be helpful and informative",
    "The agent should not provide medical advice",
    "The agent should stay within the conversation context"
]

results = await janus.run_simulations(
    num_simulations=20,
    max_turns=8,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key",  # Contact team@withjanus.com to get your API key
    context="You are a customer service representative testing a new AI assistant.",
    goal="Test the AI assistant's ability to handle customer inquiries professionally.",
    rules=rules
)
```

### Custom Context and Personas

```python
# Customize the testing scenario
results = await janus.run_simulations(
    num_simulations=15,
    max_turns=6,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key",  # Contact team@withjanus.com to get your API key
    context="You are a technical support specialist helping users with software issues.",
    goal="Evaluate the AI assistant's technical knowledge and problem-solving abilities.",
    persona_kwargs={
        "expertise_level": "intermediate",
        "communication_style": "professional"
    }
)
```



### Error Handling

The function handles various error scenarios:

<AccordionGroup>
<Accordion title="Network Errors">
  If the Janus API is unavailable, simulations will fail gracefully with error details in the response.
</Accordion>

<Accordion title="Agent Errors">
  If your agent throws an exception, the simulation will be marked as failed with error information.
</Accordion>

<Accordion title="Timeout Errors">
  If simulations take too long, they will be automatically terminated and marked as timeout.
</Accordion>
</AccordionGroup>

### Performance Considerations

<Tip>
- Start with `num_simulations=10` for quick testing
- Use `max_turns=5` for faster results
- Increase simulation count gradually as you optimize your agent
- Monitor API rate limits for large-scale testing
</Tip>

<Warning>
Running many simulations can be resource-intensive. Monitor your system resources and API usage.
</Warning>

### Webhook Simulation

Test external workflows by simulating webhook triggers to N8N or other workflow engines:

```python
import asyncio
from janus_sdk import run_simulations, create_webhook_target_agent

async def main():
    # Configuration for your workflow engine
    n8n_config = {
        "base_url": "https://your-n8n-instance.com",
        "api_key": "your_api_key_here",
        "workflow_id": "your_workflow_id"
    }
    
    # Webhook payload to trigger the workflow
    webhook_payload = {
        "case_id": "CASE-12345",
        "client_name": "Example Corp",
        "submission_type": "Document Processing",
        "priority": "High"
    }
    
    # Create target agent that triggers webhook
    target_agent = await create_webhook_target_agent(
        n8n_config["base_url"],
        n8n_config["api_key"],
        n8n_config["workflow_id"],
        webhook_payload
    )
    
    # Run simulation with webhook trigger
    results = await run_simulations(
        num_simulations=5,
        max_turns=1,  # Single webhook trigger = 1 turn
        target_agent=target_agent,
        api_key="your_janus_api_key",
        context="Testing workflow execution via webhook simulation",
        goal="Evaluate workflow reliability and performance"
    )

asyncio.run(main())
```

<Info>
Webhook simulation allows you to test entire workflows end-to-end by triggering them via HTTP webhooks, then monitoring their execution and results.
</Info>

### Related Functions

<CardGroup cols={3}>
<Card title="arun_simulations" icon="code" href="/api-reference/arun-simulations">
  Async version of run_simulations for advanced use cases
</Card>

<Card title="track decorator" icon="route" href="/api-reference/track-decorator">
  Automatically trace function calls in your agent
</Card>

<Card title="webhook-trigger" icon="webhook" href="/api-reference/webhook-trigger">
  Create webhook target agents for external workflow testing
</Card>
</CardGroup> 