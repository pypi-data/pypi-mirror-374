---
title: 'Webapp Setup'
description: 'Complete guide to configuring your testing environment in the Janus webapp'
---

# Webapp Setup

The Janus webapp works hand-in-hand with the Python SDK to provide a comprehensive testing environment. While the SDK handles programmatic execution, the webapp handles configuration, visualization, and analysis.

## Overview

The webapp provides three main areas for configuration:
- **Test & Rules**: Define test cases, evaluation rules, and knowledge base
- **Chat**: Configure simulation templates and run chat simulations  
- **Results**: Analyze performance and export data

## Test & Rules Configuration

### Test Cases
Navigate to **Test & Rules → Test Cases** to create and manage your test queries:

- **Create Test Cases**: Add specific queries you want to test against
- **Set Context**: Provide background information and goals for each test case
- **Define Expectations**: Specify what constitutes a successful response
- **Organize by Category**: Group related test cases for better organization

**Example Test Case Structure:**
```
Query: "What are the side effects of medication X?"
Context: Healthcare scenario, patient asking about medication
Goal: Ensure agent provides accurate medical information
Category: Healthcare, Medication Safety
```

### Rule Sets
Go to **Test & Rules → Rule Sets** to define evaluation criteria:

- **Compliance Rules**: Ensure responses follow company policies
- **Quality Checks**: Validate response accuracy and completeness
- **Behavioral Constraints**: Set boundaries for agent behavior
- **Domain-Specific Rules**: Industry-specific requirements

**Common Rule Categories:**
- **Safety**: No medical advice, no harmful content
- **Accuracy**: Must cite sources, no hallucinations
- **Compliance**: Follow regulatory requirements
- **Tone**: Professional, helpful, appropriate

### Knowledge Base
Access **Test & Rules → Knowledge Base** to upload documents for hallucination detection:

- **Document Upload**: Support for PDF, DOCX, TXT, and other formats
- **RAG Hallucination Detection**: Automatically detect when agents make false claims
- **File Management**: Organize and manage uploaded documents
- **Status Tracking**: Monitor upload progress and processing status

**Supported File Types:**
- PDF documents
- Word documents (DOCX)
- Text files (TXT)
- Markdown files (MD)
- CSV files

## Chat Simulation Configuration

### Chat Templates
The **Chat** page is where you configure simulation parameters:

- **Create Templates**: Set up different simulation configurations
- **Configure Personas**: Define user characteristics and behaviors
- **Set Parameters**: Adjust simulation length, complexity, and goals
- **Star Templates**: Mark templates as active for testing

**Template Configuration Options:**
- **Test Cases**: Select specific test cases or randomize
- **Rule Sets**: Choose evaluation criteria for responses
- **User Personas**: Different personality types, knowledge levels
- **Demographics**: Age range (18-100), gender, race, residence area
- **Economic**: Annual income range ($0-$1M)
- **Psychological**: Personality types (MBTI), emotions, urgency levels
- **Communication**: Language proficiency, conversation intensity
- **Behavioral**: Patience level, complaint propensity
- **Occupation**: Custom occupation tags
- **Miscellaneous Notes**: Additional context or constraints
- **CSV Data Integration**: Upload user data for personalized simulations
- **Assignment Strategies**: Sequential, random, or round-robin user assignment
- **RAG Hallucination Judge**: Enable AI-powered factual accuracy verification

**CSV Requirements:**
- Must include a `user_id` column
- Each row becomes a unique synthetic user
- Supports custom fields for personalization

## Running Simulations

### From the SDK
Use the webapp configuration with your SDK code:

```python
await run_simulations(
    target_agent=lambda: MyAgent().chat,
    api_key=os.getenv("JANUS_API_KEY"),
    num_simulations=10,
    max_turns=5,
    # Webapp persona and context data
    persona_kwargs={
        "user_type": "technical_expert",
        "context": "software_development"
    }
)
```

## Results and Analysis

### Viewing Results
Navigate to **Results** to analyze your simulation outcomes:

- **Conversation Logs**: Full transcript of each simulation
- **Performance Metrics**: Response times, token usage, success rates
- **Rule Violations**: Instances where rules were broken
- **Tool Usage**: Detailed tracing of function calls and API usage
- **RAG Hallucinations**: Detected factual inaccuracies against knowledge base

**Available Analysis Tabs:**
- **Transcript**: Full conversation flow with metrics and function calls
- **Rules**: Individual rule violations with context
- **Overall**: Overall rule compliance assessment
- **Functions**: Detailed function call traces and performance
- **Hallucinations**: RAG accuracy verification results
- **Investigation**: Janus's intelligent third-party judging system that autonomously analyzes every simulation to identify potential issues, anomalies, and areas that warrant your attention - think of it as your AI-powered quality assurance team that never sleeps
- **Eval**: Evaluation comparisons (when available)
- **Identity**: User persona and backstory details used in simulation

### Exporting Data
**Evals Export**: Download results in JSON format for:
- Internal benchmarking
- Fine-tuning models
- Compliance reporting
- Performance analysis

**Custom Schemas**: Define your own export format for specific use cases

## Insights and Reporting

### AI Performance Reports
The **Insights** feature provides:

- **Performance Analysis**: Comprehensive evaluation of agent behavior
- **Issue Clustering**: Groups similar problems for easier resolution
- **Recommendations**: Actionable suggestions for improvement
- **Progress Tracking**: Monitor resolved issues over time
- **Trend Analysis**: Track performance over multiple test runs

### Continuous Improvement
- **Benchmarking**: Compare against previous versions
- **A/B Testing**: Evaluate different agent configurations
- **Compliance Monitoring**: Ensure ongoing adherence to rules
- **Performance Trends**: Visualize improvement over time

## Best Practices

### Configuration Strategy
- **Start Simple**: Begin with basic test cases and rules
- **Iterate Gradually**: Add complexity as you understand your needs
- **Document Everything**: Keep clear records of your configuration
- **Regular Reviews**: Periodically assess and update your setup

### Testing Workflow
1. **Configure**: Set up test cases, rules, and knowledge base in the webapp
2. **Develop**: Build and test your agent with the SDK
3. **Run**: Execute simulations using webapp templates
4. **Analyze**: Review results and identify improvements
5. **Iterate**: Refine your agent and configuration


## Troubleshooting

### Common Issues
- **Template Not Working**: Ensure the template is starred and active
- **Rules Not Applied**: Check that rule sets are properly configured
- **Results Missing**: Verify simulations completed successfully
- **Export Errors**: Check data format and permissions
- **Knowledge Base Issues**: Verify document upload and processing status

### Getting Help
- **Documentation**: Review this guide and related tutorials
- **Support**: Contact team@withjanus.com for assistance
