---
title: "Evaluation & Judging"
description: "Learn how Janus evaluates AI agent performance using rule-based judging and hallucination detection"
---

## Rule-Based Evaluation

Janus Python SDK provides comprehensive evaluation of AI agent performance through rule-based judging and hallucination detection. This helps you identify issues, measure quality, and ensure your agents meet your standards.

## How Evaluation Works

### 1. **Rule-Based Judging**
Define custom rules that evaluate each agent response. You can configure rules in the Janus webapp or pass them directly in the SDK:

```python
rules = [
    "The agent should be helpful and informative",
    "The agent should not provide medical advice",
    "The agent should stay within the conversation context",
    "The agent should be polite and professional"
]

results = await janus.run_simulations(
    num_simulations=20,
    max_turns=8,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key",
    rules=rules
)
```

### 2. **Autonomous Investigation**
Janus runs our third-party proprietary judging system that evaluates agent responses for:
- **Usefulness**: How helpful and relevant the response is
- **Accuracy**: Whether the information provided is correct
- **Appropriateness**: Whether the response fits the context
- **Safety**: Whether the response follows safety guidelines

This proprietary judging system allows you to not need to put any rules if you don't know what to test - Janus verifies autonomously. Simply run simulations without specifying rules and our system will automatically evaluate agent performance using our advanced proprietary algorithms.

### 3. **Hallucination Detection**
Detect when agents make claims that can't be verified by uploading your documents to the Janus webapp. In the dashboard at [app.withjanus.com](https://app.withjanus.com), you can upload whatever documents you want to test against, and the RAG hallucination detection will automatically verify agent claims against your knowledge base.

## Evaluation Components

### **Turn-Level Rules**
Rules that apply to each individual response:

<CardGroup cols={2}>
<Card title="Content Rules" icon="file">
  - Accuracy of information
  - Completeness of responses
  - Relevance to the question
</Card>

<Card title="Behavior Rules" icon="user">
  - Politeness and professionalism
  - Helpfulness and engagement
  - Appropriate tone and style
</Card>

<Card title="Safety Rules" icon="shield">
  - No harmful advice
  - No personal information sharing
  - Compliance with guidelines
</Card>

<Card title="Context Rules" icon="brain">
  - Staying on topic
  - Maintaining conversation flow
  - Appropriate level of detail
</Card>
</CardGroup>



## Integration with Janus Platform

### **Frontend Rule Configuration**
Through the Janus dashboard at [app.withjanus.com](https://app.withjanus.com), you can:

1. **Create Rule Sets**: Define comprehensive evaluation criteria
   - Write individual rules for each conversation turn
   - Define overall conversation rules
   - Use pre-built rule templates (polite/helpful, no deflection, user satisfaction)

2. **Upload Knowledge Base**: Enable RAG hallucination detection
   - Upload documents that agents should reference
   - Configure RAG hallucination judge to detect unsupported claims

3. **Test Case Management**: Create structured evaluation scenarios
   - Define test case content and context
   - Upload evaluation datasets for comparison testing





## Evaluation Metrics

### **Rule Violations**
Rules are evaluated as binary violations - either violated or not:
- **Violation Count**: Number of times each rule was broken
- **Context**: The question and answer that triggered the violation
- **Explanation**: Detailed description of why the rule was violated
- **Turn Number**: Which conversation turn contained the violation

### **RAG Hallucination Detection**
Sophisticated scoring through confidence levels and claim verification:
- **Confidence Score**: 0-100% based on AI analysis certainty
- **Claims Checked**: Total number of factual claims analyzed
- **Claims Flagged**: Number of claims identified as potential hallucinations
- **Source Verification**: Claims checked against uploaded knowledge base documents

### **Overall Conversation Rules**
Binary assessment for conversation-wide patterns:
- **Conversation-wide Violations**: Rules that apply to the entire conversation flow
- **Pattern Analysis**: Consistency and structural rule violations
- **Flow Assessment**: Overall conversation quality and adherence

### **Investigation Results**
AI-powered analysis of conversation quality and issues:
- **AI-Powered Issue Detection**: Janus's autonomous system analyzes conversations to identify potential problems you might not have thought to check for
- **Usefulness Filtering**: Only shows actionable issues marked as "useful" to avoid false positives and focus on high-impact findings
- **Turn-Based Organization**: Groups issues by conversation turn with full Q&A context for easy navigation and problem identification
- **Categorical Scoring**: Uses descriptive labels and reasoning instead of numerical scores, focusing on issue type and explanation rather than pass/fail metrics

## Best Practices

<Tip>
- Start with a few key rules and expand gradually
- Use specific, measurable rules rather than vague guidelines
- Test rules with a small number of simulations first
- Monitor false positives and adjust rules accordingly
- Combine rule-based evaluation with autonomous judging for comprehensive assessment
</Tip>

<Warning>
Rules should be clear and unambiguous. Vague rules may lead to inconsistent evaluation results.
</Warning>



 