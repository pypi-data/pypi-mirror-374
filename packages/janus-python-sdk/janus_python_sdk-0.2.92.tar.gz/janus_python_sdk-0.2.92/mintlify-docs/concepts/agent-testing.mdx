---
title: "Agent Testing"
description: "Learn how Janus automatically tests your AI agents through conversation simulations"
---

## Automated Agent Testing

Janus Python SDK provides comprehensive automated testing for AI agents through conversation simulations. Instead of manually creating test scenarios, Janus automatically generates diverse conversation patterns to stress-test your agent.

## How It Works

### 1. **Conversation Simulation**
Janus creates realistic conversation scenarios by simulating different user personas interacting with your agent:

```python
await janus.run_simulations(
    num_simulations=10,  # Run 10 different conversations
    max_turns=5,         # Each conversation has up to 5 turns
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key"
)
```

### 2. **Diverse Personas**
Each simulation uses different personas with varying characteristics based on the template you define in the Janus webapp:
- **Demographics**: Age, income, gender, location
- **Behavioral traits**: Conversation intensity, patience, complaint propensity
- **Technical expertise**: Beginner to expert levels
- **Communication styles**: Formal, casual, urgent, relaxed

<Card
  title="Webapp Setup Guide"
  icon="book"
  href="/concepts/webapp-setup"
>
  Learn how to configure simulation templates and personas in the webapp
</Card>

### 3. **Systematic Testing**
Janus systematically tests your agent across multiple dimensions:

<CardGroup cols={2}>
<Card title="Response Quality" icon="star">
  Evaluates helpfulness, accuracy, and relevance of responses
</Card>

<Card title="Policy Compliance" icon="shield">
  Checks adherence to safety guidelines and business rules
</Card>

<Card title="Context Awareness" icon="brain">
  Tests ability to maintain conversation context
</Card>

<Card title="Error Handling" icon="info">
  Identifies how agent handles edge cases and errors
</Card>
</CardGroup>

## Testing Workflow

### Step 1: Define Your Agent
Create an agent class with a `chat` method that Janus can test:

```python
class MyAgent:
    def __init__(self):
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    async def chat(self, prompt: str) -> str:
        """This method will be tested by Janus"""
        response = await self.client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

### Step 2: Configure Your Testing Environment (Webapp)
Before running simulations, you need to set up your testing environment in the Janus webapp:

1. **Create Test Cases**: Define specific queries and scenarios to test
2. **Set Up Rule Sets**: Configure evaluation criteria and compliance rules
3. **Create Simulation Templates**: Design personas and conversation parameters

<Card
  title="Webapp Setup Guide"
  icon="settings"
  href="/concepts/webapp-setup"
>
  Learn how to configure test cases, rules, and simulation templates
</Card>

### Step 3: Configure Test Parameters
Set up your testing configuration:

```python
# Basic testing
results = await janus.run_simulations(
    num_simulations=20,
    max_turns=8,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key"
)

# Advanced testing with custom context
results = await janus.run_simulations(
    num_simulations=50,
    max_turns=10,
    target_agent=lambda: MyAgent().chat,
    api_key="your_janus_api_key",
    context="You are a customer service representative testing a new AI assistant.",
    goal="Evaluate the AI assistant's ability to handle customer inquiries professionally."
)
```

### Step 4: Analyze Results
View comprehensive results in the Janus dashboard at [app.withjanus.com/dashboard/results](https://app.withjanus.com/dashboard/results):

- **Conversation Transcripts**: Full conversation flow with timestamps
- **Rule Violations**: Specific rules violated in each turn
- **RAG Hallucinations**: Claims that couldn't be verified against knowledge base
- **Investigation Results**: Autonomous judging and usefulness assessments
- **Performance Traces**: Technical execution details and metrics

<Card
  title="View Results"
  icon="chart-line"
  href="https://app.withjanus.com/dashboard/results"
>
  Check your simulation results in the Janus dashboard
</Card>

## Best Practices

<Tip>
- Start with 10-20 simulations for initial testing
- Increase simulation count for comprehensive evaluation
- Use custom context to match your specific use case
- Monitor results in the Janus dashboard for detailed insights
</Tip>

<Warning>
Running many simulations can be resource-intensive. Monitor your API usage and system resources.
</Warning>

## Example: Customer Service Testing

```python
# Test a customer service agent
results = await janus.run_simulations(
    num_simulations=30,
    max_turns=6,
    target_agent=lambda: CustomerServiceAgent().chat,
    api_key="your_janus_api_key",
    context="You are a customer testing a new AI customer service assistant.",
    goal="Test the AI assistant's ability to handle customer inquiries and resolve issues.",
    # Custom hardcoded rules for evaluation
    rules=[
        "The agent should be polite and professional",
        "The agent should provide accurate information",
        "The agent should escalate complex issues appropriately"
    ]
)
```

## Next Steps

<CardGroup cols={2}>
<Card title="Function Tracing" icon="route" href="/concepts/tracing">
  Learn how to trace function calls and tool usage
</Card>

<Card title="Evaluation Rules" icon="shield" href="/concepts/evaluation">
  Understand how rule-based evaluation works
</Card>

<Card title="Quickstart Guide" icon="rocket" href="/quickstart">
  Run your first agent test
</Card>

<Card title="API Reference" icon="code" href="/api-reference/run-simulations">
  Detailed API documentation
</Card>
</CardGroup> 