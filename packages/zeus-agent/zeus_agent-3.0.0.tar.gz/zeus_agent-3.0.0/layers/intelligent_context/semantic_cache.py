"""
è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿ - æ™ºèƒ½æŸ¥è¯¢ç¼“å­˜å’Œå“åº”å¤ç”¨

åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼Œå¯ä»¥æ˜¾è‘—é™ä½æˆæœ¬å’Œæå‡å“åº”é€Ÿåº¦ã€‚
æ”¯æŒç”¨æˆ·è§’è‰²å…¼å®¹æ€§æ£€æŸ¥ã€ç¼“å­˜æ–°é²œåº¦ç®¡ç†ã€æ™ºèƒ½ç¼“å­˜ç­–ç•¥ç­‰åŠŸèƒ½ã€‚

Author: ADC Team
Date: 2024-12-19
Version: 1.0.0
"""

import asyncio
import hashlib
import json
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
import logging
from collections import defaultdict

from .vector_database_service import VectorDatabaseService
from .embedding_service import EmbeddingService
from ..framework.abstractions.knowledge_based_agent import UserRole

logger = logging.getLogger(__name__)

class CachePriority(Enum):
    """ç¼“å­˜ä¼˜å…ˆçº§"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class CacheStatus(Enum):
    """ç¼“å­˜çŠ¶æ€"""
    ACTIVE = "active"
    EXPIRED = "expired"
    INVALIDATED = "invalidated"
    PENDING = "pending"

@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    entry_id: str
    original_query: str
    response: str
    user_role: UserRole
    timestamp: datetime
    ttl_seconds: int = 3600 * 24 * 7  # é»˜è®¤7å¤©
    access_count: int = 0
    last_access: Optional[datetime] = None
    priority: CachePriority = CachePriority.MEDIUM
    status: CacheStatus = CacheStatus.ACTIVE
    metadata: Dict[str, Any] = field(default_factory=dict)
    quality_score: float = 0.0
    cost_saved: float = 0.0
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.status != CacheStatus.ACTIVE:
            return True
        return datetime.now() > self.timestamp + timedelta(seconds=self.ttl_seconds)
    
    def is_compatible_with_user(self, user_role: UserRole) -> bool:
        """æ£€æŸ¥ä¸ç”¨æˆ·è§’è‰²çš„å…¼å®¹æ€§"""
        # ä¸“å®¶çš„ç¼“å­˜å¯ä»¥ç»™æ‰€æœ‰äººç”¨
        if self.user_role == UserRole.EXPERT:
            return True
        
        # ç ”ç©¶è€…çš„ç¼“å­˜å¯ä»¥ç»™ä¸“å®¶å’Œç ”ç©¶è€…ç”¨
        if self.user_role == UserRole.RESEARCHER:
            return user_role in [UserRole.EXPERT, UserRole.RESEARCHER]
        
        # ä¸­çº§ç”¨æˆ·çš„ç¼“å­˜å¯ä»¥ç»™ä¸­çº§å’Œåˆå­¦è€…ç”¨
        if self.user_role == UserRole.INTERMEDIATE:
            return user_role in [UserRole.INTERMEDIATE, UserRole.BEGINNER]
        
        # åˆå­¦è€…çš„ç¼“å­˜åªèƒ½ç»™åˆå­¦è€…ç”¨
        return self.user_role == user_role
    
    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.access_count += 1
        self.last_access = datetime.now()

@dataclass
class CacheHitResult:
    """ç¼“å­˜å‘½ä¸­ç»“æœ"""
    hit: bool
    entry: Optional[CacheEntry] = None
    similarity_score: float = 0.0
    reason: str = ""
    cost_saved: float = 0.0

@dataclass
class CacheStats:
    """ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    total_entries: int = 0
    active_entries: int = 0
    expired_entries: int = 0
    hit_count: int = 0
    miss_count: int = 0
    total_cost_saved: float = 0.0
    average_similarity: float = 0.0
    cache_size_mb: float = 0.0
    
    @property
    def hit_rate(self) -> float:
        total = self.hit_count + self.miss_count
        return self.hit_count / total if total > 0 else 0.0
    
    @property
    def cost_efficiency(self) -> float:
        return self.total_cost_saved / max(self.total_entries, 1)

class SemanticCache:
    """è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(
        self,
        vector_db_service: Optional[VectorDatabaseService] = None,
        embedding_service: Optional[EmbeddingService] = None,
        similarity_threshold: float = 0.85,
        max_cache_size: int = 10000,
        cleanup_interval_hours: int = 24
    ):
        self.vector_db = vector_db_service or VectorDatabaseService()
        self.embedding_service = embedding_service or EmbeddingService()
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        self.cleanup_interval_hours = cleanup_interval_hours
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = CacheStats()
        self.cache_entries: Dict[str, CacheEntry] = {}
        
        # è§’è‰²å…¼å®¹æ€§çŸ©é˜µ
        self.role_compatibility = {
            UserRole.EXPERT: [UserRole.EXPERT, UserRole.RESEARCHER, UserRole.INTERMEDIATE, UserRole.BEGINNER],
            UserRole.RESEARCHER: [UserRole.EXPERT, UserRole.RESEARCHER],
            UserRole.INTERMEDIATE: [UserRole.INTERMEDIATE, UserRole.BEGINNER],
            UserRole.BEGINNER: [UserRole.BEGINNER]
        }
        
        logger.info(f"ğŸ¯ è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ - ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
    
    async def initialize(self):
        """åˆå§‹åŒ–ç¼“å­˜ç³»ç»Ÿ"""
        try:
            # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
            await self.vector_db.initialize()
            
            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            await self.embedding_service.initialize()
            
            # åˆ›å»ºç¼“å­˜é›†åˆ
            await self.vector_db.create_collection("semantic_cache")
            
            # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
            asyncio.create_task(self._periodic_cleanup())
            
            logger.info("âœ… è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def get_cached_response(
        self,
        query: str,
        user_role: UserRole,
        min_similarity: Optional[float] = None
    ) -> CacheHitResult:
        """è·å–ç¼“å­˜å“åº”"""
        try:
            threshold = min_similarity or self.similarity_threshold
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = await self.embedding_service.generate_embeddings([query])
            if not query_embedding:
                return CacheHitResult(hit=False, reason="åµŒå…¥ç”Ÿæˆå¤±è´¥")
            
            # åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼æŸ¥è¯¢
            similar_results = await self.vector_db.similarity_search(
                collection_name="semantic_cache",
                query_embedding=query_embedding[0],
                top_k=10,
                threshold=threshold
            )
            
            if not similar_results:
                self.stats.miss_count += 1
                return CacheHitResult(hit=False, reason="æœªæ‰¾åˆ°ç›¸ä¼¼ç¼“å­˜")
            
            # æ£€æŸ¥ç¼“å­˜æ¡ç›®çš„æœ‰æ•ˆæ€§å’Œå…¼å®¹æ€§
            for result in similar_results:
                entry_id = result.get('metadata', {}).get('entry_id')
                if not entry_id or entry_id not in self.cache_entries:
                    continue
                
                entry = self.cache_entries[entry_id]
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if entry.is_expired():
                    continue
                
                # æ£€æŸ¥ç”¨æˆ·è§’è‰²å…¼å®¹æ€§
                if not entry.is_compatible_with_user(user_role):
                    continue
                
                # æ‰¾åˆ°æœ‰æ•ˆçš„ç¼“å­˜æ¡ç›®
                entry.update_access()
                self.stats.hit_count += 1
                self.stats.total_cost_saved += entry.cost_saved
                
                logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {query[:50]}... -> ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
                
                return CacheHitResult(
                    hit=True,
                    entry=entry,
                    similarity_score=result['similarity'],
                    reason="ç¼“å­˜å‘½ä¸­",
                    cost_saved=entry.cost_saved
                )
            
            self.stats.miss_count += 1
            return CacheHitResult(hit=False, reason="æ— å…¼å®¹çš„æœ‰æ•ˆç¼“å­˜")
            
        except Exception as e:
            logger.error(f"âŒ è·å–ç¼“å­˜å“åº”å¤±è´¥: {e}")
            self.stats.miss_count += 1
            return CacheHitResult(hit=False, reason=f"ç¼“å­˜æŸ¥è¯¢é”™è¯¯: {e}")
    
    async def cache_response(
        self,
        query: str,
        response: str,
        user_role: UserRole,
        cost_saved: float = 0.0,
        quality_score: float = 0.0,
        priority: CachePriority = CachePriority.MEDIUM,
        ttl_seconds: Optional[int] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """ç¼“å­˜å“åº”"""
        try:
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥ç¼“å­˜
            should_cache, final_priority = await self._should_cache_response(
                query, response, cost_saved, quality_score, priority
            )
            
            if not should_cache:
                logger.debug(f"ğŸš« è·³è¿‡ç¼“å­˜: {query[:50]}...")
                return False
            
            # ç”Ÿæˆç¼“å­˜æ¡ç›®ID
            entry_id = self._generate_entry_id(query, user_role)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if entry_id in self.cache_entries:
                logger.debug(f"ğŸ”„ æ›´æ–°å·²å­˜åœ¨çš„ç¼“å­˜æ¡ç›®: {entry_id}")
                existing_entry = self.cache_entries[entry_id]
                existing_entry.response = response
                existing_entry.timestamp = datetime.now()
                existing_entry.quality_score = max(existing_entry.quality_score, quality_score)
                return True
            
            # åˆ›å»ºæ–°çš„ç¼“å­˜æ¡ç›®
            cache_entry = CacheEntry(
                entry_id=entry_id,
                original_query=query,
                response=response,
                user_role=user_role,
                timestamp=datetime.now(),
                ttl_seconds=ttl_seconds or (3600 * 24 * 7),  # é»˜è®¤7å¤©
                priority=final_priority,
                quality_score=quality_score,
                cost_saved=cost_saved,
                metadata=metadata or {}
            )
            
            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embedding = await self.embedding_service.generate_embeddings([query])
            if not query_embedding:
                logger.error("âŒ æ— æ³•ç”ŸæˆæŸ¥è¯¢åµŒå…¥ï¼Œç¼“å­˜å¤±è´¥")
                return False
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            doc_metadata = {
                'entry_id': entry_id,
                'user_role': user_role.value,
                'timestamp': cache_entry.timestamp.isoformat(),
                'priority': final_priority.value,
                'quality_score': quality_score,
                'cost_saved': cost_saved
            }
            
            success = await self.vector_db.add_document(
                collection_name="semantic_cache",
                document=query,
                embedding=query_embedding[0],
                metadata=doc_metadata,
                doc_id=entry_id
            )
            
            if success:
                # å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
                self.cache_entries[entry_id] = cache_entry
                self.stats.total_entries += 1
                self.stats.active_entries += 1
                
                logger.info(f"âœ… ç¼“å­˜æˆåŠŸ: {query[:50]}... (ID: {entry_id})")
                
                # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
                await self._enforce_cache_size_limit()
                
                return True
            else:
                logger.error("âŒ å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜å“åº”å¤±è´¥: {e}")
            return False
    
    async def invalidate_cache(
        self,
        query_pattern: Optional[str] = None,
        user_role: Optional[UserRole] = None,
        older_than_hours: Optional[int] = None
    ) -> int:
        """æ— æ•ˆåŒ–ç¼“å­˜"""
        invalidated_count = 0
        
        try:
            entries_to_invalidate = []
            
            for entry_id, entry in self.cache_entries.items():
                should_invalidate = False
                
                # æŒ‰æŸ¥è¯¢æ¨¡å¼è¿‡æ»¤
                if query_pattern and query_pattern.lower() in entry.original_query.lower():
                    should_invalidate = True
                
                # æŒ‰ç”¨æˆ·è§’è‰²è¿‡æ»¤
                if user_role and entry.user_role == user_role:
                    should_invalidate = True
                
                # æŒ‰æ—¶é—´è¿‡æ»¤
                if older_than_hours:
                    cutoff_time = datetime.now() - timedelta(hours=older_than_hours)
                    if entry.timestamp < cutoff_time:
                        should_invalidate = True
                
                if should_invalidate:
                    entries_to_invalidate.append(entry_id)
            
            # æ‰§è¡Œæ— æ•ˆåŒ–
            for entry_id in entries_to_invalidate:
                entry = self.cache_entries[entry_id]
                entry.status = CacheStatus.INVALIDATED
                
                # ä»å‘é‡æ•°æ®åº“ä¸­åˆ é™¤
                await self.vector_db.delete_document("semantic_cache", entry_id)
                
                # ä»å†…å­˜ä¸­åˆ é™¤
                del self.cache_entries[entry_id]
                
                invalidated_count += 1
                self.stats.active_entries -= 1
            
            logger.info(f"ğŸ—‘ï¸ æ— æ•ˆåŒ–äº† {invalidated_count} ä¸ªç¼“å­˜æ¡ç›®")
            return invalidated_count
            
        except Exception as e:
            logger.error(f"âŒ æ— æ•ˆåŒ–ç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    async def get_cache_stats(self) -> CacheStats:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        active_count = sum(1 for entry in self.cache_entries.values() 
                          if entry.status == CacheStatus.ACTIVE and not entry.is_expired())
        
        expired_count = sum(1 for entry in self.cache_entries.values() 
                           if entry.is_expired())
        
        total_cost_saved = sum(entry.cost_saved * entry.access_count 
                              for entry in self.cache_entries.values())
        
        # ä¼°ç®—ç¼“å­˜å¤§å°
        cache_size_mb = len(json.dumps([entry.__dict__ for entry in self.cache_entries.values()])) / (1024 * 1024)
        
        self.stats.active_entries = active_count
        self.stats.expired_entries = expired_count
        self.stats.total_cost_saved = total_cost_saved
        self.stats.cache_size_mb = cache_size_mb
        
        return self.stats
    
    async def optimize_cache(self) -> Dict[str, Any]:
        """ä¼˜åŒ–ç¼“å­˜"""
        optimization_result = {
            'cleaned_expired': 0,
            'promoted_entries': 0,
            'demoted_entries': 0,
            'recommendations': []
        }
        
        try:
            # æ¸…ç†è¿‡æœŸæ¡ç›®
            expired_count = await self._cleanup_expired_entries()
            optimization_result['cleaned_expired'] = expired_count
            
            # è°ƒæ•´ç¼“å­˜ä¼˜å…ˆçº§
            promoted, demoted = await self._adjust_cache_priorities()
            optimization_result['promoted_entries'] = promoted
            optimization_result['demoted_entries'] = demoted
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = await self._generate_optimization_recommendations()
            optimization_result['recommendations'] = recommendations
            
            logger.info(f"ğŸ”§ ç¼“å­˜ä¼˜åŒ–å®Œæˆ: {optimization_result}")
            return optimization_result
            
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜ä¼˜åŒ–å¤±è´¥: {e}")
            return optimization_result
    
    # ç§æœ‰æ–¹æ³•
    def _generate_entry_id(self, query: str, user_role: UserRole) -> str:
        """ç”Ÿæˆç¼“å­˜æ¡ç›®ID"""
        content = f"{query}_{user_role.value}_{datetime.now().date().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    async def _should_cache_response(
        self,
        query: str,
        response: str,
        cost_saved: float,
        quality_score: float,
        priority: CachePriority
    ) -> Tuple[bool, CachePriority]:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç¼“å­˜å“åº”"""
        
        # é«˜æˆæœ¬å“åº”å¿…é¡»ç¼“å­˜
        if cost_saved > 0.5:
            return True, CachePriority.HIGH
        
        # é«˜è´¨é‡å“åº”ä¼˜å…ˆç¼“å­˜
        if quality_score > 0.8:
            return True, max(priority, CachePriority.MEDIUM)
        
        # é•¿å“åº”ï¼ˆå¯èƒ½åŒ…å«ä¸°å¯Œä¿¡æ¯ï¼‰ä¼˜å…ˆç¼“å­˜
        if len(response) > 500:
            return True, priority
        
        # å¸¸è§æŸ¥è¯¢æ¨¡å¼
        common_patterns = ['ä»€ä¹ˆæ˜¯', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆæ ·', 'åŒºåˆ«', 'ä¼˜ç¼ºç‚¹']
        if any(pattern in query for pattern in common_patterns):
            return True, CachePriority.MEDIUM
        
        # é»˜è®¤ç¼“å­˜ï¼Œä½†ä¼˜å…ˆçº§è¾ƒä½
        return True, CachePriority.LOW
    
    async def _enforce_cache_size_limit(self):
        """å¼ºåˆ¶æ‰§è¡Œç¼“å­˜å¤§å°é™åˆ¶"""
        if len(self.cache_entries) <= self.max_cache_size:
            return
        
        # è®¡ç®—éœ€è¦åˆ é™¤çš„æ¡ç›®æ•°
        entries_to_remove = len(self.cache_entries) - self.max_cache_size
        
        # æŒ‰ä¼˜å…ˆçº§å’Œè®¿é—®é¢‘ç‡æ’åº
        entries_sorted = sorted(
            self.cache_entries.items(),
            key=lambda x: (
                x[1].priority.value,  # ä¼˜å…ˆçº§
                x[1].access_count,    # è®¿é—®æ¬¡æ•°
                x[1].timestamp        # æ—¶é—´æˆ³
            )
        )
        
        # åˆ é™¤ä¼˜å…ˆçº§æœ€ä½çš„æ¡ç›®
        for i in range(entries_to_remove):
            entry_id, entry = entries_sorted[i]
            
            # ä»å‘é‡æ•°æ®åº“åˆ é™¤
            await self.vector_db.delete_document("semantic_cache", entry_id)
            
            # ä»å†…å­˜åˆ é™¤
            del self.cache_entries[entry_id]
            self.stats.active_entries -= 1
        
        logger.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {entries_to_remove} ä¸ªä½ä¼˜å…ˆçº§ç¼“å­˜æ¡ç›®")
    
    async def _cleanup_expired_entries(self) -> int:
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        expired_entries = []
        
        for entry_id, entry in self.cache_entries.items():
            if entry.is_expired():
                expired_entries.append(entry_id)
        
        for entry_id in expired_entries:
            # ä»å‘é‡æ•°æ®åº“åˆ é™¤
            await self.vector_db.delete_document("semantic_cache", entry_id)
            
            # ä»å†…å­˜åˆ é™¤
            del self.cache_entries[entry_id]
            self.stats.active_entries -= 1
            self.stats.expired_entries += 1
        
        return len(expired_entries)
    
    async def _adjust_cache_priorities(self) -> Tuple[int, int]:
        """è°ƒæ•´ç¼“å­˜ä¼˜å…ˆçº§"""
        promoted = demoted = 0
        
        for entry in self.cache_entries.values():
            old_priority = entry.priority
            
            # é«˜è®¿é—®é¢‘ç‡ -> æå‡ä¼˜å…ˆçº§
            if entry.access_count > 10 and entry.priority == CachePriority.LOW:
                entry.priority = CachePriority.MEDIUM
                promoted += 1
            elif entry.access_count > 50 and entry.priority == CachePriority.MEDIUM:
                entry.priority = CachePriority.HIGH
                promoted += 1
            
            # ä½è®¿é—®é¢‘ç‡ -> é™ä½ä¼˜å…ˆçº§
            elif entry.access_count < 2 and entry.priority == CachePriority.HIGH:
                entry.priority = CachePriority.MEDIUM
                demoted += 1
            elif entry.access_count < 1 and entry.priority == CachePriority.MEDIUM:
                entry.priority = CachePriority.LOW
                demoted += 1
        
        return promoted, demoted
    
    async def _generate_optimization_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æå‘½ä¸­ç‡
        if self.stats.hit_rate < 0.3:
            recommendations.append("å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–å¢åŠ ç¼“å­˜æ—¶é—´")
        
        # åˆ†æç¼“å­˜å¤§å°
        if self.stats.cache_size_mb > 100:
            recommendations.append("ç¼“å­˜å ç”¨ç©ºé—´è¾ƒå¤§ï¼Œå»ºè®®æ¸…ç†ä½ä»·å€¼ç¼“å­˜æˆ–å‹ç¼©å“åº”å†…å®¹")
        
        # åˆ†ææˆæœ¬æ•ˆç›Š
        if self.stats.cost_efficiency < 0.1:
            recommendations.append("ç¼“å­˜æˆæœ¬æ•ˆç›Šè¾ƒä½ï¼Œå»ºè®®ä¼˜åŒ–ç¼“å­˜ç­–ç•¥")
        
        return recommendations
    
    async def _periodic_cleanup(self):
        """å®šæœŸæ¸…ç†ä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(self.cleanup_interval_hours * 3600)
                
                logger.info("ğŸ§¹ å¼€å§‹å®šæœŸç¼“å­˜æ¸…ç†")
                
                # æ¸…ç†è¿‡æœŸæ¡ç›®
                expired_count = await self._cleanup_expired_entries()
                
                # ä¼˜åŒ–ç¼“å­˜
                await self.optimize_cache()
                
                logger.info(f"ğŸ§¹ å®šæœŸæ¸…ç†å®Œæˆï¼Œæ¸…ç†äº† {expired_count} ä¸ªè¿‡æœŸæ¡ç›®")
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸæ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
                await asyncio.sleep(3600)  # å‡ºé”™å1å°æ—¶åé‡è¯•

# å·¥å‚å‡½æ•°
def create_semantic_cache(
    similarity_threshold: float = 0.85,
    max_cache_size: int = 10000
) -> SemanticCache:
    """åˆ›å»ºè¯­ä¹‰ç¼“å­˜å®ä¾‹"""
    return SemanticCache(
        similarity_threshold=similarity_threshold,
        max_cache_size=max_cache_size
    ) 