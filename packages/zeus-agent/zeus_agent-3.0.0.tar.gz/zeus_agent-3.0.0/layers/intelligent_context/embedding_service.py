"""
EmbeddingæœåŠ¡
é›†æˆsentence-transformersï¼Œæä¾›æ–‡æœ¬å‘é‡åŒ–èƒ½åŠ›
"""

import logging
import asyncio
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
import hashlib
import pickle
import os
from pathlib import Path

import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)


@dataclass
class EmbeddingConfig:
    """Embeddingé…ç½®"""
    model_name: str = "all-MiniLM-L6-v2"  # é»˜è®¤è½»é‡çº§æ¨¡å‹
    cache_dir: str = "./data/embeddings_cache"
    max_sequence_length: int = 512
    batch_size: int = 32
    device: str = "auto"  # auto, cpu, cuda
    normalize_embeddings: bool = True


@dataclass
class EmbeddingResult:
    """Embeddingç»“æœ"""
    text: str
    embedding: List[float]
    model_name: str
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


class EmbeddingService:
    """
    EmbeddingæœåŠ¡
    
    æä¾›æ–‡æœ¬å‘é‡åŒ–åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œç¼“å­˜æœºåˆ¶
    """
    
    # æ”¯æŒçš„æ¨¡å‹é…ç½®
    SUPPORTED_MODELS = {
        # é€šç”¨è‹±æ–‡æ¨¡å‹
        "all-MiniLM-L6-v2": {
            "description": "è½»é‡çº§é€šç”¨æ¨¡å‹ï¼Œ384ç»´",
            "dimensions": 384,
            "max_seq_length": 256,
            "language": "en",
            "size": "80MB"
        },
        "all-mpnet-base-v2": {
            "description": "é«˜è´¨é‡é€šç”¨æ¨¡å‹ï¼Œ768ç»´",
            "dimensions": 768,
            "max_seq_length": 384,
            "language": "en", 
            "size": "420MB"
        },
        # å¤šè¯­è¨€æ¨¡å‹
        "paraphrase-multilingual-MiniLM-L12-v2": {
            "description": "å¤šè¯­è¨€è½»é‡çº§æ¨¡å‹ï¼Œ384ç»´",
            "dimensions": 384,
            "max_seq_length": 128,
            "language": "multilingual",
            "size": "420MB"
        },
        "paraphrase-multilingual-mpnet-base-v2": {
            "description": "å¤šè¯­è¨€é«˜è´¨é‡æ¨¡å‹ï¼Œ768ç»´", 
            "dimensions": 768,
            "max_seq_length": 256,
            "language": "multilingual",
            "size": "970MB"
        },
        # ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
        "shibing624/text2vec-base-chinese": {
            "description": "ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ï¼Œ768ç»´",
            "dimensions": 768,
            "max_seq_length": 256,
            "language": "zh",
            "size": "400MB"
        }
    }
    
    def __init__(self, config: EmbeddingConfig = None):
        """åˆå§‹åŒ–EmbeddingæœåŠ¡"""
        self.config = config or EmbeddingConfig()
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        Path(self.config.cache_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = None
        self.current_model_name = None
        
        # ç¼“å­˜
        self._embedding_cache = {}
        self._cache_file = os.path.join(self.config.cache_dir, "embedding_cache.pkl")
        self._load_cache()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "embeddings_generated": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_texts_processed": 0,
            "average_embedding_time": 0.0
        }
        
        # ä¸åœ¨æ„é€ å‡½æ•°ä¸­è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œç­‰å¾…æ˜¾å¼åˆå§‹åŒ–
    
    async def initialize(self, model_name: str = None) -> bool:
        """åˆå§‹åŒ–åµŒå…¥æœåŠ¡ï¼ŒåŠ è½½æŒ‡å®šæ¨¡å‹"""
        model_name = model_name or self.config.model_name
        return await self._load_model(model_name)
    
    def _get_cache_key(self, text: str, model_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{model_name}:{text}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜"""
        try:
            if os.path.exists(self._cache_file):
                with open(self._cache_file, 'rb') as f:
                    self._embedding_cache = pickle.load(f)
                logger.info(f"âœ… å·²åŠ è½½ {len(self._embedding_cache)} ä¸ªç¼“å­˜embedding")
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
            self._embedding_cache = {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self._cache_file, 'wb') as f:
                pickle.dump(self._embedding_cache, f)
        except Exception as e:
            logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    async def _load_model(self, model_name: str) -> bool:
        """åŠ è½½æ¨¡å‹"""
        try:
            if self.current_model_name == model_name and self.model is not None:
                return True
            
            logger.info(f"ğŸ”„ æ­£åœ¨åŠ è½½embeddingæ¨¡å‹: {model_name}")
            
            # ç¡®å®šè®¾å¤‡
            device = self.config.device
            if device == "auto":
                device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # åŠ è½½æ¨¡å‹
            self.model = SentenceTransformer(
                model_name,
                device=device,
                cache_folder=os.path.join(self.config.cache_dir, "models")
            )
            
            # è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦
            if hasattr(self.model, 'max_seq_length'):
                self.model.max_seq_length = self.config.max_sequence_length
            
            self.current_model_name = model_name
            
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name} (è®¾å¤‡: {device})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    async def embed_text(self, 
                        text: str, 
                        model_name: str = None,
                        use_cache: bool = True) -> EmbeddingResult:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            model_name: æ¨¡å‹åç§°ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            Embeddingç»“æœ
        """
        if not text.strip():
            raise ValueError("è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        model_name = model_name or self.config.model_name
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(text, model_name)
        if use_cache and cache_key in self._embedding_cache:
            self.stats["cache_hits"] += 1
            cached_result = self._embedding_cache[cache_key]
            logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {text[:50]}...")
            return EmbeddingResult(
                text=text,
                embedding=cached_result["embedding"],
                model_name=model_name,
                metadata={"from_cache": True}
            )
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not await self._load_model(model_name):
            raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹: {model_name}")
        
        try:
            start_time = datetime.now()
            
            # ç”Ÿæˆembedding
            embedding = self.model.encode(
                text,
                normalize_embeddings=self.config.normalize_embeddings,
                convert_to_tensor=False
            )
            
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
            if isinstance(embedding, np.ndarray):
                embedding = embedding.tolist()
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stats["embeddings_generated"] += 1
            self.stats["cache_misses"] += 1
            self.stats["total_texts_processed"] += 1
            
            # æ›´æ–°å¹³å‡å¤„ç†æ—¶é—´
            total_time = (self.stats["average_embedding_time"] * (self.stats["embeddings_generated"] - 1) + processing_time)
            self.stats["average_embedding_time"] = total_time / self.stats["embeddings_generated"]
            
            # åˆ›å»ºç»“æœ
            result = EmbeddingResult(
                text=text,
                embedding=embedding,
                model_name=model_name,
                metadata={
                    "processing_time": processing_time,
                    "dimensions": len(embedding),
                    "from_cache": False
                }
            )
            
            # ç¼“å­˜ç»“æœ
            if use_cache:
                self._embedding_cache[cache_key] = {
                    "embedding": embedding,
                    "created_at": datetime.now().isoformat()
                }
                
                # å®šæœŸä¿å­˜ç¼“å­˜
                if len(self._embedding_cache) % 100 == 0:
                    self._save_cache()
            
            logger.debug(f"âœ… Embeddingç”Ÿæˆå®Œæˆ: {text[:50]}... ({processing_time:.3f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Embeddingç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def embed_texts(self, 
                         texts: List[str], 
                         model_name: str = None,
                         use_cache: bool = True) -> List[EmbeddingResult]:
        """
        æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model_name: æ¨¡å‹åç§°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            
        Returns:
            Embeddingç»“æœåˆ—è¡¨
        """
        if not texts:
            return []
        
        model_name = model_name or self.config.model_name
        results = []
        
        # åˆ†ç¦»ç¼“å­˜å‘½ä¸­å’Œæœªå‘½ä¸­çš„æ–‡æœ¬
        cached_results = {}
        texts_to_process = []
        
        if use_cache:
            for i, text in enumerate(texts):
                cache_key = self._get_cache_key(text, model_name)
                if cache_key in self._embedding_cache:
                    cached_results[i] = self._embedding_cache[cache_key]
                    self.stats["cache_hits"] += 1
                else:
                    texts_to_process.append((i, text))
                    self.stats["cache_misses"] += 1
        else:
            texts_to_process = list(enumerate(texts))
        
        # å¤„ç†æœªç¼“å­˜çš„æ–‡æœ¬
        if texts_to_process:
            # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
            if not await self._load_model(model_name):
                raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹: {model_name}")
            
            try:
                start_time = datetime.now()
                
                # æ‰¹é‡ç”Ÿæˆembedding
                batch_texts = [text for _, text in texts_to_process]
                embeddings = self.model.encode(
                    batch_texts,
                    batch_size=self.config.batch_size,
                    normalize_embeddings=self.config.normalize_embeddings,
                    convert_to_tensor=False,
                    show_progress_bar=len(batch_texts) > 10
                )
                
                processing_time = (datetime.now() - start_time).total_seconds()
                
                # å¤„ç†ç»“æœ
                for (original_idx, text), embedding in zip(texts_to_process, embeddings):
                    if isinstance(embedding, np.ndarray):
                        embedding = embedding.tolist()
                    
                    # ç¼“å­˜ç»“æœ
                    if use_cache:
                        cache_key = self._get_cache_key(text, model_name)
                        self._embedding_cache[cache_key] = {
                            "embedding": embedding,
                            "created_at": datetime.now().isoformat()
                        }
                    
                    # å­˜å‚¨åˆ°ç»“æœä¸­
                    cached_results[original_idx] = {
                        "embedding": embedding,
                        "processing_time": processing_time / len(texts_to_process)
                    }
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats["embeddings_generated"] += len(texts_to_process)
                self.stats["total_texts_processed"] += len(texts_to_process)
                
                logger.info(f"âœ… æ‰¹é‡ç”Ÿæˆ {len(texts_to_process)} ä¸ªembedding ({processing_time:.3f}s)")
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡embeddingç”Ÿæˆå¤±è´¥: {e}")
                raise
        
        # æŒ‰åŸå§‹é¡ºåºæ„å»ºç»“æœ
        for i, text in enumerate(texts):
            cached_data = cached_results.get(i, {})
            results.append(EmbeddingResult(
                text=text,
                embedding=cached_data["embedding"],
                model_name=model_name,
                metadata={
                    "processing_time": cached_data.get("processing_time", 0),
                    "dimensions": len(cached_data["embedding"]),
                    "from_cache": i not in [idx for idx, _ in texts_to_process]
                }
            ))
        
        # ä¿å­˜ç¼“å­˜
        if use_cache and texts_to_process:
            self._save_cache()
        
        return results
    
    async def get_similarity(self, 
                            text1: str, 
                            text2: str, 
                            model_name: str = None) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
            text2: ç¬¬äºŒä¸ªæ–‡æœ¬  
            model_name: æ¨¡å‹åç§°
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # ç”Ÿæˆembeddings
        results = await self.embed_texts([text1, text2], model_name)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        emb1 = np.array(results[0].embedding)
        emb2 = np.array(results[1].embedding)
        
        similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        
        return float(similarity)
    
    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        model_name = model_name or self.config.model_name
        
        if model_name in self.SUPPORTED_MODELS:
            info = self.SUPPORTED_MODELS[model_name].copy()
            info["is_loaded"] = (self.current_model_name == model_name and self.model is not None)
            return info
        
        return {"error": f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}"}
    
    def list_supported_models(self) -> Dict[str, Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹"""
        return self.SUPPORTED_MODELS.copy()
    
    async def switch_model(self, model_name: str) -> bool:
        """åˆ‡æ¢æ¨¡å‹"""
        if model_name not in self.SUPPORTED_MODELS:
            logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
            return False
        
        success = await self._load_model(model_name)
        if success:
            self.config.model_name = model_name
            logger.info(f"âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
        
        return success
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "cache_size": len(self._embedding_cache),
            "current_model": self.current_model_name,
            "cache_hit_rate": self.stats["cache_hits"] / max(1, self.stats["cache_hits"] + self.stats["cache_misses"])
        }
    
    def clear_cache(self) -> int:
        """æ¸…ç©ºç¼“å­˜"""
        cache_size = len(self._embedding_cache)
        self._embedding_cache.clear()
        
        # åˆ é™¤ç¼“å­˜æ–‡ä»¶
        if os.path.exists(self._cache_file):
            os.remove(self._cache_file)
        
        logger.info(f"âœ… å·²æ¸…ç©º {cache_size} ä¸ªç¼“å­˜embedding")
        return cache_size
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æµ‹è¯•embeddingç”Ÿæˆ
            test_text = "This is a health check test."
            result = await self.embed_text(test_text, use_cache=False)
            
            return {
                "status": "healthy",
                "model_loaded": self.model is not None,
                "current_model": self.current_model_name,
                "test_embedding_dimensions": len(result.embedding),
                "cache_size": len(self._embedding_cache),
                "stats": self.get_stats()
            }
            
        except Exception as e:
            logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "model_loaded": self.model is not None
            }


# å•ä¾‹å®ä¾‹
_embedding_service_instance = None

def get_embedding_service(config: EmbeddingConfig = None) -> EmbeddingService:
    """è·å–EmbeddingæœåŠ¡å•ä¾‹å®ä¾‹"""
    global _embedding_service_instance
    if _embedding_service_instance is None:
        _embedding_service_instance = EmbeddingService(config)
    return _embedding_service_instance 