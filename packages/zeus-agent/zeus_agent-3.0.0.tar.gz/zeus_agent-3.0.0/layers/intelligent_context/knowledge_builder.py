"""
çŸ¥è¯†åº“æ„å»ºå™¨
æ”¯æŒå¤šç§æ ¼å¼çš„çŸ¥è¯†æºï¼Œæ„å»ºç»“æ„åŒ–çš„FPGAä¸“ä¸šçŸ¥è¯†åº“

æ”¯æŒçš„çŸ¥è¯†æºæ ¼å¼ï¼š
- Markdownæ–‡æ¡£ (.md)
- YAMLç»“æ„åŒ–æ•°æ® (.yaml)
- JSONæ•°æ® (.json)
- ä»£ç ç¤ºä¾‹ (.v, .sv, .vhd)
- PDFæŠ€æœ¯æ–‡æ¡£
- ç½‘é¡µå†…å®¹
- APIæ–‡æ¡£
"""

import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
import yaml
import json
import re
import hashlib

from .integrated_knowledge_service import IntegratedKnowledgeService, KnowledgeItem

logger = logging.getLogger(__name__)


@dataclass
class KnowledgeSource:
    """çŸ¥è¯†æºå®šä¹‰"""
    name: str
    source_type: str  # markdown, yaml, json, verilog, pdf, web, api
    file_path: Optional[str] = None
    url: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    priority: int = 1  # 1=highest, 5=lowest
    tags: List[str] = field(default_factory=list)
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class KnowledgeChunk:
    """çŸ¥è¯†å—"""
    content: str
    title: str
    source: str
    chunk_type: str  # concept, example, reference, pattern, best_practice
    metadata: Dict[str, Any] = field(default_factory=dict)
    tags: List[str] = field(default_factory=list)
    relationships: List[str] = field(default_factory=list)  # å…³è”çš„å…¶ä»–çŸ¥è¯†å—


class KnowledgeBuilder:
    """
    çŸ¥è¯†åº“æ„å»ºå™¨
    
    åŠŸèƒ½ï¼š
    - å¤šæ ¼å¼çŸ¥è¯†æºè§£æ
    - æ™ºèƒ½å†…å®¹åˆ†å—
    - çŸ¥è¯†å…³ç³»æå–
    - è‡ªåŠ¨æ ‡ç­¾ç”Ÿæˆ
    - çŸ¥è¯†è´¨é‡è¯„ä¼°
    """
    
    def __init__(self, knowledge_service: IntegratedKnowledgeService):
        """åˆå§‹åŒ–çŸ¥è¯†åº“æ„å»ºå™¨"""
        self.knowledge_service = knowledge_service
        self.sources: List[KnowledgeSource] = []
        self.chunks: List[KnowledgeChunk] = []
        
        # çŸ¥è¯†åˆ†ç±»æ¨¡å¼
        self.knowledge_patterns = {
            'verilog_module': r'module\s+(\w+)',
            'always_block': r'always\s*@',
            'wire_declaration': r'wire\s+.*?;',
            'reg_declaration': r'reg\s+.*?;',
            'parameter': r'parameter\s+\w+\s*=',
            'testbench': r'module\s+tb_\w+',
            'constraint': r'constraint\s+\w+',
            'assertion': r'assert\s*\(',
            'timing_constraint': r'create_clock|set_input_delay|set_output_delay',
            'best_practice': r'(æœ€ä½³å®è·µ|best practice|æ¨è|å»ºè®®)',
            'common_mistake': r'(å¸¸è§é”™è¯¯|é”™è¯¯|é—®é¢˜|æ³¨æ„)',
            'design_pattern': r'(è®¾è®¡æ¨¡å¼|pattern|æ¨¡æ¿)'
        }
        
        logger.info("ğŸ—ï¸ çŸ¥è¯†åº“æ„å»ºå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def add_source(self, source: KnowledgeSource):
        """æ·»åŠ çŸ¥è¯†æº"""
        self.sources.append(source)
        logger.info(f"ğŸ“‹ æ·»åŠ çŸ¥è¯†æº: {source.name} ({source.source_type})")
    
    async def build_knowledge_base(self, output_stats: bool = True) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†åº“"""
        logger.info("ğŸ—ï¸ å¼€å§‹æ„å»ºFPGAçŸ¥è¯†åº“...")
        
        stats = {
            'sources_processed': 0,
            'chunks_created': 0,
            'knowledge_items_stored': 0,
            'processing_time': 0,
            'errors': []
        }
        
        start_time = datetime.now()
        
        try:
            # 1. å¤„ç†æ‰€æœ‰çŸ¥è¯†æº
            for source in self.sources:
                try:
                    await self._process_source(source)
                    stats['sources_processed'] += 1
                except Exception as e:
                    error_msg = f"å¤„ç†çŸ¥è¯†æºå¤±è´¥ {source.name}: {e}"
                    logger.error(error_msg)
                    stats['errors'].append(error_msg)
            
            # 2. å­˜å‚¨çŸ¥è¯†å—åˆ°çŸ¥è¯†åº“
            for chunk in self.chunks:
                try:
                    knowledge_item = KnowledgeItem(
                        content=chunk.content,
                        metadata={
                            'title': chunk.title,
                            'source': chunk.source,
                            'chunk_type': chunk.chunk_type,
                            'tags': chunk.tags,
                            'relationships': chunk.relationships,
                            **chunk.metadata
                        }
                    )
                    
                    await self.knowledge_service.add_knowledge(knowledge_item)
                    stats['knowledge_items_stored'] += 1
                    
                except Exception as e:
                    error_msg = f"å­˜å‚¨çŸ¥è¯†å—å¤±è´¥: {e}"
                    logger.error(error_msg)
                    stats['errors'].append(error_msg)
            
            stats['chunks_created'] = len(self.chunks)
            stats['processing_time'] = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ:")
            logger.info(f"   - å¤„ç†çŸ¥è¯†æº: {stats['sources_processed']} ä¸ª")
            logger.info(f"   - åˆ›å»ºçŸ¥è¯†å—: {stats['chunks_created']} ä¸ª")
            logger.info(f"   - å­˜å‚¨çŸ¥è¯†é¡¹: {stats['knowledge_items_stored']} ä¸ª")
            logger.info(f"   - å¤„ç†æ—¶é—´: {stats['processing_time']:.2f} ç§’")
            
            if stats['errors']:
                logger.warning(f"âš ï¸ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ {len(stats['errors'])} ä¸ªé”™è¯¯")
            
            return stats
            
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}")
            raise
    
    async def _process_source(self, source: KnowledgeSource):
        """å¤„ç†å•ä¸ªçŸ¥è¯†æº"""
        logger.debug(f"ğŸ“– å¤„ç†çŸ¥è¯†æº: {source.name}")
        
        if source.source_type == "markdown":
            await self._process_markdown(source)
        elif source.source_type == "yaml":
            await self._process_yaml(source)
        elif source.source_type == "json":
            await self._process_json(source)
        elif source.source_type == "verilog":
            await self._process_verilog(source)
        elif source.source_type == "systemverilog":
            await self._process_systemverilog(source)
        elif source.source_type == "directory":
            await self._process_directory(source)
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„çŸ¥è¯†æºç±»å‹: {source.source_type}")
    
    async def _process_markdown(self, source: KnowledgeSource):
        """å¤„ç†Markdownæ–‡æ¡£"""
        if not source.file_path or not Path(source.file_path).exists():
            raise FileNotFoundError(f"Markdownæ–‡ä»¶ä¸å­˜åœ¨: {source.file_path}")
        
        with open(source.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æŒ‰æ ‡é¢˜åˆ†å—
        sections = self._split_markdown_by_headers(content)
        
        for section in sections:
            if len(section['content'].strip()) < 50:  # è·³è¿‡å¤ªçŸ­çš„å†…å®¹
                continue
            
            # åˆ†æå†…å®¹ç±»å‹
            chunk_type = self._analyze_content_type(section['content'])
            
            # æå–æ ‡ç­¾
            tags = self._extract_tags(section['content'])
            tags.extend(source.tags)
            
            chunk = KnowledgeChunk(
                content=section['content'],
                title=section['title'] or f"{source.name} - Section",
                source=source.name,
                chunk_type=chunk_type,
                metadata={
                    'file_path': source.file_path,
                    'section_level': section['level'],
                    'word_count': len(section['content'].split()),
                    **source.metadata
                },
                tags=list(set(tags))  # å»é‡
            )
            
            self.chunks.append(chunk)
    
    async def _process_yaml(self, source: KnowledgeSource):
        """å¤„ç†YAMLç»“æ„åŒ–æ•°æ®"""
        if not source.file_path or not Path(source.file_path).exists():
            raise FileNotFoundError(f"YAMLæ–‡ä»¶ä¸å­˜åœ¨: {source.file_path}")
        
        with open(source.file_path, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
        
        # é€’å½’å¤„ç†YAMLç»“æ„
        await self._process_yaml_structure(data, source, [])
    
    async def _process_yaml_structure(self, data: Any, source: KnowledgeSource, path: List[str]):
        """é€’å½’å¤„ç†YAMLç»“æ„"""
        if isinstance(data, dict):
            for key, value in data.items():
                current_path = path + [key]
                
                if isinstance(value, str) and len(value) > 50:
                    # å­—ç¬¦ä¸²å†…å®¹ä½œä¸ºçŸ¥è¯†å—
                    chunk = KnowledgeChunk(
                        content=value,
                        title=f"{source.name} - {' > '.join(current_path)}",
                        source=source.name,
                        chunk_type="structured_data",
                        metadata={
                            'yaml_path': '.'.join(current_path),
                            'data_type': 'string',
                            **source.metadata
                        },
                        tags=source.tags + current_path
                    )
                    self.chunks.append(chunk)
                
                elif isinstance(value, (dict, list)):
                    await self._process_yaml_structure(value, source, current_path)
        
        elif isinstance(data, list):
            for i, item in enumerate(data):
                current_path = path + [str(i)]
                await self._process_yaml_structure(item, source, current_path)
    
    async def _process_verilog(self, source: KnowledgeSource):
        """å¤„ç†Verilogä»£ç æ–‡ä»¶"""
        if not source.file_path or not Path(source.file_path).exists():
            raise FileNotFoundError(f"Verilogæ–‡ä»¶ä¸å­˜åœ¨: {source.file_path}")
        
        with open(source.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ¨¡å—
        modules = self._extract_verilog_modules(content)
        
        for module in modules:
            chunk = KnowledgeChunk(
                content=module['content'],
                title=f"Verilog Module: {module['name']}",
                source=source.name,
                chunk_type="verilog_module",
                metadata={
                    'module_name': module['name'],
                    'parameters': module.get('parameters', []),
                    'ports': module.get('ports', []),
                    'file_path': source.file_path,
                    **source.metadata
                },
                tags=source.tags + ['verilog', 'module', module['name']]
            )
            self.chunks.append(chunk)
    
    async def _process_systemverilog(self, source: KnowledgeSource):
        """å¤„ç†SystemVerilogä»£ç æ–‡ä»¶"""
        if not source.file_path or not Path(source.file_path).exists():
            raise FileNotFoundError(f"SystemVerilogæ–‡ä»¶ä¸å­˜åœ¨: {source.file_path}")
        
        with open(source.file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ¨¡å—
        modules = self._extract_verilog_modules(content)
        
        for module in modules:
            chunk = KnowledgeChunk(
                content=module['content'],
                title=f"SystemVerilog Module: {module['name']}",
                source=source.name,
                chunk_type="systemverilog_module",
                metadata={
                    'module_name': module['name'],
                    'parameters': module.get('parameters', []),
                    'ports': module.get('ports', []),
                    'file_path': source.file_path,
                    **source.metadata
                },
                tags=source.tags + ['systemverilog', 'module', module['name']]
            )
            self.chunks.append(chunk)
    
    async def _process_directory(self, source: KnowledgeSource):
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
        if not source.file_path:
            raise ValueError("ç›®å½•æºå¿…é¡»æŒ‡å®šfile_path")
        
        directory = Path(source.file_path)
        if not directory.exists() or not directory.is_dir():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {source.file_path}")
        
        # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        supported_extensions = {
            '.md': 'markdown',
            '.yaml': 'yaml',
            '.yml': 'yaml',
            '.json': 'json',
            '.v': 'verilog',
            '.sv': 'systemverilog',
            '.vhd': 'vhdl'
        }
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix in supported_extensions:
                file_source = KnowledgeSource(
                    name=f"{source.name}/{file_path.relative_to(directory)}",
                    source_type=supported_extensions[file_path.suffix],
                    file_path=str(file_path),
                    metadata={**source.metadata, 'parent_directory': source.file_path},
                    tags=source.tags + [file_path.stem],
                    priority=source.priority
                )
                
                await self._process_source(file_source)
    
    def _split_markdown_by_headers(self, content: str) -> List[Dict[str, Any]]:
        """æŒ‰æ ‡é¢˜åˆ†å‰²Markdownå†…å®¹"""
        lines = content.split('\n')
        sections = []
        current_section = {'title': None, 'level': 0, 'content': ''}
        
        for line in lines:
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line)
            
            if header_match:
                # ä¿å­˜å½“å‰æ®µè½
                if current_section['content'].strip():
                    sections.append(current_section)
                
                # å¼€å§‹æ–°æ®µè½
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                current_section = {
                    'title': title,
                    'level': level,
                    'content': line + '\n'
                }
            else:
                current_section['content'] += line + '\n'
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
        if current_section['content'].strip():
            sections.append(current_section)
        
        return sections
    
    def _extract_verilog_modules(self, content: str) -> List[Dict[str, Any]]:
        """æå–Verilogæ¨¡å—"""
        modules = []
        
        # åŒ¹é…æ¨¡å—å®šä¹‰
        module_pattern = r'module\s+(\w+)\s*(?:\#\s*\([^)]*\))?\s*\([^)]*\)\s*;(.*?)endmodule'
        matches = re.finditer(module_pattern, content, re.DOTALL | re.MULTILINE)
        
        for match in matches:
            module_name = match.group(1)
            module_content = match.group(0)
            
            # æå–å‚æ•°
            param_pattern = r'parameter\s+(\w+)\s*=\s*([^,;]+)'
            parameters = re.findall(param_pattern, module_content)
            
            # æå–ç«¯å£
            port_pattern = r'(input|output|inout)\s+(?:wire|reg)?\s*(?:\[[^\]]*\])?\s*(\w+)'
            ports = re.findall(port_pattern, module_content)
            
            modules.append({
                'name': module_name,
                'content': module_content,
                'parameters': [{'name': p[0], 'value': p[1]} for p in parameters],
                'ports': [{'direction': p[0], 'name': p[1]} for p in ports]
            })
        
        return modules
    
    def _analyze_content_type(self, content: str) -> str:
        """åˆ†æå†…å®¹ç±»å‹"""
        content_lower = content.lower()
        
        # æ£€æŸ¥å„ç§æ¨¡å¼
        for pattern_name, pattern in self.knowledge_patterns.items():
            if re.search(pattern, content, re.IGNORECASE):
                return pattern_name
        
        # æ ¹æ®å…³é”®è¯åˆ¤æ–­
        if any(keyword in content_lower for keyword in ['example', 'ç¤ºä¾‹', 'ä¾‹å­']):
            return 'example'
        elif any(keyword in content_lower for keyword in ['concept', 'æ¦‚å¿µ', 'å®šä¹‰']):
            return 'concept'
        elif any(keyword in content_lower for keyword in ['reference', 'å‚è€ƒ', 'æ–‡æ¡£']):
            return 'reference'
        elif any(keyword in content_lower for keyword in ['best practice', 'æœ€ä½³å®è·µ', 'å»ºè®®']):
            return 'best_practice'
        else:
            return 'general'
    
    def _extract_tags(self, content: str) -> List[str]:
        """æå–å†…å®¹æ ‡ç­¾"""
        tags = []
        content_lower = content.lower()
        
        # FPGAç›¸å…³æ ‡ç­¾
        fpga_keywords = [
            'fpga', 'verilog', 'systemverilog', 'vhdl',
            'xilinx', 'altera', 'intel', 'vivado', 'quartus',
            'synthesis', 'simulation', 'timing', 'constraint',
            'clock', 'reset', 'state machine', 'pipeline',
            'fifo', 'memory', 'dsp', 'pcie', 'axi', 'uart', 'spi'
        ]
        
        for keyword in fpga_keywords:
            if keyword in content_lower:
                tags.append(keyword.replace(' ', '_'))
        
        return tags
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        chunk_types = {}
        tag_counts = {}
        source_counts = {}
        
        for chunk in self.chunks:
            # ç»Ÿè®¡å—ç±»å‹
            chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1
            
            # ç»Ÿè®¡æ ‡ç­¾
            for tag in chunk.tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
            
            # ç»Ÿè®¡æ¥æº
            source_counts[chunk.source] = source_counts.get(chunk.source, 0) + 1
        
        return {
            'total_chunks': len(self.chunks),
            'total_sources': len(self.sources),
            'chunk_types': chunk_types,
            'top_tags': sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10],
            'source_distribution': source_counts
        }


# ä¾¿åˆ©å‡½æ•°

async def build_fpga_knowledge_base(knowledge_dir: str = "./workspace/agents/fpga_expert/knowledge") -> Dict[str, Any]:
    """ä¾¿åˆ©å‡½æ•°ï¼šæ„å»ºFPGAçŸ¥è¯†åº“"""
    knowledge_service = IntegratedKnowledgeService()
    await knowledge_service.initialize()
    
    builder = KnowledgeBuilder(knowledge_service)
    
    # æ·»åŠ çŸ¥è¯†åº“ç›®å½•ä½œä¸ºæº
    knowledge_path = Path(knowledge_dir)
    if knowledge_path.exists():
        builder.add_source(KnowledgeSource(
            name="FPGA Knowledge Base",
            source_type="directory",
            file_path=str(knowledge_path),
            tags=['fpga', 'knowledge_base'],
            priority=1
        ))
    
    return await builder.build_knowledge_base() 