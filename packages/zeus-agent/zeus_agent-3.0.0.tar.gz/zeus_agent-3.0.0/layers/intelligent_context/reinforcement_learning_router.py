"""
å¼ºåŒ–å­¦ä¹ è·¯ç”±å™¨ - è‡ªé€‚åº”è·¯ç”±ç­–ç•¥å­¦ä¹ 

åŸºäºŽå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½è·¯ç”±å™¨ï¼Œèƒ½å¤Ÿä»Žç”¨æˆ·åé¦ˆä¸­å­¦ä¹ å¹¶æŒç»­ä¼˜åŒ–è·¯ç”±å†³ç­–ï¼Œ
å®žçŽ°çœŸæ­£çš„"è¶Šç”¨è¶Šèªæ˜Ž"çš„è‡ªé€‚åº”ç³»ç»Ÿã€‚

Author: ADC Team
Date: 2024-12-19
Version: 1.0.0
"""

import asyncio
import json
import numpy as np
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from abc import ABC, abstractmethod
import logging
from collections import defaultdict, deque
import pickle
import random

logger = logging.getLogger(__name__)

class ActionType(Enum):
    """åŠ¨ä½œç±»åž‹"""
    ROUTE_TO_LOCAL_KB = "route_to_local_kb"
    ROUTE_TO_AI_TRAINING = "route_to_ai_training"
    ROUTE_TO_WEB_SEARCH = "route_to_web_search"
    ROUTE_TO_FUSION = "route_to_fusion"
    ROUTE_TO_CACHE = "route_to_cache"

class StateFeature(Enum):
    """çŠ¶æ€ç‰¹å¾"""
    QUERY_COMPLEXITY = "query_complexity"
    USER_ROLE = "user_role"
    DOMAIN_MATCH = "domain_match"
    TIME_SENSITIVITY = "time_sensitivity"
    COST_SENSITIVITY = "cost_sensitivity"
    QUALITY_REQUIREMENT = "quality_requirement"
    CONTEXT_CONTINUITY = "context_continuity"
    HISTORICAL_SUCCESS = "historical_success"

class RewardType(Enum):
    """å¥–åŠ±ç±»åž‹"""
    USER_SATISFACTION = "user_satisfaction"
    COST_EFFICIENCY = "cost_efficiency"
    RESPONSE_TIME = "response_time"
    ACCURACY = "accuracy"
    ENGAGEMENT = "engagement"

@dataclass
class State:
    """çŠ¶æ€è¡¨ç¤º"""
    features: Dict[StateFeature, float]
    query_hash: str
    user_id: str
    session_id: str
    timestamp: datetime
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºå‘é‡è¡¨ç¤º"""
        return np.array([self.features.get(feature, 0.0) for feature in StateFeature])
    
    def __hash__(self) -> int:
        """çŠ¶æ€å“ˆå¸Œï¼Œç”¨äºŽQè¡¨ç´¢å¼•"""
        feature_tuple = tuple(round(self.features.get(f, 0.0), 2) for f in StateFeature)
        return hash(feature_tuple)

@dataclass
class Action:
    """åŠ¨ä½œè¡¨ç¤º"""
    action_type: ActionType
    parameters: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    estimated_cost: float = 0.0
    estimated_latency: float = 0.0

@dataclass
class Reward:
    """å¥–åŠ±ä¿¡å·"""
    total_reward: float
    reward_components: Dict[RewardType, float]
    feedback_timestamp: datetime
    explanation: str

@dataclass
class Experience:
    """ç»éªŒæ ·æœ¬ (State, Action, Reward, Next_State)"""
    state: State
    action: Action
    reward: Reward
    next_state: Optional[State]
    done: bool
    timestamp: datetime

class RewardFunction:
    """å¥–åŠ±å‡½æ•°"""
    
    def __init__(
        self,
        satisfaction_weight: float = 0.4,
        cost_weight: float = 0.2,
        speed_weight: float = 0.2,
        accuracy_weight: float = 0.2
    ):
        self.weights = {
            RewardType.USER_SATISFACTION: satisfaction_weight,
            RewardType.COST_EFFICIENCY: cost_weight,
            RewardType.RESPONSE_TIME: speed_weight,
            RewardType.ACCURACY: accuracy_weight
        }
    
    def calculate_reward(
        self,
        user_satisfaction: float,
        actual_cost: float,
        expected_cost: float,
        actual_latency: float,
        expected_latency: float,
        accuracy_score: float
    ) -> Reward:
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
        
        # ç”¨æˆ·æ»¡æ„åº¦å¥–åŠ± (0-1)
        satisfaction_reward = user_satisfaction
        
        # æˆæœ¬æ•ˆçŽ‡å¥–åŠ±
        cost_efficiency = max(0, 1 - actual_cost / max(expected_cost, 0.001))
        
        # å“åº”æ—¶é—´å¥–åŠ±
        speed_efficiency = max(0, 1 - actual_latency / max(expected_latency, 0.001))
        
        # å‡†ç¡®æ€§å¥–åŠ±
        accuracy_reward = accuracy_score
        
        # è®¡ç®—åŠ æƒæ€»å¥–åŠ±
        reward_components = {
            RewardType.USER_SATISFACTION: satisfaction_reward,
            RewardType.COST_EFFICIENCY: cost_efficiency,
            RewardType.RESPONSE_TIME: speed_efficiency,
            RewardType.ACCURACY: accuracy_reward
        }
        
        total_reward = sum(
            reward_components[reward_type] * self.weights[reward_type]
            for reward_type in reward_components
        )
        
        # å¥–åŠ±èŒƒå›´å½’ä¸€åŒ–åˆ° [-1, 1]
        total_reward = max(-1.0, min(1.0, total_reward * 2 - 1))
        
        explanation = f"æ»¡æ„åº¦:{satisfaction_reward:.2f}, æˆæœ¬æ•ˆçŽ‡:{cost_efficiency:.2f}, " \
                     f"é€Ÿåº¦:{speed_efficiency:.2f}, å‡†ç¡®æ€§:{accuracy_reward:.2f}"
        
        return Reward(
            total_reward=total_reward,
            reward_components=reward_components,
            feedback_timestamp=datetime.now(),
            explanation=explanation
        )

class QLearningAgent:
    """Qå­¦ä¹ æ™ºèƒ½ä½“"""
    
    def __init__(
        self,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95,
        epsilon: float = 0.1,
        epsilon_decay: float = 0.995,
        epsilon_min: float = 0.01
    ):
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Qè¡¨ï¼šçŠ¶æ€-åŠ¨ä½œå€¼å‡½æ•°
        self.q_table: Dict[Tuple[int, ActionType], float] = defaultdict(float)
        
        # ç»éªŒå›žæ”¾ç¼“å†²åŒº
        self.experience_buffer: deque = deque(maxlen=10000)
        
        # å­¦ä¹ ç»Ÿè®¡
        self.learning_stats = {
            'episodes': 0,
            'total_reward': 0.0,
            'average_reward': 0.0,
            'exploration_rate': self.epsilon,
            'q_table_size': 0
        }
        
        logger.info(f"ðŸ§  Qå­¦ä¹ æ™ºèƒ½ä½“åˆå§‹åŒ– - å­¦ä¹ çŽ‡:{learning_rate}, æŠ˜æ‰£å› å­:{discount_factor}")
    
    def get_state_key(self, state: State) -> int:
        """èŽ·å–çŠ¶æ€é”®"""
        return hash(state)
    
    def select_action(self, state: State, available_actions: List[ActionType]) -> ActionType:
        """é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªç­–ç•¥ï¼‰"""
        state_key = self.get_state_key(state)
        
        # Îµ-è´ªå©ªç­–ç•¥
        if random.random() < self.epsilon:
            # æŽ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            action = random.choice(available_actions)
            logger.debug(f"ðŸŽ² æŽ¢ç´¢é€‰æ‹©åŠ¨ä½œ: {action.value}")
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€é«˜çš„åŠ¨ä½œ
            q_values = {action: self.q_table[(state_key, action)] for action in available_actions}
            action = max(q_values.items(), key=lambda x: x[1])[0]
            logger.debug(f"ðŸŽ¯ åˆ©ç”¨é€‰æ‹©åŠ¨ä½œ: {action.value} (Qå€¼: {q_values[action]:.3f})")
        
        return action
    
    def learn_from_experience(self, experience: Experience):
        """ä»Žç»éªŒä¸­å­¦ä¹ """
        # æ·»åŠ åˆ°ç»éªŒç¼“å†²åŒº
        self.experience_buffer.append(experience)
        
        # èŽ·å–çŠ¶æ€å’ŒåŠ¨ä½œçš„é”®
        state_key = self.get_state_key(experience.state)
        action_key = (state_key, experience.action.action_type)
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        if experience.done or experience.next_state is None:
            target_q = experience.reward.total_reward
        else:
            next_state_key = self.get_state_key(experience.next_state)
            # èŽ·å–ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
            next_q_values = [
                self.q_table[(next_state_key, action)] 
                for action in ActionType
            ]
            max_next_q = max(next_q_values) if next_q_values else 0.0
            target_q = experience.reward.total_reward + self.discount_factor * max_next_q
        
        # æ›´æ–°Qå€¼
        current_q = self.q_table[action_key]
        self.q_table[action_key] += self.learning_rate * (target_q - current_q)
        
        # æ›´æ–°æŽ¢ç´¢çŽ‡
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.learning_stats['episodes'] += 1
        self.learning_stats['total_reward'] += experience.reward.total_reward
        self.learning_stats['average_reward'] = (
            self.learning_stats['total_reward'] / self.learning_stats['episodes']
        )
        self.learning_stats['exploration_rate'] = self.epsilon
        self.learning_stats['q_table_size'] = len(self.q_table)
        
        logger.debug(f"ðŸ“š å­¦ä¹ æ›´æ–° - Qå€¼: {current_q:.3f} â†’ {self.q_table[action_key]:.3f}")
    
    def batch_learn(self, batch_size: int = 32):
        """æ‰¹é‡å­¦ä¹ """
        if len(self.experience_buffer) < batch_size:
            return
        
        # éšæœºé‡‡æ ·ç»éªŒ
        batch = random.sample(list(self.experience_buffer), batch_size)
        
        for experience in batch:
            self.learn_from_experience(experience)
        
        logger.info(f"ðŸ“š æ‰¹é‡å­¦ä¹ å®Œæˆ - æ ·æœ¬æ•°: {batch_size}")
    
    def get_q_values(self, state: State) -> Dict[ActionType, float]:
        """èŽ·å–çŠ¶æ€çš„æ‰€æœ‰Qå€¼"""
        state_key = self.get_state_key(state)
        return {action: self.q_table[(state_key, action)] for action in ActionType}
    
    def save_model(self, filepath: str):
        """ä¿å­˜æ¨¡åž‹"""
        model_data = {
            'q_table': dict(self.q_table),
            'learning_stats': self.learning_stats,
            'hyperparameters': {
                'learning_rate': self.learning_rate,
                'discount_factor': self.discount_factor,
                'epsilon': self.epsilon,
                'epsilon_decay': self.epsilon_decay,
                'epsilon_min': self.epsilon_min
            }
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"ðŸ’¾ æ¨¡åž‹å·²ä¿å­˜åˆ° {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½æ¨¡åž‹"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.q_table = defaultdict(float, model_data['q_table'])
            self.learning_stats = model_data['learning_stats']
            
            # æ¢å¤è¶…å‚æ•°
            params = model_data['hyperparameters']
            self.learning_rate = params['learning_rate']
            self.discount_factor = params['discount_factor']
            self.epsilon = params['epsilon']
            self.epsilon_decay = params['epsilon_decay']
            self.epsilon_min = params['epsilon_min']
            
            logger.info(f"ðŸ“‚ æ¨¡åž‹å·²ä»Ž {filepath} åŠ è½½")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")

class StateExtractor:
    """çŠ¶æ€ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.feature_extractors = {
            StateFeature.QUERY_COMPLEXITY: self._extract_query_complexity,
            StateFeature.USER_ROLE: self._extract_user_role,
            StateFeature.DOMAIN_MATCH: self._extract_domain_match,
            StateFeature.TIME_SENSITIVITY: self._extract_time_sensitivity,
            StateFeature.COST_SENSITIVITY: self._extract_cost_sensitivity,
            StateFeature.QUALITY_REQUIREMENT: self._extract_quality_requirement,
            StateFeature.CONTEXT_CONTINUITY: self._extract_context_continuity,
            StateFeature.HISTORICAL_SUCCESS: self._extract_historical_success
        }
    
    def extract_state(
        self,
        query: str,
        user_profile: Dict[str, Any],
        context: Dict[str, Any],
        session_history: List[Dict[str, Any]]
    ) -> State:
        """æå–çŠ¶æ€ç‰¹å¾"""
        
        features = {}
        for feature_type in StateFeature:
            try:
                extractor = self.feature_extractors[feature_type]
                features[feature_type] = extractor(query, user_profile, context, session_history)
            except Exception as e:
                logger.warning(f"âš ï¸ ç‰¹å¾æå–å¤±è´¥ {feature_type.value}: {e}")
                features[feature_type] = 0.0
        
        return State(
            features=features,
            query_hash=str(hash(query)),
            user_id=user_profile.get('user_id', 'anonymous'),
            session_id=context.get('session_id', 'default'),
            timestamp=datetime.now()
        )
    
    def _extract_query_complexity(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–æŸ¥è¯¢å¤æ‚åº¦ç‰¹å¾"""
        # åŸºäºŽæŸ¥è¯¢é•¿åº¦å’Œå¤æ‚æ€§å…³é”®è¯
        complexity_score = 0.0
        
        # é•¿åº¦å› å­
        length_score = min(1.0, len(query) / 200)
        complexity_score += length_score * 0.3
        
        # å¤æ‚æ€§å…³é”®è¯
        complex_keywords = ['å¦‚ä½•å®žçŽ°', 'è®¾è®¡æ–¹æ¡ˆ', 'ä¼˜åŒ–', 'æž¶æž„', 'ç®—æ³•', 'æ¯”è¾ƒ', 'åˆ†æž']
        keyword_score = sum(1 for keyword in complex_keywords if keyword in query) / len(complex_keywords)
        complexity_score += keyword_score * 0.4
        
        # æŠ€æœ¯æœ¯è¯­å¯†åº¦
        technical_terms = ['FPGA', 'HDL', 'RTL', 'Verilog', 'VHDL', 'æ—¶åº', 'ç»¼åˆ']
        term_density = sum(1 for term in technical_terms if term in query) / max(len(query.split()), 1)
        complexity_score += min(1.0, term_density * 10) * 0.3
        
        return min(1.0, complexity_score)
    
    def _extract_user_role(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–ç”¨æˆ·è§’è‰²ç‰¹å¾"""
        role_mapping = {
            'beginner': 0.0,
            'intermediate': 0.33,
            'expert': 0.67,
            'researcher': 1.0
        }
        return role_mapping.get(user_profile.get('role', 'beginner'), 0.0)
    
    def _extract_domain_match(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–é¢†åŸŸåŒ¹é…åº¦ç‰¹å¾"""
        fpga_keywords = ['FPGA', 'HDL', 'Verilog', 'VHDL', 'å¯ç¼–ç¨‹', 'é€»è¾‘', 'ç»¼åˆ', 'ä»¿çœŸ']
        matches = sum(1 for keyword in fpga_keywords if keyword.lower() in query.lower())
        return min(1.0, matches / 3)  # å½’ä¸€åŒ–åˆ°[0,1]
    
    def _extract_time_sensitivity(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–æ—¶é—´æ•æ„Ÿæ€§ç‰¹å¾"""
        time_keywords = ['æœ€æ–°', 'å½“å‰', 'çŽ°åœ¨', 'ä»Šå¹´', 'æœ€è¿‘', 'è¶‹åŠ¿', 'å‘å±•']
        time_score = sum(1 for keyword in time_keywords if keyword in query) / len(time_keywords)
        return min(1.0, time_score * 2)
    
    def _extract_cost_sensitivity(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–æˆæœ¬æ•æ„Ÿæ€§ç‰¹å¾"""
        # åŸºäºŽç”¨æˆ·è§’è‰²å’ŒåŽ†å²è¡Œä¸ºæŽ¨æ–­æˆæœ¬æ•æ„Ÿæ€§
        role = user_profile.get('role', 'beginner')
        if role == 'beginner':
            return 0.8  # åˆå­¦è€…å¯¹æˆæœ¬æ•æ„Ÿ
        elif role == 'intermediate':
            return 0.5  # ä¸­çº§ç”¨æˆ·ä¸­ç­‰æ•æ„Ÿ
        else:
            return 0.2  # ä¸“å®¶å’Œç ”ç©¶è€…å¯¹æˆæœ¬ä¸å¤ªæ•æ„Ÿ
    
    def _extract_quality_requirement(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–è´¨é‡è¦æ±‚ç‰¹å¾"""
        quality_keywords = ['æœ€ä½³', 'æŽ¨è', 'æ ‡å‡†', 'è§„èŒƒ', 'æƒå¨', 'å®˜æ–¹', 'ä¸“ä¸š']
        quality_score = sum(1 for keyword in quality_keywords if keyword in query) / len(quality_keywords)
        
        # ç»“åˆç”¨æˆ·è§’è‰²
        role = user_profile.get('role', 'beginner')
        role_quality_factor = {'beginner': 0.5, 'intermediate': 0.7, 'expert': 0.9, 'researcher': 1.0}
        
        return min(1.0, quality_score + role_quality_factor.get(role, 0.5))
    
    def _extract_context_continuity(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–ä¸Šä¸‹æ–‡è¿žç»­æ€§ç‰¹å¾"""
        if not history:
            return 0.0
        
        # æ£€æŸ¥ä¸Žæœ€è¿‘æŸ¥è¯¢çš„ç›¸ä¼¼æ€§
        recent_queries = [item.get('query', '') for item in history[-3:]]
        if not recent_queries:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å åº¦è®¡ç®—
        query_words = set(query.lower().split())
        max_overlap = 0.0
        
        for recent_query in recent_queries:
            recent_words = set(recent_query.lower().split())
            if recent_words:
                overlap = len(query_words & recent_words) / len(query_words | recent_words)
                max_overlap = max(max_overlap, overlap)
        
        return max_overlap
    
    def _extract_historical_success(
        self, query: str, user_profile: Dict, context: Dict, history: List
    ) -> float:
        """æå–åŽ†å²æˆåŠŸçŽ‡ç‰¹å¾"""
        if not history:
            return 0.5  # é»˜è®¤ä¸­ç­‰æˆåŠŸçŽ‡
        
        # è®¡ç®—æœ€è¿‘æŸ¥è¯¢çš„å¹³å‡æ»¡æ„åº¦
        recent_satisfactions = []
        for item in history[-10:]:  # æœ€è¿‘10æ¬¡æŸ¥è¯¢
            if 'satisfaction' in item:
                recent_satisfactions.append(item['satisfaction'])
        
        if recent_satisfactions:
            return sum(recent_satisfactions) / len(recent_satisfactions)
        else:
            return 0.5

class ReinforcementLearningRouter:
    """å¼ºåŒ–å­¦ä¹ è·¯ç”±å™¨"""
    
    def __init__(
        self,
        learning_rate: float = 0.1,
        discount_factor: float = 0.95,
        exploration_rate: float = 0.1,
        model_save_path: Optional[str] = None
    ):
        self.q_agent = QLearningAgent(
            learning_rate=learning_rate,
            discount_factor=discount_factor,
            epsilon=exploration_rate
        )
        
        self.state_extractor = StateExtractor()
        self.reward_function = RewardFunction()
        self.model_save_path = model_save_path
        
        # è·¯ç”±åŽ†å²å’Œä¼šè¯ç®¡ç†
        self.routing_history: List[Dict[str, Any]] = []
        self.active_sessions: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.pending_experiences: Dict[str, Experience] = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_routes': 0,
            'successful_routes': 0,
            'average_reward': 0.0,
            'learning_episodes': 0,
            'exploration_ratio': 0.0
        }
        
        logger.info("ðŸ§  å¼ºåŒ–å­¦ä¹ è·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–è·¯ç”±å™¨"""
        # åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
        if self.model_save_path:
            try:
                self.q_agent.load_model(self.model_save_path)
                logger.info("ðŸ“‚ åŠ è½½äº†é¢„è®­ç»ƒçš„å¼ºåŒ–å­¦ä¹ æ¨¡åž‹")
            except:
                logger.info("ðŸ†• æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡åž‹ï¼Œä»Žå¤´å¼€å§‹å­¦ä¹ ")
        
        # å¯åŠ¨å®šæœŸä¿å­˜ä»»åŠ¡
        if self.model_save_path:
            asyncio.create_task(self._periodic_model_save())
    
    async def route_query(
        self,
        query: str,
        user_profile: Dict[str, Any],
        context: Dict[str, Any]
    ) -> Tuple[ActionType, Dict[str, Any]]:
        """è·¯ç”±æŸ¥è¯¢"""
        session_id = context.get('session_id', 'default')
        session_history = self.active_sessions[session_id]
        
        # æå–å½“å‰çŠ¶æ€
        current_state = self.state_extractor.extract_state(
            query, user_profile, context, session_history
        )
        
        # èŽ·å–å¯ç”¨åŠ¨ä½œ
        available_actions = list(ActionType)
        
        # é€‰æ‹©åŠ¨ä½œ
        selected_action = self.q_agent.select_action(current_state, available_actions)
        
        # åˆ›å»ºåŠ¨ä½œå¯¹è±¡
        action = Action(
            action_type=selected_action,
            parameters=self._get_action_parameters(selected_action, current_state),
            confidence=self._estimate_action_confidence(selected_action, current_state),
            estimated_cost=self._estimate_action_cost(selected_action),
            estimated_latency=self._estimate_action_latency(selected_action)
        )
        
        # è®°å½•è·¯ç”±å†³ç­–
        route_record = {
            'timestamp': datetime.now(),
            'query': query,
            'state': current_state,
            'action': action,
            'user_id': user_profile.get('user_id', 'anonymous'),
            'session_id': session_id
        }
        
        self.routing_history.append(route_record)
        session_history.append(route_record)
        
        # åˆ›å»ºå¾…å®Œæˆçš„ç»éªŒï¼ˆç­‰å¾…åé¦ˆï¼‰
        experience_key = f"{session_id}_{len(session_history)}"
        self.pending_experiences[experience_key] = Experience(
            state=current_state,
            action=action,
            reward=None,  # å¾…å¡«å……
            next_state=None,  # å¾…å¡«å……
            done=False,
            timestamp=datetime.now()
        )
        
        # æ›´æ–°ç»Ÿè®¡
        self.performance_stats['total_routes'] += 1
        self.performance_stats['exploration_ratio'] = self.q_agent.epsilon
        
        logger.info(f"ðŸŽ¯ å¼ºåŒ–å­¦ä¹ è·¯ç”±: {query[:50]}... â†’ {selected_action.value}")
        
        # è¿”å›žè·¯ç”±å†³ç­–å’Œå…ƒæ•°æ®
        return selected_action, {
            'experience_key': experience_key,
            'confidence': action.confidence,
            'estimated_cost': action.estimated_cost,
            'estimated_latency': action.estimated_latency,
            'q_values': self.q_agent.get_q_values(current_state),
            'state_features': current_state.features
        }
    
    async def provide_feedback(
        self,
        experience_key: str,
        user_satisfaction: float,
        actual_cost: float,
        actual_latency: float,
        accuracy_score: float,
        next_query: Optional[str] = None,
        next_user_profile: Optional[Dict[str, Any]] = None,
        next_context: Optional[Dict[str, Any]] = None
    ):
        """æä¾›åé¦ˆï¼Œå®Œæˆå­¦ä¹ å¾ªçŽ¯"""
        if experience_key not in self.pending_experiences:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç»éªŒé”®: {experience_key}")
            return
        
        experience = self.pending_experiences[experience_key]
        
        # è®¡ç®—å¥–åŠ±
        reward = self.reward_function.calculate_reward(
            user_satisfaction=user_satisfaction,
            actual_cost=actual_cost,
            expected_cost=experience.action.estimated_cost,
            actual_latency=actual_latency,
            expected_latency=experience.action.estimated_latency,
            accuracy_score=accuracy_score
        )
        
        # æå–ä¸‹ä¸€çŠ¶æ€ï¼ˆå¦‚æžœæœ‰ä¸‹ä¸€ä¸ªæŸ¥è¯¢ï¼‰
        next_state = None
        if next_query and next_user_profile and next_context:
            session_id = next_context.get('session_id', 'default')
            session_history = self.active_sessions[session_id]
            next_state = self.state_extractor.extract_state(
                next_query, next_user_profile, next_context, session_history
            )
        
        # å®Œæˆç»éªŒ
        experience.reward = reward
        experience.next_state = next_state
        experience.done = (next_state is None)
        
        # å­¦ä¹ 
        self.q_agent.learn_from_experience(experience)
        
        # æ›´æ–°ç»Ÿè®¡
        self.performance_stats['learning_episodes'] += 1
        if user_satisfaction > 0.7:  # è®¤ä¸ºæ»¡æ„åº¦>0.7ä¸ºæˆåŠŸ
            self.performance_stats['successful_routes'] += 1
        
        total_reward = self.q_agent.learning_stats['total_reward']
        episodes = self.q_agent.learning_stats['episodes']
        self.performance_stats['average_reward'] = total_reward / max(episodes, 1)
        
        # æ¸…ç†å·²å®Œæˆçš„ç»éªŒ
        del self.pending_experiences[experience_key]
        
        logger.info(f"ðŸ“š å¼ºåŒ–å­¦ä¹ åé¦ˆ: å¥–åŠ±={reward.total_reward:.3f}, æ»¡æ„åº¦={user_satisfaction:.2f}")
    
    def _get_action_parameters(self, action: ActionType, state: State) -> Dict[str, Any]:
        """èŽ·å–åŠ¨ä½œå‚æ•°"""
        parameters = {}
        
        if action == ActionType.ROUTE_TO_FUSION:
            # æ ¹æ®çŠ¶æ€ç‰¹å¾å†³å®šèžåˆç­–ç•¥
            if state.features.get(StateFeature.TIME_SENSITIVITY, 0) > 0.7:
                parameters['fusion_strategy'] = 'temporal_fusion'
            elif state.features.get(StateFeature.QUALITY_REQUIREMENT, 0) > 0.8:
                parameters['fusion_strategy'] = 'quality_driven_fusion'
            elif state.features.get(StateFeature.COST_SENSITIVITY, 0) > 0.7:
                parameters['fusion_strategy'] = 'cost_aware_fusion'
            else:
                parameters['fusion_strategy'] = 'semantic_fusion'
        
        return parameters
    
    def _estimate_action_confidence(self, action: ActionType, state: State) -> float:
        """ä¼°ç®—åŠ¨ä½œç½®ä¿¡åº¦"""
        # åŸºäºŽQå€¼å’ŒçŠ¶æ€ç‰¹å¾ä¼°ç®—ç½®ä¿¡åº¦
        state_key = self.q_agent.get_state_key(state)
        q_value = self.q_agent.q_table[(state_key, action)]
        
        # å°†Qå€¼è½¬æ¢ä¸ºç½®ä¿¡åº¦ [0, 1]
        confidence = max(0.0, min(1.0, (q_value + 1) / 2))
        
        return confidence
    
    def _estimate_action_cost(self, action: ActionType) -> float:
        """ä¼°ç®—åŠ¨ä½œæˆæœ¬"""
        cost_mapping = {
            ActionType.ROUTE_TO_CACHE: 0.001,
            ActionType.ROUTE_TO_LOCAL_KB: 0.1,
            ActionType.ROUTE_TO_AI_TRAINING: 1.0,
            ActionType.ROUTE_TO_WEB_SEARCH: 0.5,
            ActionType.ROUTE_TO_FUSION: 2.0
        }
        return cost_mapping.get(action, 0.5)
    
    def _estimate_action_latency(self, action: ActionType) -> float:
        """ä¼°ç®—åŠ¨ä½œå»¶è¿Ÿ"""
        latency_mapping = {
            ActionType.ROUTE_TO_CACHE: 0.1,
            ActionType.ROUTE_TO_LOCAL_KB: 0.8,
            ActionType.ROUTE_TO_AI_TRAINING: 2.5,
            ActionType.ROUTE_TO_WEB_SEARCH: 1.5,
            ActionType.ROUTE_TO_FUSION: 4.0
        }
        return latency_mapping.get(action, 2.0)
    
    async def _periodic_model_save(self):
        """å®šæœŸä¿å­˜æ¨¡åž‹"""
        while True:
            try:
                await asyncio.sleep(3600)  # æ¯å°æ—¶ä¿å­˜ä¸€æ¬¡
                
                if self.model_save_path and self.q_agent.learning_stats['episodes'] > 0:
                    self.q_agent.save_model(self.model_save_path)
                    logger.info("ðŸ’¾ å®šæœŸä¿å­˜å¼ºåŒ–å­¦ä¹ æ¨¡åž‹")
                    
            except Exception as e:
                logger.error(f"âŒ æ¨¡åž‹ä¿å­˜å¤±è´¥: {e}")
    
    def get_learning_analytics(self) -> Dict[str, Any]:
        """èŽ·å–å­¦ä¹ åˆ†æžæ•°æ®"""
        return {
            'q_learning_stats': self.q_agent.learning_stats,
            'performance_stats': self.performance_stats,
            'routing_history_size': len(self.routing_history),
            'active_sessions': len(self.active_sessions),
            'pending_experiences': len(self.pending_experiences),
            'q_table_insights': self._analyze_q_table()
        }
    
    def _analyze_q_table(self) -> Dict[str, Any]:
        """åˆ†æžQè¡¨"""
        if not self.q_agent.q_table:
            return {'empty_table': True}
        
        # ç»Ÿè®¡Qå€¼åˆ†å¸ƒ
        q_values = list(self.q_agent.q_table.values())
        
        # ç»Ÿè®¡åŠ¨ä½œåå¥½
        action_preferences = defaultdict(list)
        for (state_key, action), q_value in self.q_agent.q_table.items():
            action_preferences[action].append(q_value)
        
        action_stats = {}
        for action, q_vals in action_preferences.items():
            action_stats[action.value] = {
                'avg_q_value': sum(q_vals) / len(q_vals),
                'max_q_value': max(q_vals),
                'min_q_value': min(q_vals),
                'count': len(q_vals)
            }
        
        return {
            'total_state_action_pairs': len(q_values),
            'avg_q_value': sum(q_values) / len(q_values),
            'max_q_value': max(q_values),
            'min_q_value': min(q_values),
            'action_preferences': action_stats
        }
    
    async def export_learning_data(self, filepath: str):
        """å¯¼å‡ºå­¦ä¹ æ•°æ®"""
        export_data = {
            'timestamp': datetime.now().isoformat(),
            'learning_analytics': self.get_learning_analytics(),
            'routing_history': [
                {
                    'timestamp': record['timestamp'].isoformat(),
                    'query': record['query'],
                    'action': record['action'].action_type.value,
                    'confidence': record['action'].confidence,
                    'user_id': record['user_id']
                }
                for record in self.routing_history[-1000:]  # æœ€è¿‘1000æ¡è®°å½•
            ]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ðŸ“Š å­¦ä¹ æ•°æ®å·²å¯¼å‡ºåˆ° {filepath}")

# å·¥åŽ‚å‡½æ•°
def create_reinforcement_learning_router(
    learning_rate: float = 0.1,
    discount_factor: float = 0.95,
    exploration_rate: float = 0.1,
    model_save_path: Optional[str] = None
) -> ReinforcementLearningRouter:
    """åˆ›å»ºå¼ºåŒ–å­¦ä¹ è·¯ç”±å™¨"""
    return ReinforcementLearningRouter(
        learning_rate=learning_rate,
        discount_factor=discount_factor,
        exploration_rate=exploration_rate,
        model_save_path=model_save_path
    ) 