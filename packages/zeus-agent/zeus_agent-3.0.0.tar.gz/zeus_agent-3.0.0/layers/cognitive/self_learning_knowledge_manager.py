"""
è‡ªå­¦ä¹ çŸ¥è¯†ç®¡ç†å™¨
å®ç°Memory-Knowledgeé›†æˆçš„4é˜¶æ®µçŸ¥è¯†æ¼”åŒ–æ¶æ„

Author: ADC Team
Date: 2024-12-19
Version: 1.0.0
"""

import asyncio
import logging
import hashlib
import json
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import uuid
import re
from pathlib import Path

from .memory import MemorySystem, MemoryType, MemoryItem, EpisodicMemory, SemanticMemory
from ..intelligent_context.integrated_knowledge_service import IntegratedKnowledgeService, KnowledgeItem
from ..framework.abstractions.result import UniversalResult, ResultStatus, ErrorInfo

logger = logging.getLogger(__name__)


class KnowledgeEvolutionStage(Enum):
    """çŸ¥è¯†æ¼”åŒ–é˜¶æ®µ"""
    IMMEDIATE = "immediate"      # å³æ—¶å“åº”é˜¶æ®µ (Working Memory)
    EXPERIENTIAL = "experiential"  # ç»éªŒè®°å½•é˜¶æ®µ (Episodic Memory)
    CONCEPTUAL = "conceptual"    # æ¦‚å¿µæŠ½è±¡é˜¶æ®µ (Semantic Memory)
    CRYSTALLIZED = "crystallized"  # çŸ¥è¯†å›ºåŒ–é˜¶æ®µ (Knowledge Base)


class FeedbackType(Enum):
    """åé¦ˆç±»å‹"""
    THUMBS_UP = "thumbs_up"
    THUMBS_DOWN = "thumbs_down"
    CORRECTION = "correction"
    ENHANCEMENT = "enhancement"
    NEUTRAL = "neutral"


class QualityDimension(Enum):
    """è´¨é‡ç»´åº¦"""
    USER_FEEDBACK = "user_feedback"     # ç”¨æˆ·åé¦ˆ
    CONSISTENCY = "consistency"         # ä¸€è‡´æ€§
    COMPLETENESS = "completeness"       # å®Œæ•´æ€§
    VERIFIABILITY = "verifiability"     # å¯éªŒè¯æ€§
    RELEVANCE = "relevance"             # ç›¸å…³æ€§


@dataclass
class LearningResponse:
    """å­¦ä¹ å“åº”é¡¹"""
    response_id: str
    query: str
    llm_response: str
    user_context: Dict[str, Any]
    timestamp: datetime = field(default_factory=datetime.now)
    stage: KnowledgeEvolutionStage = KnowledgeEvolutionStage.IMMEDIATE
    
    # åé¦ˆå’Œè´¨é‡ä¿¡æ¯
    user_feedback: Optional[Dict[str, Any]] = None
    quality_score: Optional[float] = None
    quality_breakdown: Dict[QualityDimension, float] = field(default_factory=dict)
    
    # è¿½è¸ªä¿¡æ¯
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    validation_count: int = 0
    
    # å…³è”ä¿¡æ¯
    related_responses: List[str] = field(default_factory=list)
    extracted_concepts: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class KnowledgeEvolutionMetrics:
    """çŸ¥è¯†æ¼”åŒ–æŒ‡æ ‡"""
    total_responses: int = 0
    stage_distribution: Dict[str, int] = field(default_factory=lambda: {
        "immediate": 0, "experiential": 0, "conceptual": 0, "crystallized": 0
    })
    average_quality_score: float = 0.0
    crystallization_rate: float = 0.0  # å›ºåŒ–ç‡
    user_satisfaction_rate: float = 0.0
    knowledge_retention_rate: float = 0.0


class KnowledgeQualityAssessor:
    """çŸ¥è¯†è´¨é‡è¯„ä¼°å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # è´¨é‡ç»´åº¦æƒé‡
        self.quality_weights = {
            QualityDimension.USER_FEEDBACK: 0.4,
            QualityDimension.CONSISTENCY: 0.25,
            QualityDimension.COMPLETENESS: 0.15,
            QualityDimension.VERIFIABILITY: 0.1,
            QualityDimension.RELEVANCE: 0.1
        }
        
        # è´¨é‡é˜ˆå€¼
        self.high_quality_threshold = 0.8
        self.medium_quality_threshold = 0.6
        self.low_quality_threshold = 0.4
    
    async def assess_response_quality(self, 
                                    response: LearningResponse,
                                    existing_knowledge: List[str] = None) -> Tuple[float, Dict[QualityDimension, float]]:
        """è¯„ä¼°å“åº”è´¨é‡"""
        
        quality_breakdown = {}
        
        # 1. ç”¨æˆ·åé¦ˆç»´åº¦
        feedback_score = self._assess_user_feedback(response.user_feedback)
        quality_breakdown[QualityDimension.USER_FEEDBACK] = feedback_score
        
        # 2. ä¸€è‡´æ€§ç»´åº¦
        consistency_score = await self._assess_consistency(response, existing_knowledge)
        quality_breakdown[QualityDimension.CONSISTENCY] = consistency_score
        
        # 3. å®Œæ•´æ€§ç»´åº¦
        completeness_score = self._assess_completeness(response)
        quality_breakdown[QualityDimension.COMPLETENESS] = completeness_score
        
        # 4. å¯éªŒè¯æ€§ç»´åº¦
        verifiability_score = self._assess_verifiability(response)
        quality_breakdown[QualityDimension.VERIFIABILITY] = verifiability_score
        
        # 5. ç›¸å…³æ€§ç»´åº¦
        relevance_score = self._assess_relevance(response)
        quality_breakdown[QualityDimension.RELEVANCE] = relevance_score
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = sum(
            self.quality_weights[dim] * score 
            for dim, score in quality_breakdown.items()
        )
        
        logger.debug(f"è´¨é‡è¯„ä¼°å®Œæˆ: æ€»åˆ†={total_score:.3f}, æ˜ç»†={quality_breakdown}")
        
        return total_score, quality_breakdown
    
    def _assess_user_feedback(self, feedback: Optional[Dict[str, Any]]) -> float:
        """è¯„ä¼°ç”¨æˆ·åé¦ˆ"""
        if not feedback:
            return 0.5  # ä¸­æ€§åˆ†æ•°
        
        feedback_type = feedback.get("type", FeedbackType.NEUTRAL.value)
        
        if feedback_type == FeedbackType.THUMBS_UP.value:
            return 0.9
        elif feedback_type == FeedbackType.THUMBS_DOWN.value:
            return 0.1
        elif feedback_type == FeedbackType.CORRECTION.value:
            return 0.3
        elif feedback_type == FeedbackType.ENHANCEMENT.value:
            return 0.7
        else:
            return 0.5
    
    async def _assess_consistency(self, 
                                response: LearningResponse,
                                existing_knowledge: List[str] = None) -> float:
        """è¯„ä¼°ä¸€è‡´æ€§"""
        if not existing_knowledge:
            return 0.7  # é»˜è®¤åˆ†æ•°
        
        # ç®€åŒ–çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çŸ›ç›¾
        response_text = response.llm_response.lower()
        
        contradiction_indicators = [
            "ä½†æ˜¯", "ç„¶è€Œ", "ç›¸å", "ä¸æ˜¯", "é”™è¯¯", "ä¸å¯¹"
        ]
        
        contradiction_count = sum(
            1 for indicator in contradiction_indicators 
            if indicator in response_text
        )
        
        # çŸ›ç›¾æŒ‡ç¤ºè¯è¶Šå¤šï¼Œä¸€è‡´æ€§è¶Šä½
        consistency_score = max(0.1, 1.0 - contradiction_count * 0.2)
        
        return consistency_score
    
    def _assess_completeness(self, response: LearningResponse) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        response_text = response.llm_response
        
        # åŸºäºé•¿åº¦å’Œç»“æ„çš„å¯å‘å¼è¯„ä¼°
        length_score = min(1.0, len(response_text) / 500)  # 500å­—ç¬¦ä¸ºæ»¡åˆ†
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æ„åŒ–å†…å®¹
        structure_indicators = [
            "ï¼š", "ã€‚", "ï¼›", "ã€", "\n", "1.", "2.", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å"
        ]
        
        structure_count = sum(
            1 for indicator in structure_indicators 
            if indicator in response_text
        )
        
        structure_score = min(1.0, structure_count / 5)  # 5ä¸ªç»“æ„æŒ‡ç¤ºè¯ä¸ºæ»¡åˆ†
        
        return 0.6 * length_score + 0.4 * structure_score
    
    def _assess_verifiability(self, response: LearningResponse) -> float:
        """è¯„ä¼°å¯éªŒè¯æ€§"""
        response_text = response.llm_response
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“ä¿¡æ¯
        verifiable_indicators = [
            "æ ¹æ®", "æ–‡æ¡£", "æ ‡å‡†", "è§„èŒƒ", "ç¤ºä¾‹", "ä»£ç ", "å‚æ•°", "é…ç½®"
        ]
        
        verifiable_count = sum(
            1 for indicator in verifiable_indicators 
            if indicator in response_text
        )
        
        return min(1.0, verifiable_count / 3)  # 3ä¸ªå¯éªŒè¯æŒ‡ç¤ºè¯ä¸ºæ»¡åˆ†
    
    def _assess_relevance(self, response: LearningResponse) -> float:
        """è¯„ä¼°ç›¸å…³æ€§"""
        query_words = set(response.query.lower().split())
        response_words = set(response.llm_response.lower().split())
        
        # è®¡ç®—è¯æ±‡é‡å åº¦
        if not query_words:
            return 0.5
        
        overlap = len(query_words.intersection(response_words))
        relevance_score = overlap / len(query_words)
        
        return min(1.0, relevance_score)


class KnowledgeConflictResolver:
    """çŸ¥è¯†å†²çªè§£å†³å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
    
    async def resolve_knowledge_conflict(self, 
                                       existing_knowledge: Dict[str, Any],
                                       new_knowledge: Dict[str, Any]) -> Dict[str, Any]:
        """è§£å†³çŸ¥è¯†å†²çª"""
        
        # ç­–ç•¥1: æ—¶æ•ˆæ€§ä¼˜å…ˆ
        if self._is_time_sensitive(existing_knowledge, new_knowledge):
            return self._apply_temporal_resolution(existing_knowledge, new_knowledge)
        
        # ç­–ç•¥2: è´¨é‡ä¼˜å…ˆ
        if self._has_quality_difference(existing_knowledge, new_knowledge):
            return self._apply_quality_resolution(existing_knowledge, new_knowledge)
        
        # ç­–ç•¥3: ç”¨æˆ·åå¥½ä¼˜å…ˆ
        if self._has_user_preference(existing_knowledge, new_knowledge):
            return self._apply_preference_resolution(existing_knowledge, new_knowledge)
        
        # é»˜è®¤ç­–ç•¥: åˆå¹¶çŸ¥è¯†
        return self._merge_knowledge(existing_knowledge, new_knowledge)
    
    def _is_time_sensitive(self, existing: Dict, new: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæ—¶é—´æ•æ„Ÿçš„çŸ¥è¯†"""
        time_sensitive_keywords = ["æœ€æ–°", "å½“å‰", "ç°åœ¨", "ä»Šå¤©", "ç‰ˆæœ¬", "æ›´æ–°"]
        
        existing_text = str(existing.get("content", "")).lower()
        new_text = str(new.get("content", "")).lower()
        
        return any(keyword in existing_text or keyword in new_text 
                  for keyword in time_sensitive_keywords)
    
    def _apply_temporal_resolution(self, existing: Dict, new: Dict) -> Dict:
        """åº”ç”¨æ—¶é—´ä¼˜å…ˆè§£å†³ç­–ç•¥"""
        existing_time = existing.get("timestamp", datetime.min)
        new_time = new.get("timestamp", datetime.min)
        
        if isinstance(existing_time, str):
            existing_time = datetime.fromisoformat(existing_time)
        if isinstance(new_time, str):
            new_time = datetime.fromisoformat(new_time)
        
        # è¿”å›è¾ƒæ–°çš„çŸ¥è¯†ï¼Œä½†ä¿ç•™å†å²è®°å½•
        if new_time > existing_time:
            result = new.copy()
            result["superseded_knowledge"] = existing
            return result
        else:
            return existing
    
    def _has_quality_difference(self, existing: Dict, new: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è´¨é‡å·®å¼‚"""
        existing_quality = existing.get("quality_score", 0.5)
        new_quality = new.get("quality_score", 0.5)
        
        return abs(existing_quality - new_quality) > 0.2
    
    def _apply_quality_resolution(self, existing: Dict, new: Dict) -> Dict:
        """åº”ç”¨è´¨é‡ä¼˜å…ˆè§£å†³ç­–ç•¥"""
        existing_quality = existing.get("quality_score", 0.5)
        new_quality = new.get("quality_score", 0.5)
        
        if new_quality > existing_quality:
            return new
        else:
            return existing
    
    def _has_user_preference(self, existing: Dict, new: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·åå¥½"""
        # ç®€åŒ–å®ç°ï¼šæ£€æŸ¥ç”¨æˆ·åé¦ˆ
        existing_feedback = existing.get("user_feedback", {})
        new_feedback = new.get("user_feedback", {})
        
        return (existing_feedback.get("type") == FeedbackType.THUMBS_UP.value or 
                new_feedback.get("type") == FeedbackType.THUMBS_UP.value)
    
    def _apply_preference_resolution(self, existing: Dict, new: Dict) -> Dict:
        """åº”ç”¨ç”¨æˆ·åå¥½ä¼˜å…ˆç­–ç•¥"""
        existing_feedback = existing.get("user_feedback", {})
        new_feedback = new.get("user_feedback", {})
        
        if new_feedback.get("type") == FeedbackType.THUMBS_UP.value:
            return new
        elif existing_feedback.get("type") == FeedbackType.THUMBS_UP.value:
            return existing
        else:
            return self._merge_knowledge(existing, new)
    
    def _merge_knowledge(self, existing: Dict, new: Dict) -> Dict:
        """åˆå¹¶çŸ¥è¯†"""
        merged = existing.copy()
        
        # åˆå¹¶å†…å®¹
        existing_content = existing.get("content", "")
        new_content = new.get("content", "")
        
        if new_content and new_content not in existing_content:
            merged["content"] = f"{existing_content}\n\nè¡¥å……ä¿¡æ¯ï¼š{new_content}"
        
        # ä¿ç•™æ›´é«˜çš„è´¨é‡åˆ†æ•°
        if new.get("quality_score", 0) > existing.get("quality_score", 0):
            merged["quality_score"] = new["quality_score"]
        
        # è®°å½•åˆå¹¶å†å²
        merged["merged_from"] = [existing.get("id"), new.get("id")]
        merged["merged_at"] = datetime.now().isoformat()
        
        return merged


class IntelligentKnowledgeEvolutionManager:
    """æ™ºèƒ½çŸ¥è¯†æ¼”åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, 
                 memory_system: MemorySystem,
                 knowledge_service: IntegratedKnowledgeService,
                 config: Dict[str, Any] = None):
        """åˆå§‹åŒ–çŸ¥è¯†æ¼”åŒ–ç®¡ç†å™¨"""
        
        self.memory_system = memory_system
        self.knowledge_service = knowledge_service
        self.config = config or {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.quality_assessor = KnowledgeQualityAssessor(self.config.get("quality", {}))
        self.conflict_resolver = KnowledgeConflictResolver(self.config.get("conflict", {}))
        
        # å­˜å‚¨æ¼”åŒ–ä¸­çš„å“åº”
        self.evolving_responses: Dict[str, LearningResponse] = {}
        
        # æ¼”åŒ–æŒ‡æ ‡
        self.metrics = KnowledgeEvolutionMetrics()
        
        # é…ç½®å‚æ•°
        self.consolidation_interval = self.config.get("consolidation_interval", 3600)  # 1å°æ—¶
        self.crystallization_interval = self.config.get("crystallization_interval", 86400)  # 24å°æ—¶
        
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # å¯åŠ¨åå°ä»»åŠ¡
        self._background_tasks = set()
        self._start_background_tasks()
    
    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        
        # å®šæœŸå·©å›ºä»»åŠ¡
        consolidation_task = asyncio.create_task(self._periodic_consolidation())
        self._background_tasks.add(consolidation_task)
        consolidation_task.add_done_callback(self._background_tasks.discard)
        
        # å®šæœŸå›ºåŒ–ä»»åŠ¡
        crystallization_task = asyncio.create_task(self._periodic_crystallization())
        self._background_tasks.add(crystallization_task)
        crystallization_task.add_done_callback(self._background_tasks.discard)
    
    async def process_llm_response(self, 
                                 query: str,
                                 llm_response: str,
                                 user_context: Dict[str, Any] = None) -> str:
        """å¤„ç†LLMå“åº”ï¼Œå¯åŠ¨çŸ¥è¯†æ¼”åŒ–æµç¨‹"""
        
        # ç”Ÿæˆå“åº”ID
        response_id = self._generate_response_id(query, llm_response)
        
        # åˆ›å»ºå­¦ä¹ å“åº”é¡¹
        learning_response = LearningResponse(
            response_id=response_id,
            query=query,
            llm_response=llm_response,
            user_context=user_context or {},
            stage=KnowledgeEvolutionStage.IMMEDIATE
        )
        
        # å­˜å‚¨åˆ°æ¼”åŒ–é˜Ÿåˆ—
        self.evolving_responses[response_id] = learning_response
        
        # Stage 1: å­˜å‚¨åˆ°Working Memory
        await self._store_to_working_memory(learning_response)
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics.total_responses += 1
        self.metrics.stage_distribution["immediate"] += 1
        
        self.logger.info(f"ğŸ“ å¯åŠ¨çŸ¥è¯†æ¼”åŒ–æµç¨‹: {response_id[:8]}...")
        
        return response_id
    
    async def collect_user_feedback(self, 
                                  response_id: str,
                                  feedback: Dict[str, Any]) -> UniversalResult:
        """æ”¶é›†ç”¨æˆ·åé¦ˆï¼Œæ¨è¿›çŸ¥è¯†æ¼”åŒ–"""
        
        if response_id not in self.evolving_responses:
            return UniversalResult(
                content="å“åº”IDä¸å­˜åœ¨",
                status=ResultStatus.FAILURE,
                error=ErrorInfo(
                    error_type="response_not_found",
                    error_message=f"Response ID {response_id} not found"
                )
            )
        
        learning_response = self.evolving_responses[response_id]
        learning_response.user_feedback = feedback
        learning_response.validation_count += 1
        
        # è¯„ä¼°è´¨é‡
        quality_score, quality_breakdown = await self.quality_assessor.assess_response_quality(
            learning_response
        )
        
        learning_response.quality_score = quality_score
        learning_response.quality_breakdown = quality_breakdown
        
        # Stage 2: æ ¹æ®è´¨é‡å†³å®šæ¼”åŒ–è·¯å¾„
        if quality_score > self.quality_assessor.high_quality_threshold:
            # é«˜è´¨é‡ï¼šå¿«é€Ÿé€šé“åˆ°è¯­ä¹‰è®°å¿†
            await self._fast_track_to_semantic_memory(learning_response)
        elif quality_score > self.quality_assessor.medium_quality_threshold:
            # ä¸­ç­‰è´¨é‡ï¼šæ ‡å‡†æƒ…æ™¯è®°å¿†æµç¨‹
            await self._promote_to_episodic_memory(learning_response)
        else:
            # ä½è´¨é‡ï¼šæ ‡è®°æˆ–ä¸¢å¼ƒ
            await self._mark_low_quality_response(learning_response)
        
        # æ›´æ–°ç”¨æˆ·æ»¡æ„åº¦æŒ‡æ ‡
        if feedback.get("type") == FeedbackType.THUMBS_UP.value:
            self.metrics.user_satisfaction_rate = (
                self.metrics.user_satisfaction_rate * 0.9 + 0.1
            )
        elif feedback.get("type") == FeedbackType.THUMBS_DOWN.value:
            self.metrics.user_satisfaction_rate = (
                self.metrics.user_satisfaction_rate * 0.9
            )
        
        self.logger.info(f"âœ… æ”¶é›†åé¦ˆå®Œæˆ: {response_id[:8]}... è´¨é‡={quality_score:.3f}")
        
        return UniversalResult(
            content="åé¦ˆæ”¶é›†å®Œæˆ",
            status=ResultStatus.SUCCESS,
            data={
                "quality_score": quality_score,
                "quality_breakdown": quality_breakdown,
                "evolution_stage": learning_response.stage.value
            }
        )
    
    async def _store_to_working_memory(self, learning_response: LearningResponse):
        """å­˜å‚¨åˆ°å·¥ä½œè®°å¿†"""
        
        # åˆ›å»ºè®°å¿†é¡¹
        memory_item = MemoryItem(
            item_id=learning_response.response_id,
            content={
                "query": learning_response.query,
                "response": learning_response.llm_response,
                "context": learning_response.user_context
            },
            memory_type=MemoryType.WORKING,
            importance=0.5,
            tags=["llm_response", "learning"],
            metadata={
                "stage": learning_response.stage.value,
                "timestamp": learning_response.timestamp.isoformat()
            }
        )
        
        # å­˜å‚¨åˆ°å·¥ä½œè®°å¿†
        await self.memory_system.working_memory.store(memory_item)
        
        self.logger.debug(f"ğŸ’¾ å·²å­˜å‚¨åˆ°å·¥ä½œè®°å¿†: {learning_response.response_id[:8]}...")
    
    async def _promote_to_episodic_memory(self, learning_response: LearningResponse):
        """æå‡åˆ°æƒ…æ™¯è®°å¿†"""
        
        learning_response.stage = KnowledgeEvolutionStage.EXPERIENTIAL
        
        # åˆ›å»ºæƒ…æ™¯è®°å¿†
        episode_data = {
            "event": f"ç”¨æˆ·è¯¢é—®: {learning_response.query}",
            "context": {
                "query": learning_response.query,
                "response": learning_response.llm_response,
                "user_context": learning_response.user_context,
                "user_feedback": learning_response.user_feedback,
                "quality_score": learning_response.quality_score
            },
            "participants": ["user", "assistant"],
            "importance": learning_response.quality_score or 0.5,
            "emotional_valence": self._calculate_emotional_valence(learning_response),
            "metadata": {
                "response_id": learning_response.response_id,
                "stage": learning_response.stage.value
            }
        }
        
        # å­˜å‚¨åˆ°æƒ…æ™¯è®°å¿†
        await self.memory_system.store_memory(
            episode_data,
            memory_type=MemoryType.EPISODIC,
            **episode_data
        )
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics.stage_distribution["experiential"] += 1
        
        self.logger.debug(f"ğŸ“š å·²æå‡åˆ°æƒ…æ™¯è®°å¿†: {learning_response.response_id[:8]}...")
    
    async def _fast_track_to_semantic_memory(self, learning_response: LearningResponse):
        """å¿«é€Ÿé€šé“åˆ°è¯­ä¹‰è®°å¿†"""
        
        # å…ˆç»è¿‡æƒ…æ™¯è®°å¿†
        await self._promote_to_episodic_memory(learning_response)
        
        # ç«‹å³æå–æ¦‚å¿µ
        await self._abstract_to_semantic_memory(learning_response)
    
    async def _abstract_to_semantic_memory(self, learning_response: LearningResponse):
        """æŠ½è±¡åˆ°è¯­ä¹‰è®°å¿†"""
        
        learning_response.stage = KnowledgeEvolutionStage.CONCEPTUAL
        
        # æå–æ¦‚å¿µå’Œå…³é”®è¯
        concepts = self._extract_concepts(learning_response)
        learning_response.extracted_concepts = concepts
        
        # ä¸ºæ¯ä¸ªæ¦‚å¿µåˆ›å»ºè¯­ä¹‰è®°å¿†
        for concept in concepts:
            concept_data = {
                "concept": concept,
                "definition": f"åŸºäºç”¨æˆ·æŸ¥è¯¢'{learning_response.query}'å­¦ä¹ çš„æ¦‚å¿µ",
                "properties": {
                    "source": "llm_learning",
                    "query": learning_response.query,
                    "response": learning_response.llm_response,
                    "quality_score": learning_response.quality_score
                },
                "confidence": learning_response.quality_score or 0.5,
                "source": "self_learning",
                "metadata": {
                    "response_id": learning_response.response_id,
                    "learning_timestamp": learning_response.timestamp.isoformat()
                }
            }
            
            # å­˜å‚¨åˆ°è¯­ä¹‰è®°å¿†
            await self.memory_system.store_memory(
                concept_data,
                memory_type=MemoryType.SEMANTIC,
                **concept_data
            )
        
        # æ›´æ–°æŒ‡æ ‡
        self.metrics.stage_distribution["conceptual"] += 1
        
        self.logger.debug(f"ğŸ§  å·²æŠ½è±¡åˆ°è¯­ä¹‰è®°å¿†: {learning_response.response_id[:8]}... æ¦‚å¿µ={concepts}")
    
    async def _crystallize_to_knowledge_base(self, learning_response: LearningResponse):
        """å›ºåŒ–åˆ°çŸ¥è¯†åº“"""
        
        learning_response.stage = KnowledgeEvolutionStage.CRYSTALLIZED
        
        # åˆ›å»ºçŸ¥è¯†é¡¹
        knowledge_content = self._format_knowledge_content(learning_response)
        
        knowledge_metadata = {
            "source": "self_learning",
            "quality_score": learning_response.quality_score,
            "validation_count": learning_response.validation_count,
            "user_feedback": learning_response.user_feedback,
            "learning_timestamp": learning_response.timestamp.isoformat(),
            "crystallization_timestamp": datetime.now().isoformat(),
            "concepts": learning_response.extracted_concepts,
            "original_query": learning_response.query
        }
        
        # æ£€æŸ¥å†²çª
        existing_knowledge = await self._find_conflicting_knowledge(learning_response)
        
        if existing_knowledge:
            # è§£å†³å†²çª
            resolved_knowledge = await self.conflict_resolver.resolve_knowledge_conflict(
                existing_knowledge, {
                    "content": knowledge_content,
                    "metadata": knowledge_metadata,
                    "quality_score": learning_response.quality_score,
                    "timestamp": learning_response.timestamp
                }
            )
            knowledge_content = resolved_knowledge.get("content", knowledge_content)
            knowledge_metadata.update(resolved_knowledge.get("metadata", {}))
        
        # æ·»åŠ åˆ°çŸ¥è¯†åº“
        try:
            doc_id = await self.knowledge_service.add_knowledge(
                content=knowledge_content,
                metadata=knowledge_metadata
            )
            
            learning_response.metadata["knowledge_doc_id"] = doc_id
            
            # æ›´æ–°æŒ‡æ ‡
            self.metrics.stage_distribution["crystallized"] += 1
            self.metrics.crystallization_rate = (
                self.metrics.stage_distribution["crystallized"] / 
                max(1, self.metrics.total_responses)
            )
            
            self.logger.info(f"ğŸ’ å·²å›ºåŒ–åˆ°çŸ¥è¯†åº“: {learning_response.response_id[:8]}... doc_id={doc_id[:8]}...")
            
        except Exception as e:
            self.logger.error(f"âŒ çŸ¥è¯†å›ºåŒ–å¤±è´¥: {e}")
    
    async def _mark_low_quality_response(self, learning_response: LearningResponse):
        """æ ‡è®°ä½è´¨é‡å“åº”"""
        
        # ä»ç„¶å­˜å‚¨åˆ°æƒ…æ™¯è®°å¿†ï¼Œä½†æ ‡è®°ä¸ºä½è´¨é‡
        learning_response.metadata["low_quality"] = True
        learning_response.metadata["quality_issues"] = self._identify_quality_issues(learning_response)
        
        await self._promote_to_episodic_memory(learning_response)
        
        self.logger.debug(f"âš ï¸ æ ‡è®°ä¸ºä½è´¨é‡: {learning_response.response_id[:8]}...")
    
    def _generate_response_id(self, query: str, response: str) -> str:
        """ç”Ÿæˆå“åº”ID"""
        content = f"{query}:{response}:{datetime.now().isoformat()}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _calculate_emotional_valence(self, learning_response: LearningResponse) -> float:
        """è®¡ç®—æƒ…æ„Ÿæ•ˆä»·"""
        if not learning_response.user_feedback:
            return 0.0
        
        feedback_type = learning_response.user_feedback.get("type", FeedbackType.NEUTRAL.value)
        
        if feedback_type == FeedbackType.THUMBS_UP.value:
            return 0.8
        elif feedback_type == FeedbackType.THUMBS_DOWN.value:
            return -0.8
        elif feedback_type == FeedbackType.ENHANCEMENT.value:
            return 0.3
        elif feedback_type == FeedbackType.CORRECTION.value:
            return -0.3
        else:
            return 0.0
    
    def _extract_concepts(self, learning_response: LearningResponse) -> List[str]:
        """æå–æ¦‚å¿µå’Œå…³é”®è¯"""
        
        # ç®€åŒ–çš„æ¦‚å¿µæå–
        response_text = learning_response.llm_response
        query_text = learning_response.query
        
        # æå–åè¯å’ŒæŠ€æœ¯æœ¯è¯­
        import re
        
        # æŠ€æœ¯æœ¯è¯­æ¨¡å¼
        tech_patterns = [
            r'\b[A-Z]{2,}\b',  # å¤§å†™ç¼©ç•¥è¯
            r'\b\w+(?:_\w+)+\b',  # ä¸‹åˆ’çº¿è¿æ¥çš„æœ¯è¯­
            r'\b\w+\.\w+\b',  # ç‚¹è¿æ¥çš„æœ¯è¯­
        ]
        
        concepts = set()
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, response_text)
            concepts.update(matches)
        
        # ä»æŸ¥è¯¢ä¸­æå–å…³é”®è¯
        query_words = [word for word in query_text.split() if len(word) > 3]
        concepts.update(query_words)
        
        return list(concepts)[:10]  # é™åˆ¶æ•°é‡
    
    def _format_knowledge_content(self, learning_response: LearningResponse) -> str:
        """æ ¼å¼åŒ–çŸ¥è¯†å†…å®¹"""
        
        return f"""é—®é¢˜ï¼š{learning_response.query}

å›ç­”ï¼š{learning_response.llm_response}

è´¨é‡è¯„ä¼°ï¼š{learning_response.quality_score:.3f}
å­¦ä¹ æ—¶é—´ï¼š{learning_response.timestamp.strftime('%Y-%m-%d %H:%M:%S')}
éªŒè¯æ¬¡æ•°ï¼š{learning_response.validation_count}
ç›¸å…³æ¦‚å¿µï¼š{', '.join(learning_response.extracted_concepts)}"""
    
    async def _find_conflicting_knowledge(self, learning_response: LearningResponse) -> Optional[Dict]:
        """æŸ¥æ‰¾å†²çªçŸ¥è¯†"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ¦‚å¿µé‡å æ£€æŸ¥å†²çª
        # å®é™…å®ç°å¯ä»¥æ›´å¤æ‚
        return None
    
    def _identify_quality_issues(self, learning_response: LearningResponse) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""
        issues = []
        
        if learning_response.quality_score and learning_response.quality_score < 0.4:
            for dim, score in learning_response.quality_breakdown.items():
                if score < 0.5:
                    issues.append(f"ä½{dim.value}åˆ†æ•°: {score:.3f}")
        
        return issues
    
    async def _periodic_consolidation(self):
        """å®šæœŸå·©å›ºä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(self.consolidation_interval)
                await self._consolidate_memories()
            except Exception as e:
                self.logger.error(f"âŒ å·©å›ºä»»åŠ¡å¤±è´¥: {e}")
    
    async def _periodic_crystallization(self):
        """å®šæœŸå›ºåŒ–ä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(self.crystallization_interval)
                await self._crystallize_mature_knowledge()
            except Exception as e:
                self.logger.error(f"âŒ å›ºåŒ–ä»»åŠ¡å¤±è´¥: {e}")
    
    async def _consolidate_memories(self):
        """å·©å›ºè®°å¿†"""
        # æŸ¥æ‰¾å¯ä»¥ä»æƒ…æ™¯è®°å¿†æå‡åˆ°è¯­ä¹‰è®°å¿†çš„å†…å®¹
        conceptual_candidates = []
        
        for response_id, learning_response in self.evolving_responses.items():
            if (learning_response.stage == KnowledgeEvolutionStage.EXPERIENTIAL and
                learning_response.quality_score and
                learning_response.quality_score > self.quality_assessor.medium_quality_threshold and
                learning_response.validation_count >= 2):
                
                conceptual_candidates.append(learning_response)
        
        for candidate in conceptual_candidates:
            await self._abstract_to_semantic_memory(candidate)
        
        if conceptual_candidates:
            self.logger.info(f"ğŸ”„ å·©å›ºäº† {len(conceptual_candidates)} ä¸ªè®°å¿†åˆ°è¯­ä¹‰å±‚")
    
    async def _crystallize_mature_knowledge(self):
        """å›ºåŒ–æˆç†ŸçŸ¥è¯†"""
        # æŸ¥æ‰¾å¯ä»¥å›ºåŒ–åˆ°çŸ¥è¯†åº“çš„å†…å®¹
        crystallization_candidates = []
        
        for response_id, learning_response in self.evolving_responses.items():
            if (learning_response.stage == KnowledgeEvolutionStage.CONCEPTUAL and
                learning_response.quality_score and
                learning_response.quality_score > self.quality_assessor.high_quality_threshold and
                learning_response.validation_count >= 3 and
                learning_response.access_count >= 2):
                
                crystallization_candidates.append(learning_response)
        
        for candidate in crystallization_candidates:
            await self._crystallize_to_knowledge_base(candidate)
        
        if crystallization_candidates:
            self.logger.info(f"ğŸ’ å›ºåŒ–äº† {len(crystallization_candidates)} ä¸ªçŸ¥è¯†åˆ°çŸ¥è¯†åº“")
    
    def get_evolution_metrics(self) -> Dict[str, Any]:
        """è·å–æ¼”åŒ–æŒ‡æ ‡"""
        return {
            "total_responses": self.metrics.total_responses,
            "stage_distribution": self.metrics.stage_distribution,
            "average_quality_score": self.metrics.average_quality_score,
            "crystallization_rate": self.metrics.crystallization_rate,
            "user_satisfaction_rate": self.metrics.user_satisfaction_rate,
            "knowledge_retention_rate": self.metrics.knowledge_retention_rate,
            "active_responses": len(self.evolving_responses)
        }
    
    async def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        # å–æ¶ˆåå°ä»»åŠ¡
        for task in self._background_tasks:
            task.cancel()
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        if self._background_tasks:
            await asyncio.gather(*self._background_tasks, return_exceptions=True)
        
        self.logger.info("ğŸ”„ çŸ¥è¯†æ¼”åŒ–ç®¡ç†å™¨å·²å…³é—­") 