"""
æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚é›†æˆæµ‹è¯•

æµ‹è¯•æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- IntelligentContextLayer ä¸»ç»„ä»¶
- Context Engineering ä¸Šä¸‹æ–‡å·¥ç¨‹
- RAG System æ£€ç´¢å¢å¼ºç”Ÿæˆ
- Knowledge Management çŸ¥è¯†ç®¡ç†  
- Quality Control è´¨é‡æ§åˆ¶

éªŒè¯ç»„ä»¶é—´çš„ååŒå·¥ä½œå’Œç«¯åˆ°ç«¯å¤„ç†æµç¨‹ã€‚
"""

import asyncio
import sys
import os
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from layers.framework.abstractions.context import UniversalContext
from layers.framework.abstractions.task import UniversalTask, TaskType, TaskPriority, TaskRequirements
from layers.intelligent_context.intelligent_context_layer import IntelligentContextLayer, ProcessingMode


async def test_comprehensive_intelligent_context():
    """ç»¼åˆæµ‹è¯•æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚é›†æˆæµ‹è¯•...")
    print("=" * 60)
    
    # 1. åˆå§‹åŒ–æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚
    print("\nğŸ“‹ 1. åˆå§‹åŒ–æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚")
    intelligent_context = IntelligentContextLayer()
    
    # è·å–åˆå§‹çŠ¶æ€
    initial_status = intelligent_context.get_status()
    print(f"âœ… æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚åˆå§‹åŒ–å®Œæˆ")
    print(f"   - å¤„ç†æ¨¡å¼: {initial_status['processing_mode']}")
    print(f"   - å±‚åç§°: {initial_status['layer_name']}")
    print(f"   - ç»„ä»¶æ•°é‡: {len(initial_status['components_status'])} ä¸ª")
    
    # 2. åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡
    print("\nğŸ“‹ 2. åˆ›å»ºæµ‹è¯•ä¸Šä¸‹æ–‡")
    test_context = UniversalContext({
        'user_prompt': 'I need to implement a machine learning model for sentiment analysis. Can you help me design the architecture and provide implementation guidance?',
        'history': [
            {'content': 'Previously discussed natural language processing basics', 'timestamp': '2025-01-01T10:00:00'},
            {'content': 'Explored different ML frameworks like TensorFlow and PyTorch', 'timestamp': '2025-01-01T10:15:00'}
        ],
        'memory': {
            'ml_experience': 'Intermediate level with supervised learning',
            'preferred_framework': 'PyTorch',
            'project_context': 'Building a customer feedback analysis system'
        },
        'retrieved_docs': [
            {
                'title': 'Sentiment Analysis with Deep Learning',
                'content': 'Deep learning approaches to sentiment analysis have shown significant improvements over traditional methods. LSTM and Transformer models are particularly effective.',
                'source': 'research_paper'
            },
            {
                'title': 'PyTorch for NLP',
                'content': 'PyTorch provides excellent support for NLP tasks with libraries like torchtext and transformers.',
                'source': 'documentation'
            }
        ],
        'keywords': ['machine learning', 'sentiment analysis', 'deep learning', 'pytorch', 'nlp'],
        'context_id': 'test_context_integration_001'
    })
    test_context.set('context_id', 'test_context_integration_001')
    
    print(f"âœ… æµ‹è¯•ä¸Šä¸‹æ–‡åˆ›å»ºå®Œæˆ")
    print(f"   - ä¸Šä¸‹æ–‡ID: {test_context.get('context_id')}")
    print(f"   - ç”¨æˆ·æç¤º: {test_context.get('user_prompt')[:50]}...")
    print(f"   - å†å²è®°å½•: {len(test_context.get('history', []))} æ¡")
    print(f"   - æ£€ç´¢æ–‡æ¡£: {len(test_context.get('retrieved_docs', []))} ä¸ª")
    
    # 3. åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    print("\nğŸ“‹ 3. åˆ›å»ºæµ‹è¯•ä»»åŠ¡")
    requirements = TaskRequirements(
        capabilities=['code_editor', 'documentation_search', 'model_training'],
        max_execution_time=7200,  # 2 hours in seconds
        memory_limit=8192,  # 8GB in MB
        preferred_framework='pytorch'
    )
    
    test_task = UniversalTask(
        content="Design and implement a sentiment analysis model using PyTorch, including data preprocessing, model architecture, training pipeline, and evaluation metrics.",
        task_type=TaskType.CODE_GENERATION,
        priority=TaskPriority.HIGH,
        requirements=requirements,
        context={
            'domain': 'machine_learning',
            'complexity': 'intermediate',
            'expected_output': 'complete_implementation'
        },
        task_id="task_sentiment_analysis_001"
    )
    
    print(f"âœ… æµ‹è¯•ä»»åŠ¡åˆ›å»ºå®Œæˆ")
    print(f"   - ä»»åŠ¡ID: {test_task.id}")
    print(f"   - ä»»åŠ¡ç±»å‹: {test_task.task_type.name}")
    print(f"   - ä¼˜å…ˆçº§: {test_task.priority.name}")
    print(f"   - æ‰€éœ€èƒ½åŠ›: {requirements.capabilities}")
    
    # 4. æµ‹è¯•é¡ºåºå¤„ç†æ¨¡å¼
    print("\nğŸ“‹ 4. æµ‹è¯•é¡ºåºå¤„ç†æ¨¡å¼")
    print("ğŸ”„ æ‰§è¡Œé¡ºåºå¤„ç†...")
    
    # è®¾ç½®ä¸ºé¡ºåºæ¨¡å¼
    intelligent_context.configure({'processing_mode': 'sequential'})
    
    sequential_result = await intelligent_context.process_context(
        context=test_context,
        task=test_task
    )
    
    print(f"âœ… é¡ºåºå¤„ç†å®Œæˆ")
    print(f"   - å¤„ç†çŠ¶æ€: {sequential_result['status']}")
    print(f"   - å¤„ç†æ—¶é—´: {sequential_result['processing_time']:.2f}ç§’")
    print(f"   - è´¨é‡åˆ†æ•°: {sequential_result['quality_score']:.2f}")
    print(f"   - ç»„ä»¶ç»“æœ: {len(sequential_result['component_results'])} ä¸ª")
    
    # æ˜¾ç¤ºå„ç»„ä»¶å¤„ç†ç»“æœ
    for component, result in sequential_result['component_results'].items():
        print(f"     - {component}: æˆåŠŸ={result.get('success', False)}, æ—¶é—´={result.get('processing_time', 0):.2f}s")
    
    # 5. æµ‹è¯•å¹¶è¡Œå¤„ç†æ¨¡å¼
    print("\nğŸ“‹ 5. æµ‹è¯•å¹¶è¡Œå¤„ç†æ¨¡å¼")
    print("ğŸ”„ æ‰§è¡Œå¹¶è¡Œå¤„ç†...")
    
    # è®¾ç½®ä¸ºå¹¶è¡Œæ¨¡å¼
    intelligent_context.configure({'processing_mode': 'parallel'})
    
    parallel_result = await intelligent_context.process_context(
        context=test_context,
        task=test_task
    )
    
    print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ")
    print(f"   - å¤„ç†çŠ¶æ€: {parallel_result['status']}")
    print(f"   - å¤„ç†æ—¶é—´: {parallel_result['processing_time']:.2f}ç§’")
    print(f"   - è´¨é‡åˆ†æ•°: {parallel_result['quality_score']:.2f}")
    print(f"   - æ•ˆç‡æå‡: {((sequential_result['processing_time'] - parallel_result['processing_time']) / sequential_result['processing_time'] * 100):.1f}%")
    
    # 6. æµ‹è¯•è‡ªé€‚åº”å¤„ç†æ¨¡å¼
    print("\nğŸ“‹ 6. æµ‹è¯•è‡ªé€‚åº”å¤„ç†æ¨¡å¼")
    print("ğŸ”„ æ‰§è¡Œè‡ªé€‚åº”å¤„ç†...")
    
    # è®¾ç½®ä¸ºè‡ªé€‚åº”æ¨¡å¼
    intelligent_context.configure({'processing_mode': 'adaptive'})
    
    adaptive_result = await intelligent_context.process_context(
        context=test_context,
        task=test_task
    )
    
    print(f"âœ… è‡ªé€‚åº”å¤„ç†å®Œæˆ")
    print(f"   - å¤„ç†çŠ¶æ€: {adaptive_result['status']}")
    print(f"   - å¤„ç†æ—¶é—´: {adaptive_result['processing_time']:.2f}ç§’")
    print(f"   - è´¨é‡åˆ†æ•°: {adaptive_result['quality_score']:.2f}")
    print(f"   - é€‰æ‹©æ¨¡å¼: {adaptive_result.get('selected_mode', 'unknown')}")
    
    # 7. åˆ†æå¤„ç†åçš„ä¸Šä¸‹æ–‡
    print("\nğŸ“‹ 7. åˆ†æå¤„ç†åçš„ä¸Šä¸‹æ–‡")
    enhanced_context = adaptive_result['enhanced_context']
    
    # æ£€æŸ¥ä¸Šä¸‹æ–‡å·¥ç¨‹ç»“æœ
    context_quality = enhanced_context.get('context_quality_metrics', {})
    print(f"âœ… ä¸Šä¸‹æ–‡å·¥ç¨‹åˆ†æ:")
    print(f"   - ç›¸å…³æ€§åˆ†æ•°: {context_quality.get('relevance_score', 0):.2f}")
    print(f"   - è¿è´¯æ€§åˆ†æ•°: {context_quality.get('coherence_score', 0):.2f}")
    print(f"   - å®Œæ•´æ€§åˆ†æ•°: {context_quality.get('completeness_score', 0):.2f}")
    print(f"   - æ•ˆç‡åˆ†æ•°: {context_quality.get('efficiency_score', 0):.2f}")
    print(f"   - ä½¿ç”¨ç­–ç•¥: {context_quality.get('strategy_used', 'unknown')}")
    
    # æ£€æŸ¥RAGç³»ç»Ÿç»“æœ
    rag_metadata = enhanced_context.get('rag_metadata', {})
    print(f"âœ… RAGç³»ç»Ÿåˆ†æ:")
    print(f"   - æ£€ç´¢ç­–ç•¥: {rag_metadata.get('strategy_used', 'unknown')}")
    print(f"   - æ–‡æ¡£æ•°é‡: {rag_metadata.get('documents_count', 0)}")
    print(f"   - è´¨é‡åˆ†æ•°: {rag_metadata.get('quality_score', 0):.2f}")
    print(f"   - å¤„ç†æ—¶é—´: {rag_metadata.get('processing_time', 0):.2f}ç§’")
    
    # æ£€æŸ¥çŸ¥è¯†ç®¡ç†ç»“æœ
    km_info = enhanced_context.get('knowledge_management_info', {})
    print(f"âœ… çŸ¥è¯†ç®¡ç†åˆ†æ:")
    print(f"   - æ–°è®°å¿†åˆ›å»º: {km_info.get('new_memories_created', 0)} ä¸ª")
    print(f"   - ç›¸å…³è®°å¿†: {km_info.get('relevant_memories_found', 0)} ä¸ª")
    print(f"   - æ•´åˆåˆ†æ•°: {km_info.get('knowledge_integration_score', 0):.2f}")
    
    memory_stats = km_info.get('memory_layer_stats', {})
    for layer, count in memory_stats.items():
        print(f"     - {layer}: {count} ä¸ªè®°å¿†")
    
    # 8. æ€§èƒ½æŒ‡æ ‡åˆ†æ
    print("\nğŸ“‹ 8. æ€§èƒ½æŒ‡æ ‡åˆ†æ")
    
    # æ”¶é›†æ‰€æœ‰å¤„ç†ç»“æœçš„æŒ‡æ ‡
    results = {
        'sequential': sequential_result,
        'parallel': parallel_result,
        'adaptive': adaptive_result
    }
    
    print("âœ… å¤„ç†æ¨¡å¼æ€§èƒ½å¯¹æ¯”:")
    print(f"{'æ¨¡å¼':<12} {'æ—¶é—´(s)':<10} {'è´¨é‡':<8} {'æ•ˆç‡':<8}")
    print("-" * 40)
    
    for mode, result in results.items():
        time_taken = result['processing_time']
        quality = result['quality_score']
        efficiency = quality / time_taken if time_taken > 0 else 0
        print(f"{mode:<12} {time_taken:<10.2f} {quality:<8.2f} {efficiency:<8.2f}")
    
    # 9. è·å–æœ€ç»ˆçŠ¶æ€
    print("\nğŸ“‹ 9. è·å–æœ€ç»ˆçŠ¶æ€")
    final_status = intelligent_context.get_status()
    
    print(f"âœ… æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚æœ€ç»ˆçŠ¶æ€:")
    print(f"   - å±‚åç§°: {final_status['layer_name']}")
    print(f"   - å¤„ç†æ¨¡å¼: {final_status['processing_mode']}")
    metrics = final_status.get('metrics', {})
    print(f"   - æŒ‡æ ‡: {len(metrics)} ä¸ª")
    
    # æ˜¾ç¤ºå„ç»„ä»¶çŠ¶æ€
    components_status = final_status['components_status']
    print(f"\n   ğŸ“Š å„ç»„ä»¶çŠ¶æ€:")
    for component, status in components_status.items():
        print(f"     - {component}: {status}")
    
    # 10. æµ‹è¯•é”™è¯¯å¤„ç†
    print("\nğŸ“‹ 10. æµ‹è¯•é”™è¯¯å¤„ç†")
    
    # åˆ›å»ºä¸€ä¸ªå¯èƒ½å¯¼è‡´é”™è¯¯çš„ä¸Šä¸‹æ–‡
    error_context = UniversalContext({'invalid_data': None})
    error_task = UniversalTask(
        content="",  # ç©ºå†…å®¹å¯èƒ½å¯¼è‡´å¤„ç†é—®é¢˜
        task_type=TaskType.CODE_GENERATION,
        priority=TaskPriority.LOW,
        requirements=TaskRequirements(),
        context={},
        task_id="error_test_task"
    )
    
    error_result = await intelligent_context.process_context(
        context=error_context,
        task=error_task
    )
    
    print(f"âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆ:")
    print(f"   - å¤„ç†çŠ¶æ€: {error_result['status']}")
    print(f"   - æ˜¯å¦æœ‰é”™è¯¯å¤„ç†: {'error_info' in error_result}")
    
    if error_result['status'] == 'completed':
        print("   - ç³»ç»ŸæˆåŠŸå¤„ç†äº†æ½œåœ¨çš„é”™è¯¯æƒ…å†µ")
    else:
        print("   - ç³»ç»Ÿæ­£ç¡®è¯†åˆ«å¹¶æŠ¥å‘Šäº†é”™è¯¯æƒ…å†µ")
    
    # 11. æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ‰ æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚é›†æˆæµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   âœ… ç»„ä»¶åˆå§‹åŒ–: æˆåŠŸ")
    print(f"   âœ… é¡ºåºå¤„ç†: æˆåŠŸ ({sequential_result['processing_time']:.2f}s)")
    print(f"   âœ… å¹¶è¡Œå¤„ç†: æˆåŠŸ ({parallel_result['processing_time']:.2f}s)")
    print(f"   âœ… è‡ªé€‚åº”å¤„ç†: æˆåŠŸ ({adaptive_result['processing_time']:.2f}s)")
    print(f"   âœ… ä¸Šä¸‹æ–‡å·¥ç¨‹: æˆåŠŸ")
    print(f"   âœ… RAGç³»ç»Ÿ: æˆåŠŸ")
    print(f"   âœ… çŸ¥è¯†ç®¡ç†: æˆåŠŸ")
    print(f"   âœ… è´¨é‡æ§åˆ¶: æˆåŠŸ")
    print(f"   âœ… é”™è¯¯å¤„ç†: æˆåŠŸ")
    
    # æ€§èƒ½äº®ç‚¹
    best_mode = min(results.keys(), key=lambda k: results[k]['processing_time'])
    best_quality = max(results.keys(), key=lambda k: results[k]['quality_score'])
    
    print(f"\nğŸ† æ€§èƒ½äº®ç‚¹:")
    print(f"   - æœ€å¿«å¤„ç†æ¨¡å¼: {best_mode} ({results[best_mode]['processing_time']:.2f}s)")
    print(f"   - æœ€é«˜è´¨é‡æ¨¡å¼: {best_quality} ({results[best_quality]['quality_score']:.2f})")
    print(f"   - ç³»ç»ŸçŠ¶æ€: {final_status['layer_name']} æ­£å¸¸è¿è¡Œ")
    
    print(f"\nğŸš€ æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒ!")
    
    return {
        'test_passed': True,
        'performance_metrics': results,
        'final_status': final_status,
        'recommendations': [
            f"æ¨èä½¿ç”¨ {best_mode} æ¨¡å¼ä»¥è·å¾—æœ€ä½³æ€§èƒ½",
            f"æ¨èä½¿ç”¨ {best_quality} æ¨¡å¼ä»¥è·å¾—æœ€é«˜è´¨é‡",
            "æ‰€æœ‰æ ¸å¿ƒç»„ä»¶è¿è¡Œæ­£å¸¸ï¼Œå¯ä»¥æŠ•å…¥ä½¿ç”¨",
            "é”™è¯¯å¤„ç†æœºåˆ¶å·¥ä½œè‰¯å¥½ï¼Œç³»ç»Ÿå…·æœ‰è‰¯å¥½çš„é²æ£’æ€§"
        ]
    }


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        print("ğŸ§ª æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚é›†æˆæµ‹è¯•")
        print("æµ‹è¯•æ‰€æœ‰ç»„ä»¶çš„ååŒå·¥ä½œå’Œç«¯åˆ°ç«¯å¤„ç†èƒ½åŠ›")
        print("=" * 60)
        
        # æ‰§è¡Œç»¼åˆæµ‹è¯•
        test_result = await test_comprehensive_intelligent_context()
        
        if test_result['test_passed']:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ™ºèƒ½ä¸Šä¸‹æ–‡å±‚é›†æˆæµ‹è¯•æˆåŠŸå®Œæˆã€‚")
            return 0
        else:
            print("\nâŒ æµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 