"""
è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶ - ç³»ç»Ÿæ€§èƒ½è‡ªåŠ¨åŒ–æµ‹è¯•ä¸ä¼˜åŒ–

å®ç°å®Œæ•´çš„è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿï¼ŒåŒ…æ‹¬åŸºå‡†æµ‹è¯•ã€æ€§èƒ½ç›‘æ§ã€
è´¨é‡è¯„ä¼°ã€è¶‹åŠ¿åˆ†æå’Œè‡ªåŠ¨åŒ–ä¼˜åŒ–å»ºè®®ã€‚

Author: ADC Team
Date: 2024-12-19
Version: 1.0.0
"""

import asyncio
import json
import statistics
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from abc import ABC, abstractmethod
import logging
from collections import defaultdict
import numpy as np

logger = logging.getLogger(__name__)

class EvaluationMetricType(Enum):
    """è¯„ä¼°æŒ‡æ ‡ç±»å‹"""
    ROUTE_ACCURACY = "route_accuracy"
    RESPONSE_QUALITY = "response_quality"
    COST_EFFICIENCY = "cost_efficiency"
    USER_SATISFACTION = "user_satisfaction"
    SYSTEM_PERFORMANCE = "system_performance"
    CACHE_PERFORMANCE = "cache_performance"

class BenchmarkCategory(Enum):
    """åŸºå‡†æµ‹è¯•ç±»åˆ«"""
    FPGA_BASICS = "fpga_basics"
    ADVANCED_DESIGN = "advanced_design"
    TROUBLESHOOTING = "troubleshooting"
    CREATIVE_TASKS = "creative_tasks"
    MIXED_COMPLEXITY = "mixed_complexity"

class TestResult(Enum):
    """æµ‹è¯•ç»“æœçŠ¶æ€"""
    PASS = "pass"
    FAIL = "fail"
    WARNING = "warning"
    SKIP = "skip"

@dataclass
class BenchmarkQuestion:
    """åŸºå‡†æµ‹è¯•é—®é¢˜"""
    question_id: str
    category: BenchmarkCategory
    query: str
    expected_source: str
    expected_confidence_min: float
    expected_cost_max: float
    expected_latency_max: float
    user_role: str
    complexity_level: str
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    question_id: str
    query: str
    actual_source: str
    expected_source: str
    actual_confidence: float
    expected_confidence_min: float
    actual_cost: float
    expected_cost_max: float
    actual_latency: float
    expected_latency_max: float
    result_status: TestResult
    score: float
    reasoning: List[str]
    timestamp: datetime

@dataclass
class MetricResult:
    """æŒ‡æ ‡ç»“æœ"""
    metric_type: EvaluationMetricType
    value: float
    target: float
    status: TestResult
    trend: str  # "improving", "stable", "degrading"
    details: Dict[str, Any]

@dataclass
class EvaluationReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    report_id: str
    timestamp: datetime
    overall_score: float
    test_results: List[EvaluationResult]
    metric_results: List[MetricResult]
    performance_trends: Dict[str, List[float]]
    recommendations: List[str]
    next_evaluation_time: datetime
    summary: Dict[str, Any]

class EvaluationMetric(ABC):
    """è¯„ä¼°æŒ‡æ ‡æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    async def calculate(self, test_results: List[EvaluationResult]) -> MetricResult:
        """è®¡ç®—æŒ‡æ ‡"""
        pass
    
    @abstractmethod
    def get_target_value(self) -> float:
        """è·å–ç›®æ ‡å€¼"""
        pass

class RouteAccuracyMetric(EvaluationMetric):
    """è·¯ç”±å‡†ç¡®æ€§æŒ‡æ ‡"""
    
    async def calculate(self, test_results: List[EvaluationResult]) -> MetricResult:
        if not test_results:
            return MetricResult(
                metric_type=EvaluationMetricType.ROUTE_ACCURACY,
                value=0.0,
                target=self.get_target_value(),
                status=TestResult.FAIL,
                trend="unknown",
                details={"total_tests": 0}
            )
        
        correct_routes = sum(1 for result in test_results 
                           if result.actual_source == result.expected_source)
        accuracy = correct_routes / len(test_results)
        
        status = TestResult.PASS if accuracy >= self.get_target_value() else TestResult.FAIL
        
        return MetricResult(
            metric_type=EvaluationMetricType.ROUTE_ACCURACY,
            value=accuracy,
            target=self.get_target_value(),
            status=status,
            trend="stable",  # éœ€è¦å†å²æ•°æ®æ¥è®¡ç®—è¶‹åŠ¿
            details={
                "total_tests": len(test_results),
                "correct_routes": correct_routes,
                "accuracy_percentage": accuracy * 100
            }
        )
    
    def get_target_value(self) -> float:
        return 0.85  # 85%å‡†ç¡®ç‡ç›®æ ‡

class ResponseQualityMetric(EvaluationMetric):
    """å“åº”è´¨é‡æŒ‡æ ‡"""
    
    async def calculate(self, test_results: List[EvaluationResult]) -> MetricResult:
        if not test_results:
            return MetricResult(
                metric_type=EvaluationMetricType.RESPONSE_QUALITY,
                value=0.0,
                target=self.get_target_value(),
                status=TestResult.FAIL,
                trend="unknown",
                details={"total_tests": 0}
            )
        
        # åŸºäºç½®ä¿¡åº¦è¯„ä¼°è´¨é‡
        confidence_scores = [result.actual_confidence for result in test_results]
        avg_confidence = statistics.mean(confidence_scores)
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æœ€ä½ç½®ä¿¡åº¦è¦æ±‚
        meets_min_confidence = sum(1 for result in test_results 
                                 if result.actual_confidence >= result.expected_confidence_min)
        confidence_compliance = meets_min_confidence / len(test_results)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality_score = (avg_confidence + confidence_compliance) / 2
        
        status = TestResult.PASS if quality_score >= self.get_target_value() else TestResult.FAIL
        
        return MetricResult(
            metric_type=EvaluationMetricType.RESPONSE_QUALITY,
            value=quality_score,
            target=self.get_target_value(),
            status=status,
            trend="stable",
            details={
                "average_confidence": avg_confidence,
                "confidence_compliance": confidence_compliance,
                "min_confidence": min(confidence_scores),
                "max_confidence": max(confidence_scores)
            }
        )
    
    def get_target_value(self) -> float:
        return 0.80  # 80%è´¨é‡ç›®æ ‡

class CostEfficiencyMetric(EvaluationMetric):
    """æˆæœ¬æ•ˆç‡æŒ‡æ ‡"""
    
    async def calculate(self, test_results: List[EvaluationResult]) -> MetricResult:
        if not test_results:
            return MetricResult(
                metric_type=EvaluationMetricType.COST_EFFICIENCY,
                value=0.0,
                target=self.get_target_value(),
                status=TestResult.FAIL,
                trend="unknown",
                details={"total_tests": 0}
            )
        
        # è®¡ç®—æˆæœ¬åˆè§„æ€§
        within_budget = sum(1 for result in test_results 
                          if result.actual_cost <= result.expected_cost_max)
        cost_compliance = within_budget / len(test_results)
        
        # è®¡ç®—å¹³å‡æˆæœ¬æ•ˆç‡
        total_actual_cost = sum(result.actual_cost for result in test_results)
        total_expected_cost = sum(result.expected_cost_max for result in test_results)
        cost_efficiency = 1 - (total_actual_cost / max(total_expected_cost, 0.001))
        
        # ç»¼åˆæ•ˆç‡åˆ†æ•°
        efficiency_score = (cost_compliance + max(0, cost_efficiency)) / 2
        
        status = TestResult.PASS if efficiency_score >= self.get_target_value() else TestResult.FAIL
        
        return MetricResult(
            metric_type=EvaluationMetricType.COST_EFFICIENCY,
            value=efficiency_score,
            target=self.get_target_value(),
            status=status,
            trend="stable",
            details={
                "cost_compliance": cost_compliance,
                "total_actual_cost": total_actual_cost,
                "total_expected_cost": total_expected_cost,
                "average_cost": total_actual_cost / len(test_results)
            }
        )
    
    def get_target_value(self) -> float:
        return 0.75  # 75%æˆæœ¬æ•ˆç‡ç›®æ ‡

class SystemPerformanceMetric(EvaluationMetric):
    """ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    
    async def calculate(self, test_results: List[EvaluationResult]) -> MetricResult:
        if not test_results:
            return MetricResult(
                metric_type=EvaluationMetricType.SYSTEM_PERFORMANCE,
                value=0.0,
                target=self.get_target_value(),
                status=TestResult.FAIL,
                trend="unknown",
                details={"total_tests": 0}
            )
        
        # è®¡ç®—å»¶è¿Ÿåˆè§„æ€§
        within_latency = sum(1 for result in test_results 
                           if result.actual_latency <= result.expected_latency_max)
        latency_compliance = within_latency / len(test_results)
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        latencies = [result.actual_latency for result in test_results]
        avg_latency = statistics.mean(latencies)
        expected_avg_latency = statistics.mean([result.expected_latency_max for result in test_results])
        
        # æ€§èƒ½åˆ†æ•°ï¼ˆå»¶è¿Ÿè¶Šä½è¶Šå¥½ï¼‰
        performance_ratio = min(1.0, expected_avg_latency / max(avg_latency, 0.001))
        performance_score = (latency_compliance + performance_ratio) / 2
        
        status = TestResult.PASS if performance_score >= self.get_target_value() else TestResult.FAIL
        
        return MetricResult(
            metric_type=EvaluationMetricType.SYSTEM_PERFORMANCE,
            value=performance_score,
            target=self.get_target_value(),
            status=status,
            trend="stable",
            details={
                "latency_compliance": latency_compliance,
                "average_latency": avg_latency,
                "expected_average_latency": expected_avg_latency,
                "min_latency": min(latencies),
                "max_latency": max(latencies)
            }
        )
    
    def get_target_value(self) -> float:
        return 0.85  # 85%æ€§èƒ½ç›®æ ‡

class AutomatedEvaluationFramework:
    """è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶"""
    
    def __init__(
        self,
        benchmark_file_path: Optional[str] = None,
        evaluation_interval_hours: int = 24,
        history_retention_days: int = 30
    ):
        self.benchmark_file_path = benchmark_file_path
        self.evaluation_interval_hours = evaluation_interval_hours
        self.history_retention_days = history_retention_days
        
        # åŸºå‡†æµ‹è¯•é—®é¢˜
        self.benchmark_questions: List[BenchmarkQuestion] = []
        
        # è¯„ä¼°æŒ‡æ ‡
        self.evaluation_metrics = [
            RouteAccuracyMetric(),
            ResponseQualityMetric(),
            CostEfficiencyMetric(),
            SystemPerformanceMetric()
        ]
        
        # å†å²æ•°æ®
        self.evaluation_history: List[EvaluationReport] = []
        
        # æ€§èƒ½è¶‹åŠ¿
        self.performance_trends: Dict[str, List[Tuple[datetime, float]]] = defaultdict(list)
        
        logger.info("ğŸ”¬ è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize(self):
        """åˆå§‹åŒ–è¯„ä¼°æ¡†æ¶"""
        try:
            # åŠ è½½åŸºå‡†æµ‹è¯•é—®é¢˜
            await self._load_benchmark_questions()
            
            # å¯åŠ¨å®šæœŸè¯„ä¼°ä»»åŠ¡
            asyncio.create_task(self._periodic_evaluation())
            
            logger.info("âœ… è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _load_benchmark_questions(self):
        """åŠ è½½åŸºå‡†æµ‹è¯•é—®é¢˜"""
        # å†…ç½®åŸºå‡†æµ‹è¯•é—®é¢˜
        self.benchmark_questions = [
            # FPGAåŸºç¡€é—®é¢˜
            BenchmarkQuestion(
                question_id="fpga_001",
                category=BenchmarkCategory.FPGA_BASICS,
                query="ä»€ä¹ˆæ˜¯FPGAï¼Ÿ",
                expected_source="local_kb",
                expected_confidence_min=0.85,
                expected_cost_max=0.2,
                expected_latency_max=1.0,
                user_role="beginner",
                complexity_level="simple",
                tags=["concept", "basic"]
            ),
            BenchmarkQuestion(
                question_id="fpga_002",
                category=BenchmarkCategory.FPGA_BASICS,
                query="FPGAå’ŒASICæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
                expected_source="local_kb",
                expected_confidence_min=0.80,
                expected_cost_max=0.3,
                expected_latency_max=1.5,
                user_role="intermediate",
                complexity_level="moderate",
                tags=["comparison", "basic"]
            ),
            
            # é«˜çº§è®¾è®¡é—®é¢˜
            BenchmarkQuestion(
                question_id="fpga_003",
                category=BenchmarkCategory.ADVANCED_DESIGN,
                query="å¦‚ä½•ä¼˜åŒ–FPGAè®¾è®¡çš„æ—¶åºæ€§èƒ½ï¼Ÿ",
                expected_source="ai_training",
                expected_confidence_min=0.75,
                expected_cost_max=1.5,
                expected_latency_max=3.0,
                user_role="expert",
                complexity_level="complex",
                tags=["optimization", "timing"]
            ),
            BenchmarkQuestion(
                question_id="fpga_004",
                category=BenchmarkCategory.ADVANCED_DESIGN,
                query="è®¾è®¡ä¸€ä¸ªé«˜æ€§èƒ½çš„8ä½è®¡æ•°å™¨",
                expected_source="ai_training",
                expected_confidence_min=0.70,
                expected_cost_max=2.0,
                expected_latency_max=4.0,
                user_role="expert",
                complexity_level="complex",
                tags=["design", "counter"]
            ),
            
            # æ•…éšœæ’æŸ¥é—®é¢˜
            BenchmarkQuestion(
                question_id="fpga_005",
                category=BenchmarkCategory.TROUBLESHOOTING,
                query="FPGAç»¼åˆå¤±è´¥ï¼Œæ—¶åºä¸æ»¡è¶³æ€ä¹ˆåŠï¼Ÿ",
                expected_source="local_kb",
                expected_confidence_min=0.80,
                expected_cost_max=0.5,
                expected_latency_max=2.0,
                user_role="intermediate",
                complexity_level="moderate",
                tags=["troubleshooting", "timing"]
            ),
            
            # åˆ›é€ æ€§ä»»åŠ¡
            BenchmarkQuestion(
                question_id="fpga_006",
                category=BenchmarkCategory.CREATIVE_TASKS,
                query="è®¾è®¡ä¸€ä¸ªåˆ›æ–°çš„FPGAå›¾åƒå¤„ç†æ¶æ„",
                expected_source="ai_training",
                expected_confidence_min=0.65,
                expected_cost_max=3.0,
                expected_latency_max=5.0,
                user_role="researcher",
                complexity_level="complex",
                tags=["creative", "image_processing"]
            )
        ]
        
        logger.info(f"ğŸ“š åŠ è½½äº† {len(self.benchmark_questions)} ä¸ªåŸºå‡†æµ‹è¯•é—®é¢˜")
    
    async def run_evaluation(self, router=None) -> EvaluationReport:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        start_time = datetime.now()
        report_id = f"eval_{start_time.strftime('%Y%m%d_%H%M%S')}"
        
        logger.info(f"ğŸ”¬ å¼€å§‹è¿è¡Œè¯„ä¼° - {report_id}")
        
        try:
            # è¿è¡ŒåŸºå‡†æµ‹è¯•
            test_results = await self._run_benchmark_tests(router)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            metric_results = await self._calculate_metrics(test_results)
            
            # åˆ†ææ€§èƒ½è¶‹åŠ¿
            performance_trends = await self._analyze_performance_trends(metric_results)
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = await self._generate_recommendations(metric_results, test_results)
            
            # è®¡ç®—æ€»ä½“è¯„åˆ†
            overall_score = self._calculate_overall_score(metric_results)
            
            # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            report = EvaluationReport(
                report_id=report_id,
                timestamp=start_time,
                overall_score=overall_score,
                test_results=test_results,
                metric_results=metric_results,
                performance_trends=performance_trends,
                recommendations=recommendations,
                next_evaluation_time=start_time + timedelta(hours=self.evaluation_interval_hours),
                summary=self._generate_summary(metric_results, test_results)
            )
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            self.evaluation_history.append(report)
            
            # æ›´æ–°æ€§èƒ½è¶‹åŠ¿
            await self._update_performance_trends(metric_results)
            
            # æ¸…ç†è¿‡æœŸå†å²æ•°æ®
            await self._cleanup_expired_history()
            
            execution_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"âœ… è¯„ä¼°å®Œæˆ - {report_id} (è€—æ—¶: {execution_time:.2f}s)")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°è¿è¡Œå¤±è´¥: {e}")
            raise
    
    async def _run_benchmark_tests(self, router) -> List[EvaluationResult]:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        test_results = []
        
        for question in self.benchmark_questions:
            try:
                # æ¨¡æ‹Ÿè·¯ç”±å™¨è°ƒç”¨ï¼ˆåœ¨å®é™…å®ç°ä¸­ä¼šè°ƒç”¨çœŸå®çš„è·¯ç”±å™¨ï¼‰
                if router:
                    # çœŸå®è·¯ç”±å™¨è°ƒç”¨
                    decision = await router.route_query(question.query)
                    actual_source = decision.primary_source.value
                    actual_confidence = decision.confidence
                    actual_cost = decision.estimated_cost
                    actual_latency = decision.expected_latency
                else:
                    # æ¨¡æ‹Ÿç»“æœ
                    actual_source, actual_confidence, actual_cost, actual_latency = \
                        await self._simulate_routing_decision(question)
                
                # è¯„ä¼°ç»“æœ
                result = self._evaluate_test_result(
                    question, actual_source, actual_confidence, actual_cost, actual_latency
                )
                
                test_results.append(result)
                
                logger.debug(f"ğŸ§ª æµ‹è¯• {question.question_id}: {result.result_status.value}")
                
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯• {question.question_id} å¤±è´¥: {e}")
                # åˆ›å»ºå¤±è´¥çš„æµ‹è¯•ç»“æœ
                result = EvaluationResult(
                    question_id=question.question_id,
                    query=question.query,
                    actual_source="error",
                    expected_source=question.expected_source,
                    actual_confidence=0.0,
                    expected_confidence_min=question.expected_confidence_min,
                    actual_cost=999.0,
                    expected_cost_max=question.expected_cost_max,
                    actual_latency=999.0,
                    expected_latency_max=question.expected_latency_max,
                    result_status=TestResult.FAIL,
                    score=0.0,
                    reasoning=[f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {str(e)}"],
                    timestamp=datetime.now()
                )
                test_results.append(result)
        
        return test_results
    
    async def _simulate_routing_decision(self, question: BenchmarkQuestion) -> Tuple[str, float, float, float]:
        """æ¨¡æ‹Ÿè·¯ç”±å†³ç­–ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
        import random
        
        # åŸºäºé—®é¢˜å¤æ‚åº¦æ¨¡æ‹Ÿç»“æœ
        if question.complexity_level == "simple":
            source_options = ["local_kb", "local_kb", "ai_training"]  # åå‘local_kb
            confidence_base = 0.85
            cost_base = 0.15
            latency_base = 0.8
        elif question.complexity_level == "moderate":
            source_options = ["local_kb", "ai_training", "ai_training"]  # åå‘ai_training
            confidence_base = 0.75
            cost_base = 0.6
            latency_base = 1.5
        else:  # complex
            source_options = ["ai_training", "ai_training", "web_search"]
            confidence_base = 0.70
            cost_base = 1.2
            latency_base = 2.5
        
        # æ·»åŠ éšæœºå˜åŒ–
        actual_source = random.choice(source_options)
        actual_confidence = confidence_base + random.uniform(-0.1, 0.1)
        actual_cost = cost_base + random.uniform(-0.2, 0.3)
        actual_latency = latency_base + random.uniform(-0.5, 1.0)
        
        return actual_source, actual_confidence, actual_cost, actual_latency
    
    def _evaluate_test_result(
        self,
        question: BenchmarkQuestion,
        actual_source: str,
        actual_confidence: float,
        actual_cost: float,
        actual_latency: float
    ) -> EvaluationResult:
        """è¯„ä¼°æµ‹è¯•ç»“æœ"""
        reasoning = []
        score = 0.0
        
        # è¯„ä¼°è·¯ç”±å‡†ç¡®æ€§ (40%)
        if actual_source == question.expected_source:
            score += 0.4
            reasoning.append("âœ… è·¯ç”±æºæ­£ç¡®")
        else:
            reasoning.append(f"âŒ è·¯ç”±æºé”™è¯¯: æœŸæœ›{question.expected_source}, å®é™…{actual_source}")
        
        # è¯„ä¼°ç½®ä¿¡åº¦ (25%)
        if actual_confidence >= question.expected_confidence_min:
            confidence_score = min(1.0, actual_confidence / question.expected_confidence_min)
            score += 0.25 * confidence_score
            reasoning.append(f"âœ… ç½®ä¿¡åº¦æ»¡è¶³è¦æ±‚: {actual_confidence:.3f}")
        else:
            reasoning.append(f"âŒ ç½®ä¿¡åº¦ä¸è¶³: æœŸæœ›â‰¥{question.expected_confidence_min}, å®é™…{actual_confidence:.3f}")
        
        # è¯„ä¼°æˆæœ¬ (20%)
        if actual_cost <= question.expected_cost_max:
            cost_score = max(0.0, 1.0 - actual_cost / question.expected_cost_max)
            score += 0.2 * cost_score
            reasoning.append(f"âœ… æˆæœ¬æ§åˆ¶è‰¯å¥½: ${actual_cost:.3f}")
        else:
            reasoning.append(f"âŒ æˆæœ¬è¶…é¢„ç®—: æœŸæœ›â‰¤${question.expected_cost_max}, å®é™…${actual_cost:.3f}")
        
        # è¯„ä¼°å»¶è¿Ÿ (15%)
        if actual_latency <= question.expected_latency_max:
            latency_score = max(0.0, 1.0 - actual_latency / question.expected_latency_max)
            score += 0.15 * latency_score
            reasoning.append(f"âœ… å“åº”æ—¶é—´è‰¯å¥½: {actual_latency:.2f}s")
        else:
            reasoning.append(f"âŒ å“åº”æ—¶é—´è¿‡é•¿: æœŸæœ›â‰¤{question.expected_latency_max}s, å®é™…{actual_latency:.2f}s")
        
        # ç¡®å®šæµ‹è¯•çŠ¶æ€
        if score >= 0.8:
            result_status = TestResult.PASS
        elif score >= 0.6:
            result_status = TestResult.WARNING
        else:
            result_status = TestResult.FAIL
        
        return EvaluationResult(
            question_id=question.question_id,
            query=question.query,
            actual_source=actual_source,
            expected_source=question.expected_source,
            actual_confidence=actual_confidence,
            expected_confidence_min=question.expected_confidence_min,
            actual_cost=actual_cost,
            expected_cost_max=question.expected_cost_max,
            actual_latency=actual_latency,
            expected_latency_max=question.expected_latency_max,
            result_status=result_status,
            score=score,
            reasoning=reasoning,
            timestamp=datetime.now()
        )
    
    async def _calculate_metrics(self, test_results: List[EvaluationResult]) -> List[MetricResult]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        metric_results = []
        
        for metric in self.evaluation_metrics:
            try:
                result = await metric.calculate(test_results)
                metric_results.append(result)
                logger.debug(f"ğŸ“Š æŒ‡æ ‡ {metric.__class__.__name__}: {result.value:.3f}")
            except Exception as e:
                logger.error(f"âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥ {metric.__class__.__name__}: {e}")
        
        return metric_results
    
    async def _analyze_performance_trends(self, metric_results: List[MetricResult]) -> Dict[str, List[float]]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        trends = {}
        
        for metric_result in metric_results:
            metric_name = metric_result.metric_type.value
            
            # è·å–å†å²æ•°æ®
            historical_values = []
            for report in self.evaluation_history[-10:]:  # æœ€è¿‘10æ¬¡è¯„ä¼°
                for historical_metric in report.metric_results:
                    if historical_metric.metric_type == metric_result.metric_type:
                        historical_values.append(historical_metric.value)
                        break
            
            # æ·»åŠ å½“å‰å€¼
            historical_values.append(metric_result.value)
            trends[metric_name] = historical_values
        
        return trends
    
    async def _generate_recommendations(
        self,
        metric_results: List[MetricResult],
        test_results: List[EvaluationResult]
    ) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åˆ†æå„é¡¹æŒ‡æ ‡
        for metric_result in metric_results:
            if metric_result.status == TestResult.FAIL:
                if metric_result.metric_type == EvaluationMetricType.ROUTE_ACCURACY:
                    recommendations.append(
                        f"è·¯ç”±å‡†ç¡®æ€§åä½ ({metric_result.value:.1%})ï¼Œ"
                        "å»ºè®®è°ƒæ•´è·¯ç”±æƒé‡æˆ–å¢åŠ è®­ç»ƒæ•°æ®"
                    )
                elif metric_result.metric_type == EvaluationMetricType.RESPONSE_QUALITY:
                    recommendations.append(
                        f"å“åº”è´¨é‡éœ€è¦æ”¹å–„ ({metric_result.value:.1%})ï¼Œ"
                        "å»ºè®®ä¼˜åŒ–çŸ¥è¯†åº“å†…å®¹æˆ–æé«˜ç½®ä¿¡åº¦é˜ˆå€¼"
                    )
                elif metric_result.metric_type == EvaluationMetricType.COST_EFFICIENCY:
                    recommendations.append(
                        f"æˆæœ¬æ•ˆç‡æœ‰å¾…æå‡ ({metric_result.value:.1%})ï¼Œ"
                        "å»ºè®®å¢åŠ ç¼“å­˜å‘½ä¸­ç‡æˆ–ä¼˜åŒ–è·¯ç”±ç­–ç•¥"
                    )
                elif metric_result.metric_type == EvaluationMetricType.SYSTEM_PERFORMANCE:
                    recommendations.append(
                        f"ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ– ({metric_result.value:.1%})ï¼Œ"
                        "å»ºè®®æ£€æŸ¥ç³»ç»Ÿèµ„æºæˆ–ä¼˜åŒ–ç®—æ³•"
                    )
        
        # åˆ†æå¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹
        failed_tests = [r for r in test_results if r.result_status == TestResult.FAIL]
        if len(failed_tests) > len(test_results) * 0.2:  # è¶…è¿‡20%å¤±è´¥ç‡
            recommendations.append(
                f"æµ‹è¯•å¤±è´¥ç‡è¿‡é«˜ ({len(failed_tests)}/{len(test_results)})ï¼Œ"
                "å»ºè®®å…¨é¢æ£€æŸ¥ç³»ç»Ÿé…ç½®"
            )
        
        # åˆ†æç‰¹å®šç±»åˆ«çš„é—®é¢˜
        category_failures = defaultdict(int)
        for result in failed_tests:
            # æ‰¾åˆ°å¯¹åº”çš„é—®é¢˜ç±»åˆ«
            question = next((q for q in self.benchmark_questions 
                           if q.question_id == result.question_id), None)
            if question:
                category_failures[question.category.value] += 1
        
        for category, count in category_failures.items():
            if count >= 2:  # æŸç±»åˆ«å¤šæ¬¡å¤±è´¥
                recommendations.append(
                    f"{category} ç±»åˆ«é—®é¢˜é¢‘ç¹å¤±è´¥ï¼Œ"
                    "å»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–ç›¸å…³åŠŸèƒ½"
                )
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¿æŒå½“å‰é…ç½®")
        
        return recommendations
    
    def _calculate_overall_score(self, metric_results: List[MetricResult]) -> float:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        if not metric_results:
            return 0.0
        
        # å„æŒ‡æ ‡æƒé‡
        weights = {
            EvaluationMetricType.ROUTE_ACCURACY: 0.30,
            EvaluationMetricType.RESPONSE_QUALITY: 0.25,
            EvaluationMetricType.COST_EFFICIENCY: 0.20,
            EvaluationMetricType.SYSTEM_PERFORMANCE: 0.25
        }
        
        total_score = 0.0
        total_weight = 0.0
        
        for metric_result in metric_results:
            weight = weights.get(metric_result.metric_type, 0.0)
            total_score += metric_result.value * weight
            total_weight += weight
        
        return total_score / max(total_weight, 1.0)
    
    def _generate_summary(
        self,
        metric_results: List[MetricResult],
        test_results: List[EvaluationResult]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦"""
        passed_tests = sum(1 for r in test_results if r.result_status == TestResult.PASS)
        warning_tests = sum(1 for r in test_results if r.result_status == TestResult.WARNING)
        failed_tests = sum(1 for r in test_results if r.result_status == TestResult.FAIL)
        
        metric_summary = {}
        for metric_result in metric_results:
            metric_summary[metric_result.metric_type.value] = {
                'value': metric_result.value,
                'target': metric_result.target,
                'status': metric_result.status.value,
                'trend': metric_result.trend
            }
        
        return {
            'test_summary': {
                'total_tests': len(test_results),
                'passed': passed_tests,
                'warning': warning_tests,
                'failed': failed_tests,
                'pass_rate': passed_tests / len(test_results) if test_results else 0
            },
            'metric_summary': metric_summary,
            'evaluation_time': datetime.now().isoformat()
        }
    
    async def _update_performance_trends(self, metric_results: List[MetricResult]):
        """æ›´æ–°æ€§èƒ½è¶‹åŠ¿"""
        current_time = datetime.now()
        
        for metric_result in metric_results:
            metric_name = metric_result.metric_type.value
            self.performance_trends[metric_name].append((current_time, metric_result.value))
            
            # ä¿æŒæœ€è¿‘30å¤©çš„æ•°æ®
            cutoff_time = current_time - timedelta(days=30)
            self.performance_trends[metric_name] = [
                (time, value) for time, value in self.performance_trends[metric_name]
                if time >= cutoff_time
            ]
    
    async def _cleanup_expired_history(self):
        """æ¸…ç†è¿‡æœŸå†å²æ•°æ®"""
        cutoff_time = datetime.now() - timedelta(days=self.history_retention_days)
        self.evaluation_history = [
            report for report in self.evaluation_history
            if report.timestamp >= cutoff_time
        ]
    
    async def _periodic_evaluation(self):
        """å®šæœŸè¯„ä¼°ä»»åŠ¡"""
        while True:
            try:
                await asyncio.sleep(self.evaluation_interval_hours * 3600)
                
                logger.info("ğŸ• å¼€å§‹å®šæœŸè¯„ä¼°")
                report = await self.run_evaluation()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ä¸¥é‡é—®é¢˜
                if report.overall_score < 0.5:
                    logger.warning(f"âš ï¸ ç³»ç»Ÿè¯„åˆ†è¿‡ä½: {report.overall_score:.3f}")
                
                logger.info(f"âœ… å®šæœŸè¯„ä¼°å®Œæˆï¼Œæ€»ä½“è¯„åˆ†: {report.overall_score:.3f}")
                
            except Exception as e:
                logger.error(f"âŒ å®šæœŸè¯„ä¼°å¤±è´¥: {e}")
                # ç­‰å¾…1å°æ—¶åé‡è¯•
                await asyncio.sleep(3600)
    
    def get_latest_report(self) -> Optional[EvaluationReport]:
        """è·å–æœ€æ–°è¯„ä¼°æŠ¥å‘Š"""
        return self.evaluation_history[-1] if self.evaluation_history else None
    
    def get_performance_trends(self, days: int = 7) -> Dict[str, List[Tuple[datetime, float]]]:
        """è·å–æ€§èƒ½è¶‹åŠ¿æ•°æ®"""
        cutoff_time = datetime.now() - timedelta(days=days)
        
        trends = {}
        for metric_name, data_points in self.performance_trends.items():
            trends[metric_name] = [
                (time, value) for time, value in data_points
                if time >= cutoff_time
            ]
        
        return trends
    
    async def export_report(self, report: EvaluationReport, format: str = "json") -> str:
        """å¯¼å‡ºè¯„ä¼°æŠ¥å‘Š"""
        if format == "json":
            # åºåˆ—åŒ–ä¸ºJSON
            report_dict = {
                'report_id': report.report_id,
                'timestamp': report.timestamp.isoformat(),
                'overall_score': report.overall_score,
                'test_results': [
                    {
                        'question_id': r.question_id,
                        'query': r.query,
                        'actual_source': r.actual_source,
                        'expected_source': r.expected_source,
                        'score': r.score,
                        'status': r.result_status.value,
                        'reasoning': r.reasoning
                    }
                    for r in report.test_results
                ],
                'metric_results': [
                    {
                        'metric_type': m.metric_type.value,
                        'value': m.value,
                        'target': m.target,
                        'status': m.status.value,
                        'trend': m.trend
                    }
                    for m in report.metric_results
                ],
                'recommendations': report.recommendations,
                'summary': report.summary
            }
            
            return json.dumps(report_dict, indent=2, ensure_ascii=False)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")

# å·¥å‚å‡½æ•°
def create_evaluation_framework(
    evaluation_interval_hours: int = 24,
    history_retention_days: int = 30
) -> AutomatedEvaluationFramework:
    """åˆ›å»ºè‡ªåŠ¨åŒ–è¯„ä¼°æ¡†æ¶"""
    return AutomatedEvaluationFramework(
        evaluation_interval_hours=evaluation_interval_hours,
        history_retention_days=history_retention_days
    ) 