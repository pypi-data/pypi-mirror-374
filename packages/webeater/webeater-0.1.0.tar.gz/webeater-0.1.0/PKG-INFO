Metadata-Version: 2.4
Name: webeater
Version: 0.1.0
Summary: A web content extraction tool designed to fetch and process web pages efficiently
Author-email: Tiago Ribeiro <webeater@tiagoribeiro.pt>
Maintainer-email: Tiago Ribeiro <webeater@tiagoribeiro.pt>
License-Expression: MIT
Project-URL: Homepage, https://github.com/tiagrib/webeater
Project-URL: Repository, https://github.com/tiagrib/webeater.git
Project-URL: Issues, https://github.com/tiagrib/webeater/issues
Project-URL: Documentation, https://github.com/tiagrib/webeater#readme
Keywords: web,scraping,extraction,selenium,beautifulsoup,content
Classifier: Development Status :: 3 - Alpha
Classifier: Environment :: Console
Classifier: Environment :: Web Environment
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Information Technology
Classifier: Natural Language :: English
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Internet :: WWW/HTTP
Classifier: Topic :: Internet :: WWW/HTTP :: Browsers
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing :: Markup :: HTML
Classifier: Topic :: Utilities
Classifier: Typing :: Typed
Requires-Python: >=3.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: annotated-types>=0.7.0
Requires-Dist: attrs>=25.3.0
Requires-Dist: beautifulsoup4>=4.13.4
Requires-Dist: bs4>=0.0.2
Requires-Dist: certifi>=2025.8.3
Requires-Dist: cffi>=1.17.1
Requires-Dist: coloredlogs>=15.0.1
Requires-Dist: h11>=0.16.0
Requires-Dist: humanfriendly>=10.0
Requires-Dist: idna>=3.10
Requires-Dist: outcome>=1.3.0.post0
Requires-Dist: pycparser>=2.22
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pydantic_core>=2.33.2
Requires-Dist: pyreadline3>=3.5.4
Requires-Dist: PySocks>=1.7.1
Requires-Dist: selenium>=4.34.2
Requires-Dist: sniffio>=1.3.1
Requires-Dist: sortedcontainers>=2.4.0
Requires-Dist: soupsieve>=2.7
Requires-Dist: trio>=0.30.0
Requires-Dist: trio-websocket>=0.12.2
Requires-Dist: typing-inspection>=0.4.1
Requires-Dist: typing_extensions>=4.14.1
Requires-Dist: urllib3>=2.5.0
Requires-Dist: websocket-client>=1.8.0
Requires-Dist: wsproto>=1.2.0
Provides-Extra: dev
Requires-Dist: pytest>=6.0; extra == "dev"
Requires-Dist: pytest-asyncio; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: mypy; extra == "dev"
Requires-Dist: twine; extra == "dev"
Requires-Dist: build; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=6.0; extra == "test"
Requires-Dist: pytest-asyncio; extra == "test"
Dynamic: license-file

<img src="img/logo.png" alt="Logo" style="max-height: 150px;">

# WebEater (weat)

WebEater is a web content extraction tool designed to fetch and process web pages.\
It is made for developers and researchers who need to extract structured data from web pages efficiently.\
The tool goes straight to the point, focusing on extracting text and structured data from web pages,
while providing some additional configurations and hits for better effectiveness.

Its main purpose is to serve as a go-to-component that works out of the box for most general use cases.

As it's currently at an early stage, it may not cover all edge cases or complex scenarios.\
We welcome contributions and feedback to help improve its capabilities.

## Features
- Fetches web pages and extracts text content into Markdown format.
- Handles JavaScript-heavy pages using Selenium and BeautifulSoup

## Quick Start
To use WebEater, you can import the `WebeaterEngine` class and
create an instance of it.\
The engine will automatically load the necessary configurations
and provide methods to perform web content extraction actions.

Note that it must be loaded within an async context.

```
from webeater import WebeaterEngine

async def main():
    weat = await WebeaterEngine.create()
    content = weat.get(url="https://example.com")

    print(content)
```

## Help and Contributions

For questions or discussions about changes and new features, please start a new [Discussion in the Webeater GitHub repository](https://github.com/tiagrib/webeater/discussions).

If you find bugs or want to contribute, please open an [Issue](https://github.com/tiagrib/webeater/issues).

## Installation from Source

To install Web Eater from source code, you can clone the repository at:
```
https://github.com/tiagrib/webeater.git
```

then navigate to the project directory and install the required dependencies:

```
pip install -r requirements.txt
```
The current code was tested using python version 3.12.3, though other versions may work.


## Configuration and Advanced documentation
Web Eater uses a configuration file to manage its settings.
The configuration file is typically located at `config/weat.yaml`.

You can customize the settings in this file to suit your needs,
such as specifying the default user agent, timeout settings, and other parameters.

For more detailed documentation on configuration options and advanced usage,
please refer to the [Hints Documentation](hints/README.md).

    
    
