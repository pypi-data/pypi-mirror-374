"""
Base agent class for MAI-DxO specialized agents.

Provides common functionality and interface for all MAI-DxO agents
based on the Microsoft Research architecture.
"""

from abc import abstractmethod
from datetime import datetime
from typing import Any, Dict, List, Optional

from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_core.models import ChatCompletionClient
from autogen_core import CancellationToken

from ...information.gatekeeper import InformationGatekeeper
from ...models.mai_dxo import (
    ActionDecision,
    AgentMessage,
    AgentOutput,
    AgentRole,
    DecisionContext,
    Evidence,
    Hypothesis,
    InformationType,
    MessageType,
    NextAction,
    SearchResult,
    UserResponse,
)


class MAIDxOBaseAgent(AssistantAgent):
    """Base class for all MAI-DxO specialized agents.

    Provides common functionality for hypothesis management, evidence tracking,
    and communication with the Information Gatekeeper.

    Attributes:
        agent_role: The specialized role this agent performs
        domain_context: The domain context (e.g., 'business', 'technical')
        information_gatekeeper: Gateway for accessing external information
        current_hypotheses: List of hypotheses currently maintained by the agent
        gathered_evidence: List of evidence collected by the agent
        agent_outputs: List of all outputs generated by the agent
        decision_context: Current decision context being analyzed
        information_requests_made: Count of information requests made
    """

    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        agent_role: AgentRole,
        domain_context: str = "business",
        information_gatekeeper: Optional[InformationGatekeeper] = None,
        **kwargs,
    ):
        """Initialize the MAI-DxO base agent.

        Args:
            name: Display name for the agent
            model_client: Chat completion client for LLM interactions
            agent_role: The specialized role this agent performs
            domain_context: Domain context for specialized prompts
            information_gatekeeper: Gateway for accessing external information
            **kwargs: Additional arguments passed to parent class
        """
        # Get the specialized system prompt for this agent role
        system_message = self._get_system_prompt(agent_role, domain_context)

        super().__init__(
            name=name,
            model_client=model_client,
            system_message=system_message,
            **kwargs,
        )

        # Agent configuration
        self.agent_role = agent_role
        self.domain_context = domain_context
        self.information_gatekeeper = information_gatekeeper

        # Agent state tracking
        self.current_hypotheses: List[Hypothesis] = []
        self.gathered_evidence: List[Evidence] = []
        self.agent_outputs: List[AgentOutput] = []
        self.decision_context: Optional[DecisionContext] = None

        # Performance tracking
        self.information_requests_made = 0

    @abstractmethod
    def _get_system_prompt(self, agent_role: AgentRole, domain_context: str) -> str:
        """Get the specialized system prompt for this agent role and domain.

        Args:
            agent_role: The role this agent performs
            domain_context: The domain context for specialization

        Returns:
            String containing the system prompt for this agent
        """
        pass

    async def process_decision_context(self, context: DecisionContext) -> AgentOutput:
        """Process a decision context and generate initial analysis.

        Args:
            context: The decision context to analyze

        Returns:
            AgentOutput containing the agent's analysis and findings
        """
        self.decision_context = context

        # Generate initial analysis based on agent specialization using LLM
        initial_analysis = await self._generate_initial_analysis(context)

        # Create agent output
        output = AgentOutput(
            agent_role=self.agent_role,
            message_type=MessageType.ANALYSIS,
            content=initial_analysis,
            confidence_level=self._assess_confidence(initial_analysis),
            supporting_evidence=self.gathered_evidence.copy(),
            timestamp=datetime.now().isoformat(),
            reasoning=f"Initial {self.agent_role.value} analysis of the decision context",
        )

        self.agent_outputs.append(output)
        return output

    async def _call_llm_for_analysis(self, context: DecisionContext) -> str:
        """Call the LLM to generate analysis based on the decision context.

        Args:
            context: The decision context to analyze

        Returns:
            Raw text response from the LLM
        """
        # Create a comprehensive prompt for the LLM
        prompt = f"""
As a {self.agent_role.value} specialist, analyze the following decision context and provide your expert assessment:

**Problem Description:**
{context.problem_description}

**Domain:** {context.domain}

**Stakeholders:** {", ".join(context.stakeholders) if context.stakeholders else "Not specified"}

**Constraints:**
- Maximum information requests: {context.constraints.max_information_requests}
- Time limit: {context.constraints.time_limit} hours
- Quality threshold: {context.constraints.quality_threshold}
- Confidence threshold: {context.constraints.confidence_threshold}

**Initial Information:**
{context.initial_information if context.initial_information else "None provided"}

**Success Criteria:**
{", ".join(context.success_criteria) if context.success_criteria else "Not specified"}

Based on your {self.agent_role.value} expertise, provide a comprehensive analysis. Focus on your specialized perspective and deliver actionable insights that will contribute to the overall decision-making process.

Please structure your response as a detailed analysis covering the key aspects relevant to your role.
"""

        try:
            # Call the LLM using AutoGen's AssistantAgent interface
            message = TextMessage(content=prompt, source="user")
            response = await self.on_messages([message], CancellationToken())

            # Extract the response content
            if hasattr(response, "chat_message") and hasattr(
                response.chat_message, "content"
            ):
                return response.chat_message.content
            else:
                return "LLM response could not be extracted"

        except Exception as e:
            return f"Error calling LLM: {str(e)}"

    async def _call_llm_with_prompt(
        self, prompt: str, expected_format: str | None = None
    ) -> str:
        """Call the LLM with a specific prompt for specialized analysis.

        Args:
            prompt: The analysis prompt to send to the LLM
            expected_format: Optional description of expected response format

        Returns:
            Raw text response from the LLM
        """
        try:
            # Create the full prompt with formatting instructions
            full_prompt = f"""
{prompt}

{f"Please respond in the following format: {expected_format}" if expected_format else ""}

Provide a thorough analysis from your specialized perspective as a {self.agent_role.value}.
"""

            # Call the LLM using AutoGen's AssistantAgent interface
            message = TextMessage(content=full_prompt, source="user")
            response = await self.on_messages([message], CancellationToken())

            # Extract the response content
            if hasattr(response, "chat_message") and hasattr(
                response.chat_message, "content"
            ):
                return response.chat_message.content
            else:
                return "LLM response could not be extracted"

        except Exception as e:
            return f"Error calling LLM: {str(e)}"

    async def request_information(
        self,
        query: str,
        information_type: InformationType,
        priority: float = 0.5,
    ) -> Optional[str]:
        """Request information through the Information Gatekeeper.

        Args:
            query: The information query to request
            information_type: Type of information being requested
            priority: Priority level of the request (0.0-1.0)

        Returns:
            Information content if successful, None if failed
        """
        if not self.information_gatekeeper:
            return None

        try:
            context = {
                "agent_role": self.agent_role.value,
                "domain": self.domain_context,
                "current_hypotheses": len(self.current_hypotheses),
                "decision_context_id": self.decision_context.id
                if self.decision_context
                else None,
            }

            response = await self.information_gatekeeper.process_information_request(
                requesting_agent=self.agent_role,
                query=query,
                context=context,
                information_type=information_type,
                priority=priority,
            )

            # Track the request
            self.information_requests_made += 1

            # Convert response to evidence
            evidence = Evidence(
                source=response.source_type,
                content=response.information,
                reliability_score=response.reliability_score,
                relevance_score=priority,  # Use priority as relevance proxy
                timestamp=response.timestamp,
            )
            self.gathered_evidence.append(evidence)

            return response.information

        except Exception as e:
            # Handle information request failures gracefully
            print(f"Information request failed for {self.agent_role}: {e}")
            return None

    def update_hypotheses(self, new_hypotheses: List[Hypothesis]) -> None:
        """Update the current hypotheses based on new evidence.

        Args:
            new_hypotheses: List of updated hypotheses to replace current ones
        """
        self.current_hypotheses = new_hypotheses

    def get_current_hypotheses(self) -> List[Hypothesis]:
        """Get the current hypotheses maintained by this agent.

        Returns:
            List of current hypotheses (copy to prevent external modification)
        """
        return self.current_hypotheses.copy()

    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get performance metrics for this agent.

        Returns:
            Dictionary containing various performance metrics and statistics
        """
        return {
            "agent_role": self.agent_role.value,
            "information_requests_made": self.information_requests_made,
            "evidence_gathered": len(self.gathered_evidence),
            "outputs_generated": len(self.agent_outputs),
            "current_hypotheses_count": len(self.current_hypotheses),
            "average_confidence": (
                sum(output.confidence_level for output in self.agent_outputs)
                / len(self.agent_outputs)
                if self.agent_outputs
                else 0.0
            ),
        }

    @abstractmethod
    async def _generate_initial_analysis(
        self, context: DecisionContext
    ) -> Dict[str, Any]:
        """Generate initial analysis specific to this agent's role.

        This method should be implemented by specialized agents to provide their
        unique analysis perspective. Agents should use the _call_llm_for_analysis
        method to leverage LLM capabilities instead of returning hardcoded responses.

        Args:
            context: The decision context to analyze

        Returns:
            Dictionary containing the agent's specialized analysis
        """
        pass

    def _create_context_prompt(self, context: DecisionContext) -> str:
        """Create a comprehensive prompt from the decision context for LLM analysis.

        Args:
            context: The decision context to convert to prompt

        Returns:
            String prompt describing the context for LLM analysis
        """
        prompt_parts = [
            f"Decision Context: {context.description}",
            f"Domain: {context.domain}",
            f"Priority Level: {context.priority_level}",
        ]

        if context.constraints:
            constraints_info = []
            constraints_info.append(
                f"Time Limit: {context.constraints.time_limit} hours"
            )
            constraints_info.append(
                f"Max Information Requests: {context.constraints.max_information_requests}"
            )

            if (
                hasattr(context.constraints, "budget_limit")
                and context.constraints.budget_limit
            ):
                constraints_info.append(
                    f"Budget Limit: {context.constraints.budget_limit}"
                )

            prompt_parts.append(f"Constraints: {', '.join(constraints_info)}")

        if context.stakeholders:
            if isinstance(context.stakeholders, list):
                stakeholders_str = ", ".join(str(s) for s in context.stakeholders)
            else:
                stakeholders_str = str(context.stakeholders)
            prompt_parts.append(f"Stakeholders: {stakeholders_str}")

        if context.background_information:
            prompt_parts.append(
                f"Background Information: {context.background_information}"
            )

        if context.expected_outcomes:
            if isinstance(context.expected_outcomes, list):
                outcomes_str = ", ".join(str(o) for o in context.expected_outcomes)
            else:
                outcomes_str = str(context.expected_outcomes)
            prompt_parts.append(f"Expected Outcomes: {outcomes_str}")

        return "\n\n".join(prompt_parts)

    def _assess_confidence(self, analysis: Dict[str, Any]) -> float:
        """Assess confidence level for the analysis.

        Args:
            analysis: The analysis content to assess

        Returns:
            Confidence score between 0.0 and 1.0
        """
        # Base confidence assessment - can be overridden by specialized agents
        if not analysis:
            return 0.0

        # Simple heuristic based on content richness
        content_keys = len(analysis.keys())
        evidence_count = len(self.gathered_evidence)

        base_confidence = min(0.9, 0.3 + (content_keys * 0.1) + (evidence_count * 0.05))
        return round(base_confidence, 2)

    def _create_agent_message(
        self,
        recipient: str,
        message_type: MessageType,
        content: Dict[str, Any],
        confidence: float,
    ) -> AgentMessage:
        """Create a standardized agent message.

        Args:
            recipient: The intended recipient of the message
            message_type: Type of message being sent
            content: Message content
            confidence: Confidence level in the message

        Returns:
            AgentMessage object ready for transmission
        """
        return AgentMessage(
            sender=self.agent_role,
            recipient=recipient,
            message_type=message_type,
            content=content,
            confidence_level=confidence,
            supporting_evidence=self.gathered_evidence.copy(),
            timestamp=datetime.now().isoformat(),
        )

    async def decide_next_action(
        self,
        context: DecisionContext,
        previous_outputs: List[AgentOutput],
        search_results: List["SearchResult"] = None,
        user_responses: List["UserResponse"] = None,
    ) -> "ActionDecision":
        """Decide what action should be taken next based on current state.

        This method analyzes the current decision context, previous agent outputs,
        and any gathered information to recommend the next action.

        Args:
            context: The decision context being analyzed
            previous_outputs: All agent outputs from the current discussion
            search_results: Any internet search results gathered so far
            user_responses: Any user responses received so far

        Returns:
            ActionDecision with this agent's recommendation for next action
        """
        from ...models.mai_dxo import ActionDecision

        # Analyze current information completeness
        info_completeness = self._assess_information_completeness(
            context, previous_outputs, search_results, user_responses
        )

        # Check if agents have reached consensus
        consensus_level = self._assess_consensus_level(previous_outputs)

        # Determine recommended action based on agent's perspective and role
        action_prompt = f"""
As a {self.agent_role.value} specialist, analyze the current decision state and recommend the next action:

**Decision Context:**
{context.problem_description}

**Current Information Completeness:** {info_completeness:.1%}
**Consensus Level Among Agents:** {consensus_level:.1%}

**Previous Agent Discussion Summary:**
{self._summarize_previous_outputs(previous_outputs)}

**Available Actions:**
1. INTERNET_SEARCH - Search online for publicly available information (market data, trends, studies, etc.)
2. ASK_USER - Request user-specific information only they would know (company details, preferences, constraints)  
3. CONTINUE_DISCUSSION - Continue agent debate when more consensus building is needed
4. PROVIDE_SOLUTION - Give final recommendation when sufficient info gathered and consensus reached

From your {self.agent_role.value} perspective, which action should we take next and why?
Respond with a JSON object containing:
{{
    "recommended_action": "one of the four actions above",
    "reasoning": "detailed explanation of why this action is needed from your perspective",
    "confidence": "confidence score 0.0-1.0",
    "supporting_context": "specific details about what information is needed or why consensus isn't reached",
    "priority_level": "urgency level 1-5"
}}
"""

        try:
            response = await self._call_llm_with_prompt(action_prompt, "JSON object")

            # Parse LLM response
            import json

            try:
                action_data = json.loads(response.strip())

                # Map string action to enum
                action_mapping = {
                    "internet_search": NextAction.INTERNET_SEARCH,
                    "ask_user": NextAction.ASK_USER,
                    "continue_discussion": NextAction.CONTINUE_DISCUSSION,
                    "provide_solution": NextAction.PROVIDE_SOLUTION,
                }

                recommended_action = action_mapping.get(
                    action_data.get("recommended_action", "").lower(),
                    NextAction.CONTINUE_DISCUSSION,
                )

                return ActionDecision(
                    agent_role=self.agent_role,
                    recommended_action=recommended_action,
                    reasoning=action_data.get("reasoning", "No reasoning provided"),
                    confidence=float(action_data.get("confidence", 0.5)),
                    supporting_context=action_data.get("supporting_context", ""),
                    priority_level=int(action_data.get("priority_level", 3)),
                )

            except (json.JSONDecodeError, KeyError, ValueError):
                # Fallback to default decision if parsing fails
                return self._get_fallback_action_decision(
                    info_completeness, consensus_level
                )

        except Exception:
            # Fallback if LLM call fails
            return self._get_fallback_action_decision(
                info_completeness, consensus_level
            )

    def _assess_information_completeness(
        self,
        context: DecisionContext,
        previous_outputs: List[AgentOutput],
        search_results: List = None,
        user_responses: List = None,
    ) -> float:
        """Assess how complete our information is for making a decision.

        Returns:
            Completeness score between 0.0 and 1.0
        """
        # Base completeness from agent outputs
        base_score = min(0.6, len(previous_outputs) * 0.1)

        # Bonus for search results
        search_bonus = min(0.2, (len(search_results) if search_results else 0) * 0.05)

        # Bonus for user responses
        user_bonus = min(0.2, (len(user_responses) if user_responses else 0) * 0.1)

        return min(1.0, base_score + search_bonus + user_bonus)

    def _assess_consensus_level(self, previous_outputs: List[AgentOutput]) -> float:
        """Assess the level of consensus among agent outputs.

        Returns:
            Consensus score between 0.0 and 1.0
        """
        if not previous_outputs:
            return 0.0

        # Simple heuristic: look for agreement indicators in outputs
        confidence_scores = [output.confidence_level for output in previous_outputs]
        avg_confidence = sum(confidence_scores) / len(confidence_scores)

        # Higher average confidence suggests more consensus
        return min(1.0, avg_confidence)

    def _summarize_previous_outputs(self, previous_outputs: List[AgentOutput]) -> str:
        """Create a brief summary of previous agent outputs.

        Returns:
            String summary of key points from previous discussion
        """
        if not previous_outputs:
            return "No previous discussion."

        summary_parts = []
        for output in previous_outputs[-3:]:  # Last 3 outputs
            summary_parts.append(
                f"- {output.agent_role.value}: {output.reasoning[:100]}..."
            )

        return "\n".join(summary_parts)

    def _get_fallback_action_decision(
        self, info_completeness: float, consensus_level: float
    ) -> "ActionDecision":
        """Get a fallback action decision when LLM analysis fails.

        Uses simple heuristics based on role and current state.
        """
        from ...models.mai_dxo import ActionDecision

        # Simple decision logic based on agent role and completeness
        if info_completeness < 0.4:
            if self.agent_role.value in ["strategic_analyst", "resource_optimizer"]:
                action = NextAction.INTERNET_SEARCH
                reasoning = f"Need more external information for {self.agent_role.value} analysis"
            else:
                action = NextAction.ASK_USER
                reasoning = f"Need user-specific context for {self.agent_role.value} perspective"
        elif consensus_level < 0.6:
            action = NextAction.CONTINUE_DISCUSSION
            reasoning = "Need more discussion to reach consensus"
        else:
            action = NextAction.PROVIDE_SOLUTION
            reasoning = "Sufficient information and consensus to provide solution"

        return ActionDecision(
            agent_role=self.agent_role,
            recommended_action=action,
            reasoning=reasoning,
            confidence=0.6,
            supporting_context="Fallback decision due to analysis failure",
            priority_level=3,
        )
