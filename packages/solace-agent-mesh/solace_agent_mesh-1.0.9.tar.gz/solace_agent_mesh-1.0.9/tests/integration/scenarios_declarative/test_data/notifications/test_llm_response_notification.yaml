test_case_id: "notification_llm_response_001"
description: "Tests that an llm_response notification is sent."
tags: ["all","agent","common","notification"]
skip_intermediate_events: true

gateway_input:
  target_agent_name: "TestAgent"
  user_identity: "notification_tester@example.com"
  parts:
    - type: "text"
      text: "Query for LLM response."
llm_interactions:
  - step_id: "llm_responds"
    static_response:
      id: "chatcmpl-llmrespnotify"
      object: "chat.completion"
      model: "test-llm-model-for-resp-notification"
      choices:
        - message: {role: "assistant", content: "This is the LLM's direct response."}
          finish_reason: "stop"
      usage: {prompt_tokens: 5, completion_tokens: 7, total_tokens: 12}
expected_gateway_output:
  - type: "status_update"
    event_purpose: "llm_invocation"
    final_flag: false
  - type: "status_update"
    event_purpose: "llm_response"
    expected_llm_data_contains:
      content:
        role: "model"
        parts:
          - text: "This is the LLM's direct response."
    final_flag: true
  - type: "final_response"
    content_parts:
      - type: "text"
        text_exact: "This is the LLM's direct response."
    task_state: "completed"
