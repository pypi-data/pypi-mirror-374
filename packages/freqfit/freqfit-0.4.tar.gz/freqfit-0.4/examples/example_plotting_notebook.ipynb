{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is a simple example of how we use the plotting tools in the freqfit repo, as well as how we access our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from freqfit.models.constants import QBB, NA, M76\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "scale_factor = 1\n",
    "\n",
    "font = {\"size\": 22}\n",
    "matplotlib.rc(\"font\", **font)\n",
    "NICE_BLUE = \"#668DA5\"\n",
    "NICE_RED = \"#B4584D\"\n",
    "NICE_GREEN = \"#ABB1A2\"\n",
    "NICE_PINK = \"#CCACAD\"\n",
    "\n",
    "# Add small random gaussian numbers to the test-statistics from toys\n",
    "RANDOM = False\n",
    "RV_SIZE = 1e-6\n",
    "TRUNC = False  # truncate the test-statistics at 6 sigfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory with our toys with signal\n",
    "\n",
    "dir_name = \"/data/eliza1/LEGEND/data/L200/limit/l200_toys_minimum_minimizer_truncated_gaussian_fixed_nuisance_0_lower_limit_10312024/\"\n",
    "\n",
    "# Read in the file with the s-grid and the values of the observed test-statistic\n",
    "f = dir_name + \"l200_ts_wilks_fine.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the values of the observed test-statistic\n",
    "df = h5py.File(f, \"r\")\n",
    "s = df[\"s\"][:]\n",
    "s_scanned = s\n",
    "ts = df[\"t_s\"][:]\n",
    "df.close()\n",
    "\n",
    "\n",
    "plt.scatter(s, ts)\n",
    "plt.title(\"L200 Test Statistic Profile\")\n",
    "plt.ylabel(r\"$\\tilde{t}_S$\")\n",
    "plt.xlabel(r\"$S$\")\n",
    "plt.axhline(y=2.71)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test-statistics generated from the toys are stored in separate files keyed by S-Value_jobid.h5\n",
    "# Gather a list of lists, where each list is all the files corresponding to one value of S\n",
    "\n",
    "files_per_s = []\n",
    "s_f_name_vals = []  # s-values from the file name keys\n",
    "\n",
    "\n",
    "for s_val in s_scanned[:]:\n",
    "    files = glob.glob(dir_name + f\"{s_val}_*.h5\")\n",
    "    files_per_s.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the values of the test-statistics generated from the toys, and also determine the 90% critical value\n",
    "# The array toys_per_scanned_s is a list of lists, where each list is all of the test-statistics at one value of S\n",
    "from freqfit.statistics import ts_critical\n",
    "\n",
    "t_crits = []\n",
    "t_crit_lows = []\n",
    "t_crit_his = []\n",
    "\n",
    "toys_per_scanned_s = []\n",
    "\n",
    "for file_list in files_per_s[:]:\n",
    "    toy_ts_per_file = []\n",
    "    try:\n",
    "        if RANDOM:\n",
    "            for FILE in file_list:\n",
    "                f = h5py.File(FILE, \"r\")\n",
    "                toy_ts_per_file.extend(\n",
    "                    f[\"ts\"][:] + np.random.normal(0, RV_SIZE, len(f[\"ts\"][:]))\n",
    "                )\n",
    "                f.close()\n",
    "        elif TRUNC:\n",
    "            for FILE in file_list:\n",
    "                f = h5py.File(FILE, \"r\")\n",
    "                toy_ts_per_file.extend(np.around(f[\"ts\"][:], 9))\n",
    "                f.close()\n",
    "        else:\n",
    "            for FILE in file_list:\n",
    "                f = h5py.File(FILE, \"r\")\n",
    "                toy_ts_per_file.extend(f[\"ts\"][:])\n",
    "                f.close()\n",
    "\n",
    "        tcrit_tuple, _ = ts_critical(\n",
    "            toy_ts_per_file, threshold=0.9, confidence=0.68, plot=False\n",
    "        )\n",
    "        t_crit, t_crit_low, t_crit_high = tcrit_tuple\n",
    "\n",
    "        t_crits.append(t_crit)\n",
    "        t_crit_lows.append(t_crit_low)\n",
    "        t_crit_his.append(t_crit_high)\n",
    "\n",
    "        toys_per_scanned_s.append(toy_ts_per_file)\n",
    "    except:\n",
    "        t_crits.append(np.nan)\n",
    "        t_crit_lows.append(np.nan)\n",
    "        t_crit_his.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an example plot of the observed test statistic and the critical value of the test statistic\n",
    "# Nothing is decided by this plot, it is just nice to make to see how things look visually\n",
    "from freqfit.statistics import find_crossing\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.scatter(s_scanned, ts, color=NICE_BLUE)\n",
    "plt.plot(s_scanned, t_crits, c=NICE_RED, label=r\"MC Critical $\\tilde{t}_S$\")\n",
    "plt.fill_between(s_scanned, t_crit_lows, t_crit_his, alpha=0.6, color=NICE_RED)\n",
    "\n",
    "\n",
    "s_approx = find_crossing(s_scanned, t_crits, ts)\n",
    "\n",
    "plt.axvline(x=s_approx[-1], label=\"90% CL Crossing\", ls=\"--\", c=NICE_BLUE)\n",
    "\n",
    "T_est = 1 / (M76 * s_approx[-1] * scale_factor / (np.log(2) * NA))\n",
    "\n",
    "\n",
    "plt.xlabel(\"S\")\n",
    "plt.ylabel(r\"$\\tilde{t}_S$\")\n",
    "plt.title(r\"LEGEND-200 Neutrino 2024\" + \" Limit\\n\" + f\"{T_est: 0.2e}\", usetex=True)\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the p-values of the observed data for the Brazil plot\n",
    "from freqfit.statistics import get_p_values\n",
    "\n",
    "p_values = get_p_values(toys_per_scanned_s, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(s_scanned, p_values, color=NICE_BLUE)\n",
    "\n",
    "\n",
    "plt.xlabel(\"S\")\n",
    "plt.ylabel(r\"p-value\")\n",
    "\n",
    "\n",
    "s_approx = find_crossing(s_scanned, p_values, 0.1)\n",
    "\n",
    "plt.axhline(y=0.1, label=\"90% CL\", c=NICE_GREEN)\n",
    "plt.axvline(x=s_approx[-1], label=\"90% CL Crossing\", ls=\"--\", c=NICE_BLUE)\n",
    "\n",
    "T_est = 1 / (M76 * s_approx[-1] * scale_factor / (np.log(2) * NA))\n",
    "plt.title(\n",
    "    r\"LEGEND-200 Neutrino 2024 Dataset\" + \" Limit\\n\" + f\"{T_est: 0.2e}\", usetex=True\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the directory where our zero-signal toys are located\n",
    "\n",
    "brazil_dir_path = \"/data/eliza1/LEGEND/data/L200/limit/l200_brazil_minimum_minimizer_truncated_gaussian_fixed_nuisance_0_lower_limit_10312024\"\n",
    "brazil_files = glob.glob(brazil_dir_path + \"/*.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the s-values that were scanned for the 0-signal toys inside the files, so grab them out\n",
    "f = h5py.File(brazil_files[0], \"r\")\n",
    "s_values_zero_sig = f[\"s\"][:]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the test-statistics\n",
    "# toys_per_s_zero_sig is a list of lists, where each list is test-statistics of 0-signal toy tested against that S-value\n",
    "toys_per_s_zero_sig = []\n",
    "Es_per_s_zero_sig = []\n",
    "seeds_per_s_zero_sig = []\n",
    "for i in range(len(s_values_zero_sig[:])):\n",
    "    toys_per_file = []\n",
    "    Es_per_file = []\n",
    "    seeds_per_file = []\n",
    "\n",
    "    if RANDOM:\n",
    "        for file in brazil_files:\n",
    "            f = h5py.File(file, \"r\")\n",
    "            ts = f[\"ts\"][:]\n",
    "            Es = f[\"Es\"][:]\n",
    "            seeds = f[\"seed\"][:]\n",
    "            toys_per_file.extend(ts[i] + np.random.normal(0, RV_SIZE, len(ts[i])))\n",
    "            Es_per_file.extend(Es)\n",
    "            seeds_per_file.extend(seeds)\n",
    "\n",
    "    elif TRUNC:\n",
    "        for file in brazil_files:\n",
    "            f = h5py.File(file, \"r\")\n",
    "            ts = f[\"ts\"][:]\n",
    "            Es = f[\"Es\"][:]\n",
    "            seeds = f[\"seed\"][:]\n",
    "            toys_per_file.extend(np.around(ts[i], 9))\n",
    "            Es_per_file.extend(Es)\n",
    "            seeds_per_file.extend(seeds)\n",
    "    else:\n",
    "        for file in brazil_files:\n",
    "            f = h5py.File(file, \"r\")\n",
    "            ts = f[\"ts\"][:]\n",
    "            Es = f[\"Es\"][:]\n",
    "            seeds = f[\"seed\"][:]\n",
    "            toys_per_file.extend(ts[i])\n",
    "            Es_per_file.extend(Es)\n",
    "            seeds_per_file.extend(seeds)\n",
    "    toys_per_s_zero_sig.append(toys_per_file)\n",
    "    Es_per_s_zero_sig.append(Es_per_file)\n",
    "    seeds_per_s_zero_sig.append(seeds_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the median p-values\n",
    "from math import erf\n",
    "from freqfit.statistics import sensitivity\n",
    "\n",
    "p_values_median, p_values_hi, p_values_lo = sensitivity(\n",
    "    toys_per_s_zero_sig[:],\n",
    "    toys_per_scanned_s[:],\n",
    "    s_scanned[:],\n",
    "    CL=erf(2 / np.sqrt(2)),\n",
    "    plot=False,\n",
    "    plot_dir=\"/home/sjborden/freqfit/l200_plots/\",\n",
    "    step=0.001,\n",
    "    save=False,\n",
    ")\n",
    "p_values_median, p_values_hi_1, p_values_lo_1 = sensitivity(\n",
    "    toys_per_s_zero_sig[:],\n",
    "    toys_per_scanned_s[:],\n",
    "    s_scanned[:],\n",
    "    CL=erf(1 / np.sqrt(2)),\n",
    "    plot=False,\n",
    "    step=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the Brazil plot\n",
    "\n",
    "ma = 0.0759214027\n",
    "NA = 6.022e23\n",
    "\n",
    "\n",
    "T_12 = 1 / (M76 * s / (np.log(2) * NA))\n",
    "T_12 = 1 / T_12 / 1e-25\n",
    "\n",
    "T_12_exp = 1 / (M76 * s_values_zero_sig / (np.log(2) * NA))\n",
    "T_12_exp = 1 / T_12_exp / 1e-25\n",
    "\n",
    "BRAZIL_YELLOW = \"#FFFF00\"\n",
    "BRAZIL_GREEN = \"#00FF00\"\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(T_12, p_values, color=\"k\", label=\"Observed\")\n",
    "plt.plot(T_12_exp, p_values_median, color=\"k\", ls=\"--\", label=\"Median, No Signal\")\n",
    "plt.fill_between(\n",
    "    T_12_exp,\n",
    "    p_values_lo,\n",
    "    p_values_hi,\n",
    "    color=BRAZIL_YELLOW,\n",
    "    alpha=0.6,\n",
    "    label=r\"2 $\\sigma$ Interval\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    T_12_exp,\n",
    "    p_values_lo_1,\n",
    "    p_values_hi_1,\n",
    "    color=BRAZIL_GREEN,\n",
    "    alpha=0.6,\n",
    "    label=r\"1 $\\sigma$ Interval\",\n",
    ")\n",
    "\n",
    "\n",
    "s_approx = find_crossing(s, p_values_median, 0.1)\n",
    "T_12_median = 1 / (M76 * s_approx[-1] / (np.log(2) * NA))\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(r\"$\\Gamma_{1/2}^{0\\nu} \\, [10^{-25} \\,\\mathrm{yr}^{-1}]$ \")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.ylim([1e-4, 1])\n",
    "plt.plot([], [], ls=\"none\", label=f\"Median {T_12_median: 0.2e}\")\n",
    "plt.plot([], [], ls=\"none\", label=f\"Observed Limit {T_est: 0.2e}\")\n",
    "plt.legend()\n",
    "plt.title(\n",
    "    f\"Minimum Minimizer + Truncated Gaussian\"\n",
    "    + f\"\\n Fixed Nuisance Parameters + Poisson Initial Guess \\n No RV Added \\n Lower Limits at 0 \\nLEGEND-200 Neutrino 2024 Dataset\",\n",
    "    usetex=True,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is how to get the median exclusion sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "import freqfit.models.constants as constants\n",
    "\n",
    "# sort all of the toys we generated with non-zero signal so we can compute p-values very quickly\n",
    "sorted_toys_per_scanned_s = []\n",
    "\n",
    "for toy in toys_per_scanned_s:\n",
    "    sorted_toys_per_scanned_s.append(np.sort(toy))\n",
    "\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, fastmath=True)\n",
    "def brazil_data_new(sorted_toy_ts: np.array, ts_observed: np.array):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    toy_ts\n",
    "        List of lists. Each list is a list of test statistics from the toys generated at that value\n",
    "    ts_observed\n",
    "        List. These are the observed values of the test statistic from the experiment for a given value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p_values\n",
    "        A list of the p-values associated with the observed data\n",
    "    \"\"\"\n",
    "\n",
    "    if len(sorted_toy_ts) != len(ts_observed):\n",
    "        raise ValueError(\n",
    "            \"The number of scanned points for the toys is not equal to the number of observed test statistics\"\n",
    "        )\n",
    "\n",
    "    p_values = np.zeros(len(ts_observed))\n",
    "    for i in nb.prange(len(ts_observed)):\n",
    "        p_values[i] = len(sorted_toy_ts[i][sorted_toy_ts[i] >= ts_observed[i]]) / len(\n",
    "            sorted_toy_ts[i]\n",
    "        )\n",
    "\n",
    "    return p_values\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_toys = len(toys_per_s_zero_sig[0])\n",
    "T_12_90CL = []\n",
    "\n",
    "plot = False\n",
    "\n",
    "WINDOW = np.array(constants.WINDOW)\n",
    "\n",
    "for i in tqdm(range(num_toys)):\n",
    "    nike = []\n",
    "    for toys in toys_per_s_zero_sig:\n",
    "        nike.append(toys[i])\n",
    "\n",
    "    p_values = brazil_data_new(sorted_toys_per_scanned_s, nike)\n",
    "\n",
    "    s_approx = find_crossing(s_values_zero_sig, p_values, 0.1)\n",
    "\n",
    "    if p_values[-1] > 0.1:\n",
    "        s_approx_est = s_values_zero_sig[-1]\n",
    "    elif len(s_approx) < 1:\n",
    "        s_approx_est = s_values_zero_sig[0]\n",
    "    else:\n",
    "        s_approx_est = s_approx[-1]\n",
    "    T_est = 1 / (M76 * s_approx_est / (np.log(2) * NA))\n",
    "\n",
    "    T_12_90CL.append(T_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_12_90CL = np.array(T_12_90CL)\n",
    "print(np.nanmedian(T_12_90CL))\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(1 / T_12_90CL, bins=1000)\n",
    "plt.axvline(\n",
    "    x=np.nanmedian(1 / T_12_90CL),\n",
    "    ls=\"--\",\n",
    "    label=f\"Median 90% CL Half-Life: {np.nanmedian(T_12_90CL):.2e}\",\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"90% CL Half-Rate From 0 Signal Toys [yr^-1]\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.title(\n",
    "    f\"Minimum Minimizer + Truncated Gaussian\"\n",
    "    + f\"\\n Fixed Nuisance Parameters + Poisson Initial Guess \\n No Sigma Gaussian RV\\n  0 Lower Limit on S and BI \\n LEGEND-200 Neutrino 2024 Dataset\",\n",
    "    usetex=True,\n",
    ")\n",
    "plt.xlim([0, None])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
