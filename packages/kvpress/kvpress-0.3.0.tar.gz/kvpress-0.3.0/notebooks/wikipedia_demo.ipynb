{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we showcase how to use the KVpress pipelines by answering questions about NVIDIA Wikipedia article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from kvpress import (\n",
    "    ExpectedAttentionPress,\n",
    "    KnormPress,\n",
    "    ObservedAttentionPress,\n",
    "    RandomPress,\n",
    "    SnapKVPress,\n",
    "    StreamingLLMPress,\n",
    "    TOVAPress,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: Copyright (c) 1993-2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "# SPDX-License-Identifier: Apache-2.0\n",
    "\n",
    "\n",
    "import contextlib\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, Cache, DynamicCache, Pipeline, QuantizedCache\n",
    "from transformers.pipelines import PIPELINE_REGISTRY\n",
    "from transformers.pipelines.base import GenericTensor\n",
    "\n",
    "from kvpress.presses.base_press import BasePress\n",
    "from kvpress.presses.finch_press import FinchPress\n",
    "from kvpress.presses.key_rerotation_press import KeyRerotationPress\n",
    "from kvpress.presses.observed_attention_press import ObservedAttentionPress\n",
    "from kvpress.presses.per_layer_compression_press import PerLayerCompressionPress\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class KVPressTextGenerationPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Pipeline for key-value cache compression in causal language models.\n",
    "\n",
    "    Enables efficient processing of long contexts by applying KV cache compression\n",
    "    during pre-filling, then generating answers using greedy decoding.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    pipeline = KVPressTextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "    press = SnapKVPress(compression_ratio=0.5)\n",
    "    result = pipeline(context=\"Long text...\", question=\"A question about the long context.\", press=press)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def _sanitize_parameters(\n",
    "        self,\n",
    "        question: Optional[str] = None,\n",
    "        questions: Optional[list[str]] = None,\n",
    "        answer_prefix: Optional[str] = None,\n",
    "        press: Optional[BasePress] = None,\n",
    "        max_new_tokens: int = 50,\n",
    "        max_context_length: Optional[int] = None,\n",
    "        cache: Optional[Cache] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sanitize the input parameters for the pipeline.\n",
    "        The user can either provide a single question or a list of questions to be asked about the context.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        question : str, optional\n",
    "            The question to be asked about the context. Exclusive with `questions`.\n",
    "        questions : list[str], optional\n",
    "            A list of questions to be asked about the context. Exclusive with `question`.\n",
    "        answer_prefix : str, optional\n",
    "            The prefix to be added to the generated answer.\n",
    "        press : BasePress, optional\n",
    "            The key-value cache compression method to apply during pre-filling.\n",
    "\n",
    "            Accepts any KVPress compression method (SnapKVPress, KnormPress,\n",
    "            ExpectedAttentionPress, BlockPress, AdaKVPress, ComposedPress, etc.).\n",
    "            If None, no compression is applied.\n",
    "        max_new_tokens : int, optional\n",
    "            The maximum number of new tokens to generate for each answer.\n",
    "        max_context_length : int, optional\n",
    "            The maximum number of tokens in the context. By default will use the maximum length supported by the model.\n",
    "        cache : Cache, optional\n",
    "            The cache to use for the forward pass. Defaults to None (DynamicCache).\n",
    "        **kwargs : dict\n",
    "            Additional keyword arguments, currently ignored.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[dict, dict, dict]\n",
    "            A tuple containing three dictionaries:\n",
    "                - preprocess_kwargs: The keyword arguments for the preprocess function.\n",
    "                - forward_kwargs: The keyword arguments for the forward function.\n",
    "                - postprocess_kwargs: The keyword arguments for the postprocess function.\n",
    "        \"\"\"\n",
    "\n",
    "        answer_prefix = answer_prefix or \"\"\n",
    "        postprocess_kwargs = {\"single_question\": questions is None}\n",
    "        assert question is None or questions is None, \"Either question or questions should be provided, not both.\"\n",
    "        questions = questions or ([question] if question else [\"\"])\n",
    "        if max_context_length is None:\n",
    "            max_context_length = min(self.tokenizer.model_max_length, int(1e10))  # 1e10 to avoid overflow\n",
    "        preprocess_kwargs = {\n",
    "            \"questions\": questions,\n",
    "            \"answer_prefix\": answer_prefix,\n",
    "            \"max_context_length\": max_context_length,\n",
    "        }\n",
    "        forward_kwargs = {\"press\": press, \"max_new_tokens\": max_new_tokens, \"cache\": cache}\n",
    "        return preprocess_kwargs, forward_kwargs, postprocess_kwargs\n",
    "\n",
    "    def preprocess(\n",
    "        self,\n",
    "        context: str,\n",
    "        questions: list[str],\n",
    "        answer_prefix: str,\n",
    "        max_context_length: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply chat template and tokenize the context and questions.\n",
    "\n",
    "        Prepares input text for KV cache compression and generation by applying\n",
    "        appropriate chat templates and tokenizing. Handles models with and without\n",
    "        chat templates.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        context : str\n",
    "            Long context text to be compressed using the press method.\n",
    "        questions : list[str]\n",
    "            Questions to be asked about the context.\n",
    "        answer_prefix : str\n",
    "            Optional prefix for generated answers.\n",
    "        max_context_length : int\n",
    "            Maximum tokens allowed in context (truncated if exceeded).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, GenericTensor]\n",
    "            Dictionary with \"context_ids\" and \"questions_ids\" tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply chat template if available\n",
    "        if self.tokenizer.chat_template is None:\n",
    "            bos_token = getattr(self.tokenizer, \"bos_token\", \"\")\n",
    "            context = bos_token + context\n",
    "            question_suffix = \"\\n\"  # to separate the question from the answer\n",
    "        else:\n",
    "            separator = \"\\n\" + \"#\" * len(context)\n",
    "            context = self.tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": context + separator}],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=False,\n",
    "                enable_thinking=False,\n",
    "            )\n",
    "            context, question_suffix = context.split(separator)\n",
    "\n",
    "        # Add question_suffix and answer prefix\n",
    "        # e.g. for llama3.1, question_suffix=\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "        questions = [question + question_suffix + answer_prefix for question in questions]\n",
    "\n",
    "        # Tokenize the context and questions\n",
    "        context_ids = self.tokenizer.encode(context, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        question_ids = [\n",
    "            self.tokenizer.encode(question, return_tensors=\"pt\", add_special_tokens=False) for question in questions\n",
    "        ]\n",
    "\n",
    "        # Truncate context\n",
    "        if context_ids.shape[1] > max_context_length:\n",
    "            logger.warning(\n",
    "                f\"Context length has been truncated from {context_ids.shape[1]} to {max_context_length} tokens.\"\n",
    "            )\n",
    "            context_ids = context_ids[:, :max_context_length]\n",
    "\n",
    "        return {\"context_ids\": context_ids, \"questions_ids\": question_ids}\n",
    "\n",
    "    def _forward(\n",
    "        self,\n",
    "        input_tensors: dict[str, GenericTensor],\n",
    "        max_new_tokens: int = 50,\n",
    "        press: Optional[BasePress] = None,\n",
    "        cache: Optional[Cache] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Execute KV cache compression and text generation pipeline.\n",
    "\n",
    "        Performs context compression using the press method during pre-filling,\n",
    "        then generates answers using greedy decoding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensors : dict[str, GenericTensor]\n",
    "            Tokenized inputs with \"context_ids\" and \"questions_ids\".\n",
    "        max_new_tokens : int, default=50\n",
    "            Maximum tokens to generate for each answer.\n",
    "        press : BasePress, optional\n",
    "            Compression method for context pre-filling. If None, no compression.\n",
    "        cache : Cache, optional\n",
    "            Cache object for forward pass. If None, creates new DynamicCache.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[str]\n",
    "            Generated answers for each input question.\n",
    "        \"\"\"\n",
    "        context_ids = input_tensors[\"context_ids\"].to(self.model.device)\n",
    "        context_length = context_ids.shape[1]\n",
    "\n",
    "        if cache is None:\n",
    "            cache = DynamicCache()\n",
    "\n",
    "        with press(self.model) if press is not None else contextlib.nullcontext():\n",
    "            self.model.model(\n",
    "                input_ids=context_ids,\n",
    "                past_key_values=cache,\n",
    "                output_attentions=self.output_attentions(press),\n",
    "            )\n",
    "\n",
    "        logger.debug(f\"Context Length: {context_length}\")\n",
    "        logger.debug(f\"Compressed Context Length: {cache.get_seq_length()}\")\n",
    "\n",
    "        # Greedy decoding for each question\n",
    "        answers = []\n",
    "        for question_ids in input_tensors[\"questions_ids\"]:\n",
    "            if isinstance(press, KeyRerotationPress) or (isinstance(press, FinchPress) and press.rerotate_keys):\n",
    "                context_length = cache.get_seq_length()\n",
    "\n",
    "            answer = self.generate_answer(\n",
    "                question_ids=question_ids.to(self.model.device),\n",
    "                cache=cache,\n",
    "                context_length=context_length,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "            answers.append(answer)\n",
    "\n",
    "        return answers\n",
    "\n",
    "\n",
    "    def generate_answer(\n",
    "        self, question_ids: torch.Tensor, cache: Cache, context_length: int, max_new_tokens: int\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate an answer to a question using greedy decoding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        question_ids : torch.Tensor\n",
    "            The tokenized question.\n",
    "        cache : Cache\n",
    "            The compressed key-value cache.\n",
    "        context_length : int\n",
    "            The length of the context.\n",
    "        max_new_tokens : int\n",
    "            The maximum number of new tokens to generate.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The generated answer.\n",
    "        \"\"\"\n",
    "        cache_seq_lengths = [cache.get_seq_length(layer_idx) for layer_idx in range(len(cache))]\n",
    "        \n",
    "        input_ids = question_ids.to(self.model.device)\n",
    "        position_ids = torch.arange(\n",
    "            cache.get_seq_length(), cache.get_seq_length() + question_ids.shape[1], device=self.model.device\n",
    "        ).unsqueeze(0)\n",
    "\n",
    "        # Prepare model kwargs for the initial forward pass\n",
    "        model_kwargs = dict(num_logits_to_keep=1,\n",
    "                            past_key_values=cache,\n",
    "                            use_cache=True,\n",
    "                            position_ids=position_ids\n",
    "                           )\n",
    "\n",
    "        if self._is_flash_attention_enabled():\n",
    "            self._update_kwargs_for_flash_attention(model_kwargs, \n",
    "                                                    cache.get_seq_length(), \n",
    "                                                    question_ids.shape[1]\n",
    "                                                   )\n",
    "\n",
    "        # Initial forward pass\n",
    "        outputs = self.model(input_ids=input_ids, **model_kwargs)\n",
    "\n",
    "        position_ids = position_ids[:, -1:] + 1\n",
    "        generated_ids = [outputs.logits[0, -1].argmax()]\n",
    "\n",
    "        should_stop_token_ids = self.model.generation_config.eos_token_id\n",
    "        if not isinstance(should_stop_token_ids, list):\n",
    "            should_stop_token_ids = [should_stop_token_ids]\n",
    "\n",
    "        for i in range(max_new_tokens - 1):\n",
    "            input_ids = generated_ids[-1].unsqueeze(0).unsqueeze(0)\n",
    "            model_kwargs = dict(num_logits_to_keep=1, past_key_values=cache, use_cache=True,\n",
    "                                position_ids=position_ids + i\n",
    "                               )\n",
    "\n",
    "            if self._is_flash_attention_enabled():\n",
    "                self._update_kwargs_for_flash_attention(model_kwargs, \n",
    "                                                        cache.get_seq_length(),\n",
    "                                                        1)\n",
    "\n",
    "            outputs = self.model(input_ids=input_ids, **model_kwargs)\n",
    "            new_id = outputs.logits[0, -1].argmax()\n",
    "            generated_ids.append(new_id)\n",
    "            if new_id.item() in should_stop_token_ids:\n",
    "                break\n",
    "        answer = self.tokenizer.decode(torch.stack(generated_ids), skip_special_tokens=True)\n",
    "\n",
    "        # Remove the generated tokens from the cache\n",
    "        for layer_idx, sequence_length in enumerate(cache_seq_lengths):\n",
    "            cache.layers[layer_idx].keys = cache.layers[layer_idx].keys[:, :, :sequence_length]\n",
    "            cache.layers[layer_idx].values = cache.layers[layer_idx].values[:, :, :sequence_length]\n",
    "\n",
    "        if isinstance(cache, QuantizedCache):\n",
    "            for layer_idx, sequence_length in enumerate(cache_seq_lengths):\n",
    "                cache.cache_processor._quantized_keys[layer_idx] = cache.cache_processor._quantized_keys[layer_idx][\n",
    "                    :, :, :sequence_length\n",
    "                ]\n",
    "                cache.cache_processor._quantized_values[layer_idx] = cache.cache_processor._quantized_values[layer_idx][\n",
    "                    :, :, :sequence_length\n",
    "                ]\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def output_attentions(self, press: BasePress):\n",
    "        if isinstance(press, ObservedAttentionPress):\n",
    "            return True\n",
    "        if isinstance(press, (KeyRerotationPress, PerLayerCompressionPress)) and isinstance(\n",
    "            press.press, ObservedAttentionPress\n",
    "        ):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def postprocess(self, model_outputs, single_question):\n",
    "        if single_question:\n",
    "            return {\"answer\": model_outputs[0]}\n",
    "        return {\"answers\": model_outputs}\n",
    "\n",
    "    def _is_flash_attention_enabled(self) -> bool:\n",
    "        \"\"\"Check if flash attention is enabled for the model.\"\"\"\n",
    "        return \"flash\" in self.model.config._attn_implementation and self.model._supports_attention_backend\n",
    "        \n",
    "    def _update_kwargs_for_flash_attention(\n",
    "        self, model_kwargs: dict, cache_sequence_length: int, new_seq_len: int = 1\n",
    "    ) -> None:\n",
    "        \"\"\"Update model kwargs with flash attention specific parameters.\"\"\"\n",
    "        #return \n",
    "        # DO NOT UPDATE, AS IT WOULD BREAK LAYER-WISE COMPRESSION METHODS\n",
    "        model_kwargs.update(\n",
    "            cu_seq_lens_q=torch.tensor([0, new_seq_len], dtype=torch.int32, device=self.device),\n",
    "            cu_seq_lens_k=torch.tensor([0, cache_sequence_length], dtype=torch.int32, device=self.device),\n",
    "            max_length_q=new_seq_len,\n",
    "            max_length_k=cache_sequence_length,\n",
    "        )\n",
    "\n",
    "\n",
    "PIPELINE_REGISTRY.register_pipeline(\n",
    "    \"kv-press-text-generation-debug\",\n",
    "    pipeline_class=KVPressTextGenerationPipeline,\n",
    "    pt_model=AutoModelForCausalLM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the pipeline and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 227:     # If the lengths are not equal, most probably we are in decoding stage with cache\n",
      " 228:     # In that case the position ids will not always start with `0` and we need a better way to infer\n",
      " 229:     # cumulative seq lengths.\n",
      " 230:     if query_length != kv_length:\n",
      " 231:         indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n",
      " 232: \n",
      " 233:         tensor_kws = {\"dtype\": torch.int32, \"device\": position_ids.device}\n",
      " 234:         last_position_ids = position_ids[:, -1]\n",
      " 235: \n",
      " 236:         cu_seq_lens_k = torch.cat([torch.zeros(1, **tensor_kws), last_position_ids.cumsum(0).add(1)], 0)\n",
      " 237:         max_length_k = int(last_position_ids.max()) + 1\n",
      " 238: \n",
      " 239:         batch_size, seq_len = query.shape[:2]\n",
      " 240:         q_len = torch.ones(batch_size, **tensor_kws) if seq_len == 1 else last_position_ids.to(torch.int32).add(1)\n",
      " 241:         cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0)], 0)\n",
      " 242:         max_length_q = int(q_len.max())\n",
      " 243:     else:\n",
      " 244:         position_ids = position_ids.flatten()\n",
      " 245:         indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n",
      " 246: \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    # If the lengths are not equal, most probably we are in decoding stage with cache\n",
    "    # In that case the position ids will not always start with `0` and we need a better way to infer\n",
    "    # cumulative seq lengths.\n",
    "    if query_length != kv_length:\n",
    "        indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n",
    "\n",
    "        tensor_kws = {\"dtype\": torch.int32, \"device\": position_ids.device}\n",
    "        last_position_ids = position_ids[:, -1]\n",
    "\n",
    "        cu_seq_lens_k = torch.cat([torch.zeros(1, **tensor_kws), last_position_ids.cumsum(0).add(1)], 0)\n",
    "        max_length_k = int(last_position_ids.max()) + 1\n",
    "\"\"\"\n",
    "\n",
    "file_path = '/mnt/2tb/PyCharmProjects/kvpress/.venv/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py'\n",
    "start_line = 227\n",
    "num_lines = 20\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for i, line in enumerate(lines[start_line - 1 : start_line - 1 + num_lines], start=start_line):\n",
    "        print(f\"{i:4}: {line.rstrip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.55.0.dev0',\n",
       " '/mnt/2tb/PyCharmProjects/kvpress/.venv/lib/python3.12/site-packages/transformers/__init__.py')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__, transformers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load pipeline\n",
    "\n",
    "device = \"cuda:0\"\n",
    "ckpt = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# ckpt = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "# ckpt = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "attn_implementation = \"flash_attention_2\"  # use \"eager\" for ObservedAttentionPress and \"sdpa\" if you can't use \"flash_attention_2\"\n",
    "pipe = pipeline(\"kv-press-text-generation-debug\", model=ckpt, device=device, torch_dtype=\"auto\", model_kwargs={\"attn_implementation\":attn_implementation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: (There is no number at the end of the sentence. It is a repetition of the word \"book\".)\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline on a single question\n",
    "\n",
    "question = \"What is the book about?\"\n",
    "pred_answer = pipe(\"This book is about ghosts\"* 20,\n",
    "                   question=question, \n",
    "                   press=KnormPress(0.0),\n",
    "                   max_new_tokens=100\n",
    "                  )[\"answer\"]\n",
    "print(f\"Prediction: {pred_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 9091\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "url = \"https://en.wikipedia.org/wiki/Nvidia\"\n",
    "content = requests.get(url).content\n",
    "soup = BeautifulSoup(content, \"html.parser\")\n",
    "context = \"\".join([p.text for p in soup.find_all(\"p\")]) + \"\\n\\n\"\n",
    "tokens = pipe.tokenizer.encode(context, return_tensors=\"pt\").to(device)\n",
    "print(f\"Number of tokens: {tokens.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.55.0.dev0',\n",
       " '/mnt/2tb/PyCharmProjects/kvpress/.venv/lib/python3.12/site-packages/transformers/__init__.py')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__, transformers.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!head /mnt/2tb/PyCharmProjects/kvpress/.venv/lib/python3.12/site-packages/transformers/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the pipeline with a press"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a press with a compression ratio, you can run the following cells with different presses\n",
    "compression_ratio = 0.0\n",
    "press = ExpectedAttentionPress(compression_ratio)\n",
    "# press = KnormPress(compression_ratio)\n",
    "# press = RandomPress(compression_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:   Complete this sentence: The Nvidia GeForce Partner Program was a ...\n",
      "Answer:     marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\n",
      "Prediction: In 2025, Nvidia's CEO Jensen Huang announced that the company would be expanding its presence in the automotive industry, with a focus on developing autonomous vehicles and other mobility solutions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:   Complete this sentence: The Nvidia GeForce Partner Program was a ...\n",
      "Answer:     marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\n",
      "Prediction: marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline on a single question\n",
    "\n",
    "question = \"Complete this sentence: The Nvidia GeForce Partner Program was a ...\"\n",
    "true_answer = \"marketing program designed to provide partnering companies with benefits such as public relations support, video game bundling, and marketing development funds.\"\n",
    "pred_answer = pipe(context, question=question, press=press)[\"answer\"]\n",
    "\n",
    "print(f\"Question:   {question}\")\n",
    "print(f\"Answer:     {true_answer}\")\n",
    "print(f\"Prediction: {pred_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:   What happened on March 1, 2024?\n",
      "Answer:     Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion\n",
      "Prediction: In 2025, Nvidia's CEO Jensen Huang announced that the company would be expanding its presence in the automotive industry, with a focus on developing autonomous vehicles and other mobility solutions.\n",
      "\n",
      "Question:   What was the unofficial company motto of Nvidia during the early days?\n",
      "Answer:     Our company is thirty days from going out of business\n",
      "Prediction: In 2025, Nvidia's CEO Jensen Huang announced that the company would be expanding its presence in the automotive industry, with a focus on developing autonomous vehicles and other mobility solutions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline on multiple questions, the context will be compressed only once\n",
    "\n",
    "questions = [\n",
    "    \"What happened on March 1, 2024?\",\n",
    "    \"What was the unofficial company motto of Nvidia during the early days?\",\n",
    "]\n",
    "\n",
    "true_answers = [\n",
    "    \"Nvidia became the third company in the history of the United States to close with a market capitalization in excess of $2 trillion\",\n",
    "    \"Our company is thirty days from going out of business\",\n",
    "]\n",
    "\n",
    "pred_answers = pipe(context, questions=questions, press=press)[\"answers\"]\n",
    "for question, pred_answer, true_answer in zip(questions, pred_answers, true_answers):\n",
    "    print(f\"Question:   {question}\")\n",
    "    print(f\"Answer:     {true_answer}\")\n",
    "    print(f\"Prediction: {pred_answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:              What is GTC ?\n",
      "Answer:                Nvidia's GPU Technology Conference (GTC) is a series of technical conferences held around the world.\n",
      "Prediction w/o prefix: In 2025, Nvidia's market capitalization was worth more than the combined value of AMD, ARM, Broadcom, and Intel\n",
      "Prediction w/ prefix : Come on you don't know GTC ? Everyone's Â .\n"
     ]
    }
   ],
   "source": [
    "# Use an answer prefix and limit the number of tokens in the answer\n",
    "\n",
    "question = \"What is GTC ?\"\n",
    "true_answer = \"Nvidia's GPU Technology Conference (GTC) is a series of technical conferences held around the world.\"\n",
    "answer_prefix = \"Come on you don't know GTC ? Everyone\"\n",
    "max_new_tokens = 30\n",
    "\n",
    "pred_answer_with_prefix = pipe(context, question=question, answer_prefix=answer_prefix, press=press, max_new_tokens=max_new_tokens)[\"answer\"]\n",
    "pred_answer_without_prefix = pipe(context, question=question, press=press, max_new_tokens=max_new_tokens)[\"answer\"]\n",
    "\n",
    "print(f\"Question:              {question}\")\n",
    "print(f\"Answer:                {true_answer}\")\n",
    "print(f\"Prediction w/o prefix: {pred_answer_without_prefix}\")\n",
    "print(f\"Prediction w/ prefix : {answer_prefix + pred_answer_with_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:         Complete this sentence: In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster,\n",
      "Answer:           to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software\n",
      "Prediction w/ Q:  The information provided is a summary of the text about Nvidia Corporation.\n",
      "Prediction w/o Q: Nvidia Corporation\n"
     ]
    }
   ],
   "source": [
    "# SnapKV use the latest queries to prune the KV-cache. It's hence more efficient if we include the question during compression as the latest queries will correspond to the question.\n",
    "# However it implies also implies that SnapKV cannot compress well the context independently of the question (e.g. as in a chat use case)\n",
    "\n",
    "\n",
    "question = \"Complete this sentence: In April 2016, Nvidia produced the DGX-1 based on an 8 GPU cluster,\"\n",
    "true_answer = (\n",
    "    \"to improve the ability of users to use deep learning by combining GPUs with integrated deep learning software\"\n",
    ")\n",
    "\n",
    "press = SnapKVPress(compression_ratio=0.8)\n",
    "\n",
    "pred_answer_with_question = pipe(context + question, press=press)[\"answer\"]\n",
    "pred_answer_without_question = pipe(context, question=question, press=press)[\"answer\"]\n",
    "\n",
    "print(f\"Question:         {question}\")\n",
    "print(f\"Answer:           {true_answer}\")\n",
    "print(f\"Prediction w/ Q:  {pred_answer_with_question}\")\n",
    "print(f\"Prediction w/o Q: {pred_answer_without_question}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
