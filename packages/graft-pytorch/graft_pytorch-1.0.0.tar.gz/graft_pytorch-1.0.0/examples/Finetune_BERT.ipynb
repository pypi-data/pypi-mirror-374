{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8714df0e",
   "metadata": {},
   "source": [
    "# BERT Fine-tuning with GRAFT\n",
    "\n",
    "Setup notebook environment and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6162101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /home/ashishjv1/CODE/GRAFT-Main to Python path\n",
      "Project modules found successfully!\n",
      "Project modules found successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup notebook environment\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Get absolute path to project root\n",
    "notebook_path = Path('.').resolve()\n",
    "project_root = str(notebook_path.parent)\n",
    "\n",
    "# Add project root to Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added {project_root} to Python path\")\n",
    "\n",
    "# Verify imports will work\n",
    "import decompositions\n",
    "import grad_dist\n",
    "print(\"Project modules found successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3512ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "# !pip install -q sentence-transformers transformers datasets wandb eco2ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f299dd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashishjv1/miniconda3/envs/transformers/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-26 18:55:48.887348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-26 18:55:48.887393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-26 18:55:48.888753: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-26 18:55:48.896208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 18:55:48.887348: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-26 18:55:48.887393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-26 18:55:48.888753: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-26 18:55:48.896208: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 18:55:49.693658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-05-26 18:55:49.693658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /home/ashishjv1/CODE/GRAFT-Main\n",
      "Python path: ['/home/ashishjv1/CODE/GRAFT-Main', '/home/ashishjv1/miniconda3/envs/transformers/lib/python39.zip', '/home/ashishjv1/miniconda3/envs/transformers/lib/python3.9', '/home/ashishjv1/miniconda3/envs/transformers/lib/python3.9/lib-dynload', '', '/home/ashishjv1/miniconda3/envs/transformers/lib/python3.9/site-packages', '/home/ashishjv1/OPS_contrib/transformers/src', '/home/ashishjv1/miniconda3/envs/transformers/lib/python3.9/site-packages/setuptools/_vendor', '/tmp/tmpar__xstu']\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from datasets import load_dataset\n",
    "# Project imports - using absolute imports\n",
    "from decompositions import index_sel\n",
    "from grad_dist import calnorm\n",
    "from utils.model_mapper import ModelMapper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import tqdm\n",
    "# Third party imports - with error handling\n",
    "try:\n",
    "    import wandb\n",
    "    import eco2ai\n",
    "    EXTERNAL_IMPORTS_OK = True\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Some dependencies not found - {e}\")\n",
    "    print(\"Please run: pip install sentence-transformers transformers wandb eco2ai\")\n",
    "    EXTERNAL_IMPORTS_OK = False\n",
    "\n",
    "# System utilities\n",
    "import math\n",
    "import itertools\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "print(f\"Using project root: {project_root}\")\n",
    "print(f\"Python path: {sys.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e36039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install eco2ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "968d7b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset()\n",
    "\n",
    "# df = load_dataset_into_to_dataframe()\n",
    "# partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2c029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0ac738",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_val = pd.read_csv(\"val.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b884dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"train.csv\",\n",
    "        \"validation\": \"val.csv\",\n",
    "        \"test\": \"test.csv\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe47d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashishjv1/OPS_contrib/transformers/src/transformers/models/auto/tokenization_auto.py:897: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    ")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b259ef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877d4290",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3276.65 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3276.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db3d43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1ad23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 35000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'text', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b0efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71792daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d750d814",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1 = IMDBDataset(imdb_dataset, partition_key=\"train\")\n",
    "val_dataset1 = IMDBDataset(imdb_dataset, partition_key=\"validation\")\n",
    "test_dataset1 = IMDBDataset(imdb_dataset, partition_key=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f0984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4fa1903",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader1 = DataLoader(\n",
    "    dataset=train_dataset1,\n",
    "    batch_size=100,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader1 = DataLoader(\n",
    "    dataset=val_dataset1,\n",
    "    batch_size=100,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader1 = DataLoader(\n",
    "    dataset=test_dataset1,\n",
    "    batch_size=100,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6745316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=100,\n",
    "    shuffle=False, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=100,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=100,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29344a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "####Some Changes to rank_selection Method for BERT fine-tuning\n",
    "\n",
    "def sample_selection(trainloader, data3, model, batch_size, fraction, sel_iter, numEpochs, device):\n",
    "\n",
    "\n",
    "    indices = []\n",
    "    l2 = []    \n",
    "    len_ranks = batch_size * fraction\n",
    "\n",
    "    ranks = np.arange(int(len_ranks - (len_ranks * fraction)), int((len_ranks + (len_ranks * fraction))), 1, dtype=int)\n",
    "    num_selections = int(numEpochs / sel_iter)\n",
    "    candidates = math.ceil(len(ranks) / num_selections)\n",
    "        \n",
    "    candidate_ranks = list(np.random.choice(list(ranks), size=candidates, replace=False))\n",
    "    if len(candidate_ranks) > 3:\n",
    "        candidate_ranks = list(np.random.choice(list(candidate_ranks), size=3, replace=False))\n",
    "    print(\"current selected rank candidates:\", candidate_ranks)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for _, ((trainsamples), V) in enumerate(tqdm.tqdm(zip(trainloader, data3))):\n",
    "\n",
    "    \n",
    "        cached_state_dict = copy.deepcopy(model.state_dict())\n",
    "        clone_dict = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "#         net.load_state_dict(cached_state_dict)\n",
    "        \n",
    "        A = trainsamples[\"attention_mask\"].T\n",
    "        \n",
    "        out = model(trainsamples, indices=None, last=True, freeze=True)\n",
    "          \n",
    "        loss = out[\"loss\"]\n",
    "        loss.backward()\n",
    "        l_grad = model.model.classifier.weight.grad[0]\n",
    "        l0_grad = copy.deepcopy(l_grad)\n",
    "        model.zero_grad()\n",
    "        \n",
    "\n",
    "        distance_dict = {}\n",
    "\n",
    "        for ranks in candidate_ranks:\n",
    "            model.load_state_dict(cached_state_dict)\n",
    "            \n",
    "            idx2 = index_sel(V,  min(ranks, A.shape[1]))\n",
    "            idx2 = list(set((itertools.chain(*idx2))))\n",
    "            \n",
    "            \n",
    "            out_idx = model(trainsamples, indices=idx2, last=True, freeze=True)\n",
    "            loss_idx = out_idx[\"loss\"]\n",
    "            loss_idx.backward()\n",
    "            l0_idx_grad = model.model.classifier.weight.grad[0]\n",
    "            distance = calnorm(l0_idx_grad, l0_grad)\n",
    "            distance_dict[tuple(idx2)] = distance \n",
    "\n",
    "        \n",
    "        indices.append(list(min(distance_dict, key=distance_dict.get)))\n",
    "\n",
    "\n",
    "    del cached_state_dict\n",
    "    del clone_dict\n",
    "    del model\n",
    "    torch.cuda.empty_cache()    \n",
    "    gc.collect()\n",
    "    \n",
    "    l2 = indices[0]    \n",
    "    for i in range(len(indices) - 1):\n",
    "        l2 = l2 + list(np.array(l2[-1]) + np.array(indices[i + 1]))\n",
    "    \n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d21ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('/beegfs/home/a.jha/DEIM_IS-Tests/')\n",
    "\n",
    "\n",
    "arguments = type('', (), {'model': 'bert', 'numClasses': 2, 'device': 'cuda'})()\n",
    "model_mapper = ModelMapper(arguments)\n",
    "net = model_mapper.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef2fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.35\n",
    "batch_size = 100\n",
    "# sel_iter = 10\n",
    "numEpochs = 30\n",
    "model_name = \"bert\"\n",
    "optimizer_name = \"adam\"\n",
    "weight_decay = 0.0001\n",
    "lr = 5e-5\n",
    "grad_clip = 0.00\n",
    "selection_iter = 10\n",
    "device = \"cuda\"\n",
    "sched = \"cosine\"\n",
    "\n",
    "selection = 0\n",
    "dataset_name = \"IMDB\"\n",
    "\n",
    "\n",
    "wandb.login()\n",
    "config = {\"lr\": lr, \"batch_size\": batch_size}\n",
    "config.update({\"architecture\": f'{net}'})\n",
    "\n",
    "warm_start = True\n",
    "\n",
    "if optimizer_name.lower() == \"adam\":\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = weight_decay)\n",
    "\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "if warm_start:\n",
    "    ttype = \"warm\"\n",
    "else:\n",
    "    ttype = \"nowarm\"\n",
    "\n",
    "    \n",
    "trn_losses = list()\n",
    "val_losses = list()\n",
    "trn_acc = list()\n",
    "val_acc = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/a.jha/.local/lib/python3.7/site-packages/eco2ai/emission_track.py:141: UserWarning: \n",
      "If you use a VPN, you may have problems with identifying your country by IP.\n",
      "It is recommended to disable VPN or\n",
      "manually install the ISO-Alpha-2 code of your country during initialization of the Tracker() class.\n",
      "You can find the ISO-Alpha-2 code of your country here: https://www.iban.com/country-codes\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "tracker = eco2ai.Tracker(\n",
    "project_name=f\"BERT_dset-IMDB_bs-{batch_size}\", \n",
    "experiment_description=\"FineTune BERT\",\n",
    "file_name=f\"emission_-{model_name}_dset-{dataset_name}_bs-{batch_size}_epochs-{numEpochs}_fraction-{fraction}_{optimizer_name}_{ttype}.csv\"\n",
    ")\n",
    "tracker.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f8dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [01:06<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "device = \"cuda\"\n",
    "decomp_type = \"torch\"\n",
    "\n",
    "V_list = []\n",
    "for idx, (batch) in enumerate(tqdm(train_loader1)):\n",
    "    \n",
    "    embeddings = sentence_model.encode(batch[\"text\"])\n",
    "#     print(embeddings.shape)\n",
    "    outs = np.reshape(embeddings,(-1,embeddings.shape[0]))\n",
    "    if decomp_type == \"torch\":\n",
    "        U, S, Vt = torch.linalg.svd(torch.tensor(outs).to(device),full_matrices=False)\n",
    "        Vt = Vt.detach().cpu().numpy()\n",
    "    else:\n",
    "        U, S, Vt = np.linalg.svd(outs,full_matrices=False)\n",
    "        \n",
    "    V_list.append(Vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c0a30f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|          | 0/350 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/beegfs/home/a.jha/DEIM_IS-Tests/examples/wandb/run-20240511_214125-yuyamfc0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/pranh_lab/Smart_Sampling_bert_IMDB/runs/yuyamfc0\" target=\"_blank\">wd0.0001_optadam_bs100_gclip0.0_lr5e-05_f0.35_siter10</a></strong> to <a href=\"https://wandb.ai/pranh_lab/Smart_Sampling_bert_IMDB\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:11<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 37.0972, Train Accuracy: 96.46%\n",
      "Highest Accuracy: 0.9344\n",
      "Validation Accuracy: 0.9344\n",
      "Validation Loss tensor(8.9431, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/30], Loss: 20.6142, Train Accuracy: 98.08%\n",
      "Highest Accuracy: 0.9308\n",
      "Validation Accuracy: 0.9308\n",
      "Validation Loss tensor(10.1698, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/30], Loss: 9.9796, Train Accuracy: 99.14%\n",
      "Highest Accuracy: 0.9318\n",
      "Validation Accuracy: 0.9318\n",
      "Validation Loss tensor(11.2833, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/30], Loss: 9.0185, Train Accuracy: 99.20%\n",
      "Highest Accuracy: 0.9284\n",
      "Validation Accuracy: 0.9284\n",
      "Validation Loss tensor(11.6260, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/30], Loss: 6.3079, Train Accuracy: 99.39%\n",
      "Highest Accuracy: 0.927\n",
      "Validation Accuracy: 0.927\n",
      "Validation Loss tensor(14.1805, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/30], Loss: 4.5874, Train Accuracy: 99.58%\n",
      "Highest Accuracy: 0.931\n",
      "Validation Accuracy: 0.931\n",
      "Validation Loss tensor(13.8740, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/30], Loss: 2.4680, Train Accuracy: 99.79%\n",
      "Highest Accuracy: 0.931\n",
      "Validation Accuracy: 0.931\n",
      "Validation Loss tensor(14.1873, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/30], Loss: 2.6307, Train Accuracy: 99.83%\n",
      "Highest Accuracy: 0.9296\n",
      "Validation Accuracy: 0.9296\n",
      "Validation Loss tensor(14.7058, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/30], Loss: 4.7977, Train Accuracy: 99.60%\n",
      "Highest Accuracy: 0.9246\n",
      "Validation Accuracy: 0.9246\n",
      "Validation Loss tensor(19.2005, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 350/350 [06:06<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/30], Loss: 1.9311, Train Accuracy: 99.82%\n",
      "Highest Accuracy: 0.9284\n",
      "Validation Accuracy: 0.9284\n",
      "Validation Loss tensor(16.6825, device='cuda:0')\n",
      "current selected rank candidates: [44, 25, 23]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [04:00,  1.45it/s]\n",
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/30], Loss: 0.1621, Train Accuracy: 99.95%\n",
      "Highest Accuracy: 0.9322\n",
      "Validation Accuracy: 0.9322\n",
      "Validation Loss tensor(19.2316, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/30], Loss: 0.2363, Train Accuracy: 99.93%\n",
      "Highest Accuracy: 0.93\n",
      "Validation Accuracy: 0.93\n",
      "Validation Loss tensor(19.1298, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/30], Loss: 0.0528, Train Accuracy: 99.99%\n",
      "Highest Accuracy: 0.9322\n",
      "Validation Accuracy: 0.9322\n",
      "Validation Loss tensor(18.3071, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/30], Loss: 0.0442, Train Accuracy: 99.99%\n",
      "Highest Accuracy: 0.9364\n",
      "Validation Accuracy: 0.9364\n",
      "Validation Loss tensor(20.3868, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/30], Loss: 0.0477, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.9306\n",
      "Validation Accuracy: 0.9306\n",
      "Validation Loss tensor(19.6749, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/30], Loss: 0.0087, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.9342\n",
      "Validation Accuracy: 0.9342\n",
      "Validation Loss tensor(19.2319, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/30], Loss: 0.0749, Train Accuracy: 99.99%\n",
      "Highest Accuracy: 0.9298\n",
      "Validation Accuracy: 0.9298\n",
      "Validation Loss tensor(17.7693, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/30], Loss: 0.1542, Train Accuracy: 99.90%\n",
      "Highest Accuracy: 0.9314\n",
      "Validation Accuracy: 0.9314\n",
      "Validation Loss tensor(20.5747, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/30], Loss: 0.0061, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.9328\n",
      "Validation Accuracy: 0.9328\n",
      "Validation Loss tensor(19.0781, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/30], Loss: 0.0080, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.9304\n",
      "Validation Accuracy: 0.9304\n",
      "Validation Loss tensor(22.0908, device='cuda:0')\n",
      "current selected rank candidates: [43, 37, 29]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "350it [04:21,  1.34it/s]\n",
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/30], Loss: 0.8001, Train Accuracy: 99.73%\n",
      "Highest Accuracy: 0.925\n",
      "Validation Accuracy: 0.925\n",
      "Validation Loss tensor(19.0556, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/30], Loss: 0.0558, Train Accuracy: 99.98%\n",
      "Highest Accuracy: 0.9324\n",
      "Validation Accuracy: 0.9324\n",
      "Validation Loss tensor(20.1032, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/30], Loss: 0.0476, Train Accuracy: 99.98%\n",
      "Highest Accuracy: 0.9344\n",
      "Validation Accuracy: 0.9344\n",
      "Validation Loss tensor(18.3998, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/30], Loss: 0.0178, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.937\n",
      "Validation Accuracy: 0.937\n",
      "Validation Loss tensor(17.2147, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/30], Loss: 0.0600, Train Accuracy: 99.97%\n",
      "Highest Accuracy: 0.9298\n",
      "Validation Accuracy: 0.9298\n",
      "Validation Loss tensor(21.8764, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/30], Loss: 0.0218, Train Accuracy: 100.00%\n",
      "Highest Accuracy: 0.9366\n",
      "Validation Accuracy: 0.9366\n",
      "Validation Loss tensor(16.4206, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/30], Loss: 0.0450, Train Accuracy: 99.99%\n",
      "Highest Accuracy: 0.9284\n",
      "Validation Accuracy: 0.9284\n",
      "Validation Loss tensor(19.5699, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/30], Loss: 0.3873, Train Accuracy: 99.96%\n",
      "Highest Accuracy: 0.9322\n",
      "Validation Accuracy: 0.9322\n",
      "Validation Loss tensor(19.1495, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/30], Loss: 0.2201, Train Accuracy: 99.96%\n",
      "Highest Accuracy: 0.9308\n",
      "Validation Accuracy: 0.9308\n",
      "Validation Loss tensor(19.0834, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/30], Loss: 0.0734, Train Accuracy: 99.98%\n",
      "Highest Accuracy: 0.9336\n",
      "Validation Accuracy: 0.9336\n",
      "Validation Loss tensor(18.1433, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# tracker.start()\n",
    "for epoch in range(numEpochs):\n",
    "#     before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    net.train()\n",
    "    if (epoch) % selection_iter == 0:\n",
    "        if warm_start and selection == 0:\n",
    "            trainloader = train_loader\n",
    "            selection += 1\n",
    "        else:\n",
    "            train_model = net\n",
    "            cached_state_dict = copy.deepcopy(train_model.state_dict())\n",
    "#             clone_dict = copy.deepcopy(train_model.state_dict())\n",
    "            indices = sample_selection(train_loader, V_list, train_model, batch_size, fraction, selection_iter, numEpochs, device)\n",
    "#             indices = rank_selection(train_loader, V_list, net, batch_size, sel_iter, numEpochs, device=\"cuda\")\n",
    "            indices = [int(i) for i in indices]\n",
    "            net.load_state_dict(cached_state_dict)\n",
    "\n",
    "            selection += 1\n",
    "\n",
    "            datasubset = torch.utils.data.Subset(train_dataset, indices)\n",
    "            new_trainloader = torch.utils.data.DataLoader(datasubset, batch_size=batch_size,\n",
    "                                            shuffle=True, pin_memory=False, num_workers=1)\n",
    "\n",
    "            trainloader = new_trainloader\n",
    "            \n",
    "            ## Unfreeze all parameters for learning\n",
    "            for param in net.parameters():\n",
    "                param.requires_grad = True\n",
    "            \n",
    "            del train_model\n",
    "    \n",
    "    \n",
    "    \n",
    "    for idx, (batch) in enumerate(tqdm.tqdm(trainloader)):\n",
    "        if wandb.run is None:\n",
    "            name = f\"wd{weight_decay}_opt{optimizer_name}_bs{batch_size}_gclip{grad_clip}_lr{lr}_f{fraction}_siter{selection_iter}\"\n",
    "            wandb.init(project=f\"Smart_Sampling_{model_name}_{dataset_name}\", config=config, name=name)\n",
    "        outputs = net(batch, indices=None, last=False, freeze=False)\n",
    "        \n",
    "        logits = outputs[\"logits\"]\n",
    "        \n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip:\n",
    "            nn.utils.clip_grad_value_(net.parameters(), grad_clip)\n",
    "        optimizer.step()    \n",
    "\n",
    "    \n",
    "    if (epoch+1) % 1 == 0:\n",
    "                trn_loss = 0\n",
    "                trn_correct = 0\n",
    "                trn_total = 0\n",
    "                val_loss = 0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                tst_correct = 0\n",
    "                tst_total = 0\n",
    "                tst_loss = 0\n",
    "                curr_high = 0\n",
    "                net.eval()\n",
    "                with torch.no_grad():\n",
    "                    for _, (batch) in enumerate(trainloader):\n",
    "#                             inputs, targets = inputs.to(device), \\\n",
    "#                                               targets.to(device, non_blocking=True)\n",
    "                            output = net(batch, indices=None, last=False, freeze=False)\n",
    "#                             loss = torch.nn.functional.cross_entropy(outputs, targets)\n",
    "                            trn_loss += output[\"loss\"]\n",
    "                            logits = output[\"logits\"]\n",
    "                            predicted = torch.argmax(logits, 1)\n",
    "            \n",
    "                            targets = batch[\"label\"].to(device)\n",
    "                \n",
    "                            trn_total += targets.size(0)\n",
    "                            trn_correct += predicted.eq(targets).sum().item()\n",
    "                    trn_losses.append(trn_loss)\n",
    "                    trn_acc.append(trn_correct / trn_total)\n",
    "                    \n",
    "                with torch.no_grad():        \n",
    "                    for _, (batch) in enumerate(val_loader):\n",
    "                            output = net(batch, indices=None, last=False, freeze=False)\n",
    "                            val_loss += output[\"loss\"]\n",
    "                            logits = output[\"logits\"]\n",
    "                            predicted = torch.argmax(logits, 1)\n",
    "                            targets = batch[\"label\"].to(device)\n",
    "                            val_total += targets.size(0)\n",
    "                            val_correct += predicted.eq(targets).sum().item()\n",
    "                    val_losses.append(val_loss)\n",
    "                    val_acc.append(val_correct / val_total)\n",
    "\n",
    "                if val_acc[-1] > curr_high:\n",
    "                    curr_high = val_acc[-1]\n",
    "\n",
    "\n",
    "                wandb.log({\"Validation accuracy\": curr_high, \"Val Loss\":val_losses[-1]/100,\n",
    "                        \"loss\": trn_losses[-1]/100, \"Train Accuracy\": trn_acc[-1]*100, \"Epoch\": epoch})\n",
    "                \n",
    "                \n",
    "                wandb.log({\"loss\": trn_losses[-1]/100, \"Train Accuracy\": trn_acc[-1]*100, \"Epoch\": epoch})      \n",
    "\n",
    "                print(\"Epoch [{}/{}], Loss: {:.4f}, Train Accuracy: {:.2f}%\".format(epoch+1, numEpochs,\n",
    "                                                                                    trn_losses[-1],\n",
    "                                                                                    trn_acc[-1]*100))\n",
    "\n",
    "                print(\"Highest Accuracy:\", curr_high)\n",
    "                print(\"Validation Accuracy:\", val_acc[-1])\n",
    "                print(\"Validation Loss\", val_losses[-1])\n",
    "tracker.stop()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f98cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 12 00:06:57 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:C1:00.0 Off |                    0 |\r\n",
      "| N/A   45C    P0              89W / 400W |  35226MiB / 40960MiB |      0%      Default |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A    971844      C   ...ared/opt/python-3.7.1/bin/python3.7    35212MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b41ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
