# This workflow will upload a Python Package using Twine when a release is created
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python#publishing-to-package-registries

# This workflow uses actions that are not certified by GitHub.
# They are provided by a third-party and are governed by
# separate terms of service, privacy policy, and support
# documentation.

name: Upload Python Package

on:
  workflow_dispatch:
  release:
    types: [published]

jobs:
  build:
    name: Build distribution üì¶
    runs-on: runner-set
    steps:
      - uses: actions/checkout@v5
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install build
      - name: Compile i18n messages
        run: |
          msgfmt locale/ko/LC_MESSAGES/messages.po -o locale/ko/LC_MESSAGES/messages.mo
          msgfmt locale/en/LC_MESSAGES/messages.po -o locale/en/LC_MESSAGES/messages.mo
      - name: Build package
        run: python -m build

      - name: Install AWS CLI
        run: pip install awscli

      - name: Store the distribution packages to MinIO ‚òÅÔ∏è
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARTIFACTS_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARTIFACTS_S3_SECRET_KEY }}
        run: |
          # Create tarball of distribution packages
          tar -czf dist.tar.gz -C dist .

          # Upload to MinIO using AWS CLI
          BUCKET_NAME="${{ vars.ARTIFACTS_S3_BUCKET_NAME }}"
          OBJECT_KEY="dist.tar.gz"
          
          aws s3 cp dist.tar.gz s3://$BUCKET_NAME/$OBJECT_KEY --endpoint-url=https://datamaker-kr.cdn.datamaker.io

          echo "Artifacts uploaded to: s3://$BUCKET_NAME/$OBJECT_KEY"
          rm -f dist.tar.gz

  publish-to-pypi:
    name: Publish Python üêç distribution üì¶ to PyPI
    needs:
      - build
    environment: release
    runs-on: runner-set
    permissions:
      # IMPORTANT: this permission is mandatory for trusted publishing
      id-token: write
    steps:
      - name: Install AWS CLI
        run: pip install awscli

      - name: Retrieve distribution packages from MinIO ‚¨áÔ∏è
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARTIFACTS_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARTIFACTS_S3_SECRET_KEY }}
        run: |
          # Download from MinIO using AWS CLI
          BUCKET_NAME="${{ vars.ARTIFACTS_S3_BUCKET_NAME }}"
          OBJECT_KEY="dist.tar.gz"
          
          aws s3 cp s3://$BUCKET_NAME/$OBJECT_KEY dist.tar.gz --endpoint-url=https://datamaker-kr.cdn.datamaker.io

          # Extract artifacts
          mkdir -p dist/
          tar -xzf dist.tar.gz -C dist/
          rm -f dist.tar.gz

          echo "Artifacts retrieved from: s3://$BUCKET_NAME/$OBJECT_KEY"

      - name: Publish distribution üì¶ to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
      - name: Cleanup artifacts üßπ
        if: always()
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.ARTIFACTS_S3_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.ARTIFACTS_S3_SECRET_KEY }}
        run: |
          # Delete from MinIO using AWS CLI
          BUCKET_NAME="${{ vars.ARTIFACTS_S3_BUCKET_NAME }}"
          OBJECT_KEY="dist.tar.gz"
          
          aws s3 rm s3://$BUCKET_NAME/$OBJECT_KEY --endpoint-url=https://datamaker-kr.cdn.datamaker.io

          echo "Artifacts cleaned up from: s3://$BUCKET_NAME/$OBJECT_KEY"
