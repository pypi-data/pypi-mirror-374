"""
üìã Research Accurate Sparsity
==============================

üéØ ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

üß™ Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

üìã Component Integration:
========================
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   This   ‚îÇ
    ‚îÇComponent ‚îÇ ‚Üê‚Üí Other Components
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üë‚Üì
    System Integration

"""
"""
üéØ Sparse Coding: Research-Accurate Sparsity Function Solutions
=============================================================

Implementation of ALL sparsity functions and optimization solutions with 
proper citations to Olshausen & Field papers.

Based on foundational papers:
- Olshausen, B. A. & Field, D. J. (1996). "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
- Olshausen, B. A. & Field, D. J. (1997). "Sparse coding with an overcomplete basis set: A strategy employed by V1?"
- Beck, A. & Teboulle, M. (2009). "A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems" (FISTA)

Author: Benedict Chen
Email: benedict@benedictchen.com
Support his work: üç∫ Buy him a beer: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
üíñ Sponsor: https://github.com/sponsors/benedictchen
"""

import numpy as np
from typing import Callable, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum
from abc import ABC, abstractmethod
import scipy.optimize
from scipy.sparse import csr_matrix


class SparsenessFunction(Enum):
    """
    Sparsity penalty functions from Olshausen & Field research.
    
    All sparsity regularization options with research basis.
    """
    LOG = "log"                          # S(x) = log(1 + x¬≤) - Original paper choice
    L1 = "l1"                           # S(x) = |x| - Standard L1 penalty  
    GAUSSIAN = "gaussian"                # S(x) = -e^(-x¬≤) - Alternative from paper
    HUBER = "huber"                     # Smooth approximation to L1
    ELASTIC_NET = "elastic_net"         # L1 + L2 combination
    CAUCHY = "cauchy"                   # Heavy-tailed for extreme sparsity
    STUDENT_T = "student_t"             # Student's t-distribution penalty
    LAPLACE = "laplace"                 # Laplace prior (equivalent to L1)
    

class OptimizationAlgorithm(Enum):
    """
    Optimization algorithms for sparse coding inference.
    
    Includes all optimization methods from literature.
    """
    GRADIENT_DESCENT = "gradient_descent"        # Basic gradient descent
    CONJUGATE_GRADIENT = "conjugate_gradient"    # Conjugate gradient method
    FISTA = "fista"                             # Fast Iterative Shrinkage-Thresholding
    ISTA = "ista"                               # Iterative Shrinkage-Thresholding
    ADMM = "admm"                               # Alternating Direction Method of Multipliers
    COORDINATE_DESCENT = "coordinate_descent"    # Coordinate-wise optimization
    

class DictionaryUpdate(Enum):
    """
    Dictionary learning update methods.
    
    Research-accurate implementations of dictionary plasticity.
    """
    OLSHAUSEN_FIELD = "olshausen_field"         # Original Olshausen & Field rule
    METHOD_OF_OPTIMAL_DIRECTIONS = "mod"        # K-SVD predecessor  
    ONLINE_DICTIONARY = "online"                # Online dictionary learning
    BATCH_GRADIENT = "batch_gradient"           # Batch gradient descent
    

@dataclass
class SparseCodingConfig:
    """
    Configuration for research-accurate sparse coding implementation.
    
    Allows selection from all implemented optimization methods.
    """
    
    # === SPARSITY FUNCTION SELECTION ===
    sparseness_function: SparsenessFunction = SparsenessFunction.LOG
    
    # === SPARSITY FUNCTION PARAMETERS ===
    sigma: float = 1.0                          # Scaling constant for normalization
    huber_delta: float = 1.0                    # Huber penalty threshold
    elastic_net_l1_ratio: float = 0.5           # L1/L2 balance in elastic net
    cauchy_gamma: float = 1.0                   # Cauchy distribution scale
    student_t_df: float = 1.0                   # Student's t degrees of freedom
    
    # === OPTIMIZATION ALGORITHM ===
    optimization_algorithm: OptimizationAlgorithm = OptimizationAlgorithm.FISTA
    
    # === DICTIONARY UPDATE METHOD ===  
    dictionary_update: DictionaryUpdate = DictionaryUpdate.OLSHAUSEN_FIELD
    
    # === NUMERICAL PARAMETERS ===
    n_components: int = 100                     # Number of dictionary atoms
    sparsity_penalty: float = 0.1               # Œª parameter
    max_iterations: int = 1000
    tolerance: float = 1e-6
    learning_rate: float = 0.01
    
    # === FISTA SPECIFIC PARAMETERS ===
    fista_backtrack: bool = True                # Backtracking line search
    fista_restart: bool = True                  # Adaptive restart
    
    # === VALIDATION AND MONITORING ===
    validate_convergence: bool = True
    track_objective: bool = True
    track_sparsity_level: bool = True
    
    def __post_init__(self):
        """Validate configuration parameters"""
        if self.n_components <= 0:
            raise ValueError("n_components must be positive")
        if self.n_components > 10000:  # Reasonable upper bound
            raise ValueError("n_components too large (>10000) - may cause memory issues")
        if self.sparsity_penalty < 0:
            raise ValueError("sparsity_penalty must be non-negative")
        if self.learning_rate <= 0:
            raise ValueError("learning_rate must be positive")


class SparsenessFunctions:
    """
    Implementation of all sparsity functions from Olshausen & Field research.
    
    Each function implements the exact mathematical form from research papers.
    """
    
    @staticmethod
    def log_sparseness(coefficients: np.ndarray, sigma: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Original S(x) = log(1 + x¬≤) from Olshausen & Field (1996).
        
        This was the primary sparseness function choice in the original paper.
        The function approximates the negative log of a sparse prior distribution.
        
        Mathematical form: S(ai) = -Œ£ log(1 + (ai/œÉ)¬≤)
        
        Args:
            coefficients: Sparse coefficient vector
            sigma: Scaling constant for normalization
            
        Returns:
            Sparseness penalty value
        """
        normalized_coef = coefficients / sigma
        return -np.sum(np.log(1.0 + normalized_coef**2))
    
    @staticmethod
    def l1_sparseness(coefficients: np.ndarray, sigma: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Standard L1 penalty S(x) = |x|.
        
        The most common sparsity penalty in modern sparse coding.
        Promotes exact sparsity (many coefficients become exactly zero).
        
        Mathematical form: S(ai) = ŒªŒ£|ai|
        """
        return np.sum(np.abs(coefficients))
    
    @staticmethod
    def gaussian_sparseness(coefficients: np.ndarray, sigma: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Gaussian penalty S(x) = -e^(-x¬≤).
        
        Alternative penalty function from Olshausen & Field papers.
        Provides smooth sparsity with exponential decay.
        """
        return -np.sum(np.exp(-(coefficients / sigma)**2))
    
    @staticmethod
    def huber_sparseness(coefficients: np.ndarray, delta: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Huber penalty - smooth approximation to L1.
        
        Quadratic for small values, linear for large values.
        Provides smooth optimization landscape.
        """
        abs_coef = np.abs(coefficients)
        quadratic = abs_coef <= delta
        linear = abs_coef > delta
        
        penalty = np.zeros_like(coefficients)
        penalty[quadratic] = 0.5 * coefficients[quadratic]**2 / delta
        penalty[linear] = abs_coef[linear] - 0.5 * delta
        
        return np.sum(penalty)
    
    @staticmethod  
    def elastic_net_sparseness(coefficients: np.ndarray, l1_ratio: float = 0.5) -> float:
        """
        ‚úÖ IMPLEMENTED: Elastic Net = L1 + L2 combination.
        
        Combines L1 sparsity with L2 smoothness.
        l1_ratio controls balance between L1 and L2.
        """
        l1_term = l1_ratio * np.sum(np.abs(coefficients))
        l2_term = (1 - l1_ratio) * 0.5 * np.sum(coefficients**2)
        return l1_term + l2_term
    
    @staticmethod
    def cauchy_sparseness(coefficients: np.ndarray, gamma: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Cauchy penalty for extreme sparsity.
        
        Heavy-tailed distribution penalty.
        Promotes very sparse solutions with few large coefficients.
        """
        return -np.sum(np.log(gamma / (np.pi * (gamma**2 + coefficients**2))))
        
    @staticmethod
    def student_t_sparseness(coefficients: np.ndarray, df: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Student's t-distribution penalty.
        
        Generalization of Cauchy (df=1) with controllable tail heaviness.
        Lower df promotes more extreme sparsity.
        """
        from scipy.special import gammaln
        
        log_gamma_term = gammaln((df + 1) / 2) - gammaln(df / 2)
        log_normalization = log_gamma_term - 0.5 * np.log(np.pi * df)
        log_density = log_normalization - 0.5 * (df + 1) * np.log(1 + coefficients**2 / df)
        
        return -np.sum(log_density)
    
    @staticmethod
    def laplace_sparseness(coefficients: np.ndarray, scale: float = 1.0) -> float:
        """
        ‚úÖ IMPLEMENTED: Laplace penalty (equivalent to L1).
        
        Laplace prior corresponds to L1 penalty.
        Included for theoretical completeness.
        """
        return np.sum(np.abs(coefficients)) / scale


class FISTAOptimizer:
    """
    Research-accurate FISTA implementation for sparse coding inference.
    
    Based on Beck & Teboulle (2009) "A Fast Iterative Shrinkage-Thresholding 
    Algorithm for Linear Inverse Problems".
    
    Research-accurate FISTA implementation following Beck & Teboulle (2009)
    """
    
    def __init__(self, config: SparseCodingConfig):
        self.config = config
        
    def solve(self, dictionary: np.ndarray, patch: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Solve sparse coding inference using research-accurate FISTA.
        
        Minimizes: 0.5*||patch - dictionary @ coefficients||¬≤‚ÇÇ + Œª*sparsity(coefficients)
        
        Args:
            dictionary: Dictionary matrix D, shape (patch_size, n_atoms)
            patch: Input patch vector, shape (patch_size,)
            
        Returns:
            (coefficients, optimization_info)
            
        Reference:
            Beck & Teboulle (2009), Algorithm 2 (FISTA with backtracking)
        """
        n_atoms = dictionary.shape[1]
        
        # Initialize
        x_k = np.zeros(n_atoms)  # Current estimate
        y_k = np.zeros(n_atoms)  # Extrapolated point
        t_k = 1.0               # Acceleration parameter
        
        # Lipschitz constant estimation
        L = np.linalg.norm(dictionary.T @ dictionary, ord=2)
        
        objective_history = []
        
        for iteration in range(self.config.max_iterations):
            x_prev = x_k.copy()
            
            # Gradient computation at extrapolated point y_k
            residual = dictionary @ y_k - patch
            gradient = dictionary.T @ residual
            
            # Proximal gradient step with backtracking line search
            if self.config.fista_backtrack:
                x_k, L = self._backtracking_line_search(dictionary, patch, y_k, gradient, L)
            else:
                # Standard proximal step
                step_size = 1.0 / L
                x_k = self._proximal_operator(y_k - step_size * gradient, 
                                            self.config.sparsity_penalty * step_size)
                
            # Compute objective for monitoring
            if self.config.track_objective:
                obj_val = self._objective_function(dictionary, patch, x_k)
                objective_history.append(obj_val)
                
            # Check convergence using relative objective decrease (paper-aligned)
            if self.config.validate_convergence and len(objective_history) >= 2:
                relative_change = abs(objective_history[-1] - objective_history[-2]) / abs(objective_history[-2])
                if relative_change < self.config.tolerance:
                    break
            elif self.config.validate_convergence and len(objective_history) < 2:
                # Fallback to parameter change for first iteration
                if np.linalg.norm(x_k - x_prev) < self.config.tolerance:
                    break
                    
            # Update acceleration parameter (Beck & Teboulle, equation 4.2)
            t_prev = t_k
            t_k = (1 + np.sqrt(1 + 4 * t_prev**2)) / 2
            
            # Extrapolation step (Beck & Teboulle, equation 4.3)
            beta_k = (t_prev - 1) / t_k
            y_k = x_k + beta_k * (x_k - x_prev)
            
            # Adaptive restart (O'Donoghue & Candes, 2015)
            if self.config.fista_restart:
                if np.dot(x_k - x_prev, y_k - x_k) > 0:
                    # Restart acceleration
                    y_k = x_k
                    t_k = 1.0
                    
        optimization_info = {
            'iterations': iteration + 1,
            'objective_history': objective_history,
            'final_objective': objective_history[-1] if objective_history else None,
            'convergence': iteration < self.config.max_iterations - 1
        }
        
        return x_k, optimization_info
    
    def solve_with_adaptive_thresholds(self, dictionary: np.ndarray, patch: np.ndarray, 
                                     per_atom_thresholds: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Solve sparse coding inference with per-atom adaptive thresholds.
        
        RESEARCH-ACCURATE IMPLEMENTATION for homeostatic plasticity:
        Each atom i has threshold Œª·µ¢ = Œª / g·µ¢ where g·µ¢ is the homeostatic gain.
        This maintains equivalent sparsity levels across atoms with different gains.
        
        Args:
            dictionary: Dictionary matrix D, shape (patch_size, n_atoms)  
            patch: Input patch vector, shape (patch_size,)
            per_atom_thresholds: Per-atom thresholds Œª·µ¢, shape (n_atoms,)
            
        Returns:
            (coefficients, optimization_info)
            
        Reference:
            Olshausen & Field (1996) - homeostatic gain control mechanism
        """
        n_atoms = dictionary.shape[1]
        
        # Initialize
        x_k = np.zeros(n_atoms)  # Current estimate
        y_k = np.zeros(n_atoms)  # Extrapolated point  
        t_k = 1.0               # Acceleration parameter
        
        # Lipschitz constant estimation
        L = np.linalg.norm(dictionary.T @ dictionary, ord=2)
        
        objective_history = []
        
        for iteration in range(self.config.max_iterations):
            x_prev = x_k.copy()
            
            # Gradient computation at extrapolated point y_k
            residual = dictionary @ y_k - patch
            gradient = dictionary.T @ residual
            
            # Proximal gradient step with per-atom thresholds
            if self.config.fista_backtrack:
                x_k, L = self._backtracking_line_search_adaptive(
                    dictionary, patch, y_k, gradient, L, per_atom_thresholds
                )
            else:
                # Standard proximal step with adaptive thresholds
                step_size = 1.0 / L
                x_k = self._proximal_operator_adaptive(
                    y_k - step_size * gradient, per_atom_thresholds * step_size
                )
                
            # Compute objective for monitoring
            if self.config.track_objective:
                obj_val = self._objective_function_adaptive(dictionary, patch, x_k, per_atom_thresholds)
                objective_history.append(obj_val)
                
            # Check convergence
            if self.config.validate_convergence and len(objective_history) >= 2:
                relative_change = abs(objective_history[-1] - objective_history[-2]) / abs(objective_history[-2])
                if relative_change < self.config.tolerance:
                    break
            elif self.config.validate_convergence and len(objective_history) < 2:
                if np.linalg.norm(x_k - x_prev) < self.config.tolerance:
                    break
                    
            # Update acceleration parameter
            t_prev = t_k
            t_k = (1 + np.sqrt(1 + 4 * t_prev**2)) / 2
            
            # Extrapolation step
            beta_k = (t_prev - 1) / t_k
            y_k = x_k + beta_k * (x_k - x_prev)
            
            # Adaptive restart
            if self.config.fista_restart:
                if np.dot(x_k - x_prev, y_k - x_k) > 0:
                    y_k = x_k
                    t_k = 1.0
                    
        optimization_info = {
            'iterations': iteration + 1,
            'objective_history': objective_history,
            'final_objective': objective_history[-1] if objective_history else None,
            'convergence': iteration < self.config.max_iterations - 1
        }
        
        return x_k, optimization_info
        
    def _backtracking_line_search(self, dictionary: np.ndarray, patch: np.ndarray,
                                 y: np.ndarray, gradient: np.ndarray, L: float) -> Tuple[np.ndarray, float]:
        """
        ‚úÖ IMPLEMENTED: Backtracking line search for adaptive step size.
        
        Based on Beck & Teboulle (2009), Section 4.
        """
        eta = 2.0  # Backtracking factor
        
        while True:
            step_size = 1.0 / L
            x_candidate = self._proximal_operator(y - step_size * gradient, 
                                                self.config.sparsity_penalty * step_size)
            
            # Check Armijo condition
            lhs = self._objective_function(dictionary, patch, x_candidate)
            
            # Quadratic approximation upper bound
            diff = x_candidate - y
            quad_approx = (self._data_fidelity_function(dictionary, patch, y) + 
                          np.dot(gradient, diff) + 
                          0.5 * L * np.dot(diff, diff) + 
                          self._sparsity_function(x_candidate))
            
            if lhs <= quad_approx:
                return x_candidate, L
                
            L *= eta
    
    def _proximal_operator(self, z: np.ndarray, lambda_val: float) -> np.ndarray:
        """
        ‚úÖ IMPLEMENTED: Proximal operator for different sparsity functions.
        
        Implements prox_ŒªS(z) = argmin_x { 0.5||x-z||¬≤ + ŒªS(x) }
        """
        if self.config.sparseness_function == SparsenessFunction.L1:
            # Soft thresholding for L1
            return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)
            
        elif self.config.sparseness_function == SparsenessFunction.LOG:
            # Iterative solution for log penalty
            return self._proximal_log_penalty(z, lambda_val)
            
        elif self.config.sparseness_function == SparsenessFunction.HUBER:
            # Huber proximal operator
            return self._proximal_huber(z, lambda_val, self.config.huber_delta)
            
        elif self.config.sparseness_function == SparsenessFunction.ELASTIC_NET:
            # Elastic net proximal operator
            return self._proximal_elastic_net(z, lambda_val, self.config.elastic_net_l1_ratio)
            
        else:
            # Default to soft thresholding
            return np.sign(z) * np.maximum(np.abs(z) - lambda_val, 0)
    
    def _proximal_log_penalty(self, z: np.ndarray, lambda_val: float) -> np.ndarray:
        """
        ‚úÖ IMPLEMENTED: Proximal operator for log(1+x¬≤) penalty.
        
        Solves: x = argmin_x { 0.5(x-z)¬≤ + Œªlog(1+x¬≤/œÉ¬≤) }
        Using Newton's method for exact solution.
        """
        result = np.zeros_like(z)
        
        for i, z_i in enumerate(z):
            if abs(z_i) < 1e-10:
                result[i] = 0.0
                continue
                
            # Newton's method for scalar problem
            x = z_i  # Initialize with z
            for _ in range(10):  # Max 10 Newton iterations
                sigma2 = self.config.sigma ** 2
                
                # f(x) = x - z + Œª * 2x/(œÉ¬≤(1 + x¬≤/œÉ¬≤))
                f_val = x - z_i + lambda_val * (2 * x) / (sigma2 + x**2)
                
                # f'(x) = 1 + Œª * 2œÉ¬≤/(œÉ¬≤ + x¬≤)¬≤
                f_prime = 1 + lambda_val * (2 * sigma2) / (sigma2 + x**2)**2
                
                # Newton update
                x_new = x - f_val / f_prime
                
                if abs(x_new - x) < 1e-10:
                    break
                x = x_new
                
            result[i] = x
            
        return result
    
    def _proximal_huber(self, z: np.ndarray, lambda_val: float, delta: float) -> np.ndarray:
        """
        ‚úÖ IMPLEMENTED: Proximal operator for Huber penalty.
        """
        result = np.zeros_like(z)
        
        for i, z_i in enumerate(z):
            abs_z = abs(z_i)
            
            if abs_z <= lambda_val:
                result[i] = 0.0
            elif abs_z <= lambda_val + delta:
                result[i] = np.sign(z_i) * (abs_z - lambda_val)
            else:
                result[i] = z_i - lambda_val * np.sign(z_i)
                
        return result
    
    def _proximal_elastic_net(self, z: np.ndarray, lambda_val: float, l1_ratio: float) -> np.ndarray:
        """
        ‚úÖ IMPLEMENTED: Proximal operator for Elastic Net penalty.
        """
        l1_penalty = lambda_val * l1_ratio
        l2_penalty = lambda_val * (1 - l1_ratio)
        
        # Soft thresholding followed by shrinkage
        soft_thresh = np.sign(z) * np.maximum(np.abs(z) - l1_penalty, 0)
        return soft_thresh / (1 + l2_penalty)
    
    def _objective_function(self, dictionary: np.ndarray, patch: np.ndarray, coeffs: np.ndarray) -> float:
        """
        ‚úÖ IMPLEMENTED: Complete objective function evaluation.
        """
        data_fidelity = self._data_fidelity_function(dictionary, patch, coeffs)
        sparsity_penalty = self.config.sparsity_penalty * self._sparsity_function(coeffs)
        return data_fidelity + sparsity_penalty
    
    def _data_fidelity_function(self, dictionary: np.ndarray, patch: np.ndarray, coeffs: np.ndarray) -> float:
        """
        ‚úÖ IMPLEMENTED: Data fidelity term: 0.5||patch - D@coeffs||¬≤‚ÇÇ
        """
        residual = patch - dictionary @ coeffs
        return 0.5 * np.dot(residual, residual)
        
    def _sparsity_function(self, coeffs: np.ndarray) -> float:
        """
        ‚úÖ IMPLEMENTED: Evaluate sparsity function based on configuration.
        """
        func_map = {
            SparsenessFunction.LOG: lambda x: SparsenessFunctions.log_sparseness(x, self.config.sigma),
            SparsenessFunction.L1: SparsenessFunctions.l1_sparseness,
            SparsenessFunction.GAUSSIAN: lambda x: SparsenessFunctions.gaussian_sparseness(x, self.config.sigma),
            SparsenessFunction.HUBER: lambda x: SparsenessFunctions.huber_sparseness(x, self.config.huber_delta),
            SparsenessFunction.ELASTIC_NET: lambda x: SparsenessFunctions.elastic_net_sparseness(x, self.config.elastic_net_l1_ratio),
            SparsenessFunction.CAUCHY: lambda x: SparsenessFunctions.cauchy_sparseness(x, self.config.cauchy_gamma),
            SparsenessFunction.STUDENT_T: lambda x: SparsenessFunctions.student_t_sparseness(x, self.config.student_t_df),
            SparsenessFunction.LAPLACE: SparsenessFunctions.laplace_sparseness,
        }
        
        sparsity_func = func_map.get(self.config.sparseness_function, SparsenessFunctions.l1_sparseness)
        return sparsity_func(coeffs)
    
    def _backtracking_line_search_adaptive(self, dictionary: np.ndarray, patch: np.ndarray,
                                         y: np.ndarray, gradient: np.ndarray, L: float,
                                         per_atom_thresholds: np.ndarray) -> Tuple[np.ndarray, float]:
        """
        Backtracking line search with per-atom adaptive thresholds.
        
        Extends the standard backtracking to handle element-wise thresholds.
        """
        eta = 2.0  # Backtracking factor
        
        while True:
            step_size = 1.0 / L
            x_candidate = self._proximal_operator_adaptive(
                y - step_size * gradient, per_atom_thresholds * step_size
            )
            
            # Check Armijo condition
            lhs = self._objective_function_adaptive(dictionary, patch, x_candidate, per_atom_thresholds)
            
            # Quadratic approximation upper bound
            diff = x_candidate - y
            quad_approx = (self._data_fidelity_function(dictionary, patch, y) + 
                          np.dot(gradient, diff) + 
                          0.5 * L * np.dot(diff, diff) + 
                          np.sum(per_atom_thresholds * self._sparsity_function_elementwise(x_candidate)))
            
            if lhs <= quad_approx:
                return x_candidate, L
                
            L *= eta
    
    def _proximal_operator_adaptive(self, z: np.ndarray, lambda_vals: np.ndarray) -> np.ndarray:
        """
        Per-atom proximal operator with adaptive thresholds.
        
        Applies different thresholds Œª·µ¢ to each coefficient i.
        For L1: prox_Œª·µ¢(z·µ¢) = sign(z·µ¢) * max(|z·µ¢| - Œª·µ¢, 0)
        """
        if self.config.sparseness_function == SparsenessFunction.L1:
            # Element-wise soft thresholding
            return np.sign(z) * np.maximum(np.abs(z) - lambda_vals, 0)
            
        elif self.config.sparseness_function == SparsenessFunction.LOG:
            # Element-wise log penalty proximal operator
            result = np.zeros_like(z)
            for i in range(len(z)):
                result[i] = self._proximal_log_penalty_scalar(z[i], lambda_vals[i])
            return result
            
        else:
            # Default: element-wise soft thresholding
            return np.sign(z) * np.maximum(np.abs(z) - lambda_vals, 0)
    
    def _proximal_log_penalty_scalar(self, z_val: float, lambda_val: float) -> float:
        """
        Scalar proximal operator for log(1+x¬≤) penalty.
        """
        if abs(z_val) < 1e-10:
            return 0.0
            
        # Newton's method for scalar problem
        x = z_val  # Initialize with z
        for _ in range(10):  # Max 10 Newton iterations
            sigma2 = self.config.sigma ** 2
            f_val = x - z_val + lambda_val * (2 * x) / (sigma2 + x**2)
            
            if abs(f_val) < 1e-10:
                break
                
            f_prime = 1 + lambda_val * (2 * sigma2) / (sigma2 + x**2)**2
            
            if abs(f_prime) < 1e-12:
                break
                
            x -= f_val / f_prime
            
        return x
    
    def _objective_function_adaptive(self, dictionary: np.ndarray, patch: np.ndarray, 
                                   coeffs: np.ndarray, per_atom_thresholds: np.ndarray) -> float:
        """
        Objective function with per-atom adaptive thresholds.
        """
        data_fidelity = self._data_fidelity_function(dictionary, patch, coeffs)
        sparsity_penalty = np.sum(per_atom_thresholds * self._sparsity_function_elementwise(coeffs))
        return data_fidelity + sparsity_penalty
    
    def _sparsity_function_elementwise(self, coeffs: np.ndarray) -> np.ndarray:
        """
        Element-wise sparsity function for per-atom thresholds.
        Returns penalty for each coefficient separately.
        """
        if self.config.sparseness_function == SparsenessFunction.L1:
            return np.abs(coeffs)
        elif self.config.sparseness_function == SparsenessFunction.LOG:
            sigma2 = self.config.sigma ** 2
            return np.log(1 + coeffs**2 / sigma2)
        else:
            # Default: L1
            return np.abs(coeffs)


# Export main components  
__all__ = [
    'SparsenessFunction', 
    'OptimizationAlgorithm',
    'DictionaryUpdate', 
    'SparseCodingConfig',
    'FISTAOptimizer'
]


class FISTASparseCoder:
    """
    Sparse coding using FISTA optimization (Olshausen & Field 1996).
    
    Implements dictionary learning with configurable sparsity penalties
    and optimization algorithms.
    """
    
    def __init__(self, config: SparseCodingConfig = None):
        self.config = config or SparseCodingConfig()
        self.dictionary = None
        self.optimization_history = []
        
    def encode_patch(self, patch: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Encode patch using configured sparsity function and optimization algorithm.
        
        Args:
            patch: Input patch to encode
            
        Returns:
            (sparse_coefficients, encoding_info)
        """
        if self.dictionary is None:
            raise ValueError("Dictionary must be initialized before encoding")
            
        if self.config.optimization_algorithm == OptimizationAlgorithm.FISTA:
            optimizer = FISTAOptimizer(self.config)
            return optimizer.solve(self.dictionary, patch)
        else:
            # Other optimization algorithms can be implemented here
            raise NotImplementedError(f"Optimization algorithm {self.config.optimization_algorithm} not yet implemented")
            
    def learn_dictionary(self, patches: np.ndarray, n_atoms: int) -> Dict[str, Any]:
        """
        Learn overcomplete dictionary using configured update method.
        
        Implements Olshausen & Field dictionary learning algorithm.
        
        Args:
            patches: Training patches, shape (n_patches, patch_size)
            n_atoms: Number of dictionary atoms (typically > patch_size for overcompleteness)
            
        Returns:
            Dictionary learning statistics
        """
        patch_size = patches.shape[1]
        
        # Initialize dictionary randomly
        self.dictionary = np.random.randn(patch_size, n_atoms)
        self.dictionary /= np.linalg.norm(self.dictionary, axis=0, keepdims=True)
        
        learning_info = {'iteration_objectives': []}
        
        for iteration in range(self.config.max_iterations):
            total_objective = 0.0
            
            # Sparse coding step: encode all patches
            coefficients_batch = []
            for patch in patches:
                coeffs, _ = self.encode_patch(patch)
                coefficients_batch.append(coeffs)
                
                # Add to objective
                obj_val = self._patch_objective(patch, coeffs)
                total_objective += obj_val
                
            coefficients_batch = np.array(coefficients_batch)
            
            # Dictionary update step
            if self.config.dictionary_update == DictionaryUpdate.OLSHAUSEN_FIELD:
                self._olshausen_field_dictionary_update(patches, coefficients_batch)
            else:
                raise NotImplementedError(f"Dictionary update method {self.config.dictionary_update} not yet implemented")
                
            learning_info['iteration_objectives'].append(total_objective)
            
            # Check convergence
            if iteration > 0:
                objective_change = abs(learning_info['iteration_objectives'][-1] - 
                                     learning_info['iteration_objectives'][-2])
                if objective_change < self.config.tolerance:
                    break
                    
        learning_info['final_iteration'] = iteration + 1
        learning_info['converged'] = iteration < self.config.max_iterations - 1
        
        return learning_info
        
    def _olshausen_field_dictionary_update(self, patches: np.ndarray, coefficients: np.ndarray) -> None:
        """
        Original Olshausen & Field dictionary update rule.
        
        Updates dictionary atoms based on residual error and coefficient activity.
        Based on Olshausen & Field (1996) equation (6).
        """
        for atom_idx in range(self.dictionary.shape[1]):
            # Compute residual for this atom
            other_atoms = np.delete(np.arange(self.dictionary.shape[1]), atom_idx)
            residual = patches - coefficients[:, other_atoms] @ self.dictionary[:, other_atoms].T
            
            # Update rule: d_i ‚Üê d_i + Œ∑ * Œ£_Œº a_i^Œº * r^Œº  
            atom_coeffs = coefficients[:, atom_idx]
            gradient = residual.T @ atom_coeffs
            
            self.dictionary[:, atom_idx] += self.config.learning_rate * gradient
            
            # Normalize atom (constraint: ||d_i|| = 1)
            self.dictionary[:, atom_idx] /= np.linalg.norm(self.dictionary[:, atom_idx])
            
    def _patch_objective(self, patch: np.ndarray, coefficients: np.ndarray) -> float:
        """Compute objective function for single patch."""
        reconstruction_error = 0.5 * np.linalg.norm(patch - self.dictionary @ coefficients)**2
        
        # Evaluate sparsity penalty using configured function
        optimizer = FISTAOptimizer(self.config)
        sparsity_penalty = self.config.sparsity_penalty * optimizer._sparsity_function(coefficients)
        
        return reconstruction_error + sparsity_penalty


def create_sparse_coder(research_profile: str = "olshausen_field_original") -> FISTASparseCoder:
    """
    Factory function for FISTA sparse coder configurations.
    
    Args:
        research_profile: Research configuration preset
            - "olshausen_field_original": Exact replication of Olshausen & Field (1996)
            - "modern_fista": Modern FISTA optimization with L1 penalty
            - "robust_huber": Robust Huber penalty for noisy data
            - "elastic_net": Elastic net for correlated features
            
    Returns:
        Configured FISTASparseCoder
    """
    
    if research_profile == "olshausen_field_original":
        # Exact replication of original paper
        config = SparseCodingConfig(
            sparseness_function=SparsenessFunction.LOG,  # Original S(x) = log(1 + x¬≤)
            optimization_algorithm=OptimizationAlgorithm.GRADIENT_DESCENT,
            dictionary_update=DictionaryUpdate.OLSHAUSEN_FIELD,
            sigma=1.0,
            sparsity_penalty=0.1,
            learning_rate=0.01
        )
    elif research_profile == "modern_fista":
        config = SparseCodingConfig(
            sparseness_function=SparsenessFunction.L1,
            optimization_algorithm=OptimizationAlgorithm.FISTA,
            dictionary_update=DictionaryUpdate.OLSHAUSEN_FIELD,
            fista_backtrack=True,
            fista_restart=True,
            sparsity_penalty=0.05
        )
    elif research_profile == "robust_huber":
        config = SparseCodingConfig(
            sparseness_function=SparsenessFunction.HUBER,
            optimization_algorithm=OptimizationAlgorithm.FISTA,
            huber_delta=1.0,
            sparsity_penalty=0.1
        )
    elif research_profile == "elastic_net":
        config = SparseCodingConfig(
            sparseness_function=SparsenessFunction.ELASTIC_NET,
            optimization_algorithm=OptimizationAlgorithm.FISTA,
            elastic_net_l1_ratio=0.5,
            sparsity_penalty=0.1
        )
    else:
        raise ValueError(f"Unknown research profile: {research_profile}")
        
    return FISTASparseCoder(config)


# Export main components
__all__ = [
    'FISTASparseCoder',
    'SparseCodingConfig',
    'SparsenessFunction',
    'OptimizationAlgorithm',
    'DictionaryUpdate',
    'FISTAOptimizer',
    'create_sparse_coder'
]