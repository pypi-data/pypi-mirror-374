"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          Sparse Coding Feature Extraction                      â•‘
â•‘                   Learning Meaningful Features Through Sparsity                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Created by: Benedict Chen
Contact: github.com/benedictchen â€¢ benedict@benedictchen.com
If this code helped your research, buy me a coffee â˜•
If it got you published, maybe a fancy dinner ðŸ½ï¸
If it won you awards, definitely a Tesla ðŸš— (for science!)

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                RESEARCH FOUNDATION                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸŽ“ FOUNDATIONAL PAPERS:
â€¢ Olshausen & Field (1996) "Emergence of simple-cell receptive field properties"
  - Discovered that sparse coding naturally learns edge detectors like V1 neurons
  - Established the connection between sparsity and biological visual processing

â€¢ Olshausen & Field (1997) "Sparse coding with an overcomplete basis set"
  - Introduced the mathematical framework for overcomplete sparse representations  
  - Showed how to learn redundant but efficient feature dictionaries

â€¢ Lee et al. (2006) "Efficient sparse coding algorithms"
  - Developed practical algorithms for large-scale sparse feature learning
  - Made sparse coding computationally feasible for real applications

â€¢ Coates & Ng (2011) "The importance of encoding versus training with sparse coding"
  - Demonstrated that good features matter more than classifier complexity
  - Showed sparse features achieve state-of-the-art performance on vision tasks

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                 ELI5 SUMMARY                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸŽ¨ Imagine you're an art teacher with a box of paintbrushes...

ðŸ‘¨â€ðŸŽ¨ TRADITIONAL APPROACH (Dense):
"Use ALL the brushes for every painting!" 
Result: Muddy, overworked paintings ðŸ˜µ

ðŸ§™â€â™‚ï¸ SPARSE CODING APPROACH:
"Use only the FEW brushes that matter most for each painting!"
Result: Clean, crisp, meaningful art! âœ¨

Here's how it works:
1. ðŸ“š DICTIONARY LEARNING: "What are the most useful 'brushstrokes' (features)?"
   - Learn a library of fundamental patterns (edges, textures, shapes)
   - Like discovering the "alphabet" of visual elements

2. ðŸŽ¯ SPARSE ENCODING: "For this specific image, which few features matter?"
   - Each image uses only a small subset of available features
   - Like writing words using only necessary letters, not the whole alphabet

3. ðŸ” FEATURE EXTRACTION: "What does this image 'say' in our learned language?"
   - Represent complex images with simple, meaningful feature combinations
   - Like translating a novel into its key themes and concepts

WHY SPARSITY ROCKS:
â€¢ ðŸ§  Your brain works this way! Only a few neurons fire for each stimulus
â€¢ ðŸŽ¯ Captures the most important information while ignoring noise  
â€¢ ðŸ”§ Makes features interpretable - you can see what each one detects
â€¢ âš¡ More efficient - process only what matters, ignore the rest

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                              VISUAL ARCHITECTURE                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                    SPARSE FEATURE EXTRACTION PIPELINE

    INPUT IMAGE                 PATCH EXTRACTION              LEARNED DICTIONARY
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚ ðŸ–¼ï¸  IMAGE    â”‚    â†’      â”‚ 8x8 â”‚ 8x8 â”‚ 8x8 â”‚    â†     â”‚  Dâ‚ â”‚  Dâ‚‚ â”‚  Dâ‚ƒ â”‚ 
    â”‚             â”‚            â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚    64x64    â”‚            â”‚ 8x8 â”‚ 8x8 â”‚ 8x8 â”‚ Patches   â”‚  Dâ‚„ â”‚  Dâ‚… â”‚  Dâ‚† â”‚ Features
    â”‚             â”‚            â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤           â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚             â”‚            â”‚ 8x8 â”‚ 8x8 â”‚ 8x8 â”‚           â”‚  Dâ‚‡ â”‚  Dâ‚ˆ â”‚  Dâ‚‰ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚                            â”‚
           â”‚                           â–¼                            â”‚
           â”‚                   SPARSE CODING                       â”‚
           â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
           â”‚                 â”‚ Find coefficientsâ”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚     Î± such that: â”‚
           â”‚                 â”‚   patch â‰ˆ Î£ Î±áµ¢Dáµ¢ â”‚
           â”‚                 â”‚   with few Î±áµ¢â‰ 0  â”‚
           â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â–¼                           â–¼
    SPATIAL POOLING             SPARSE COEFFICIENTS
    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ MAX â”‚ MAX â”‚               â”‚ 0 â”‚.8 â”‚ 0 â”‚ 0 â”‚.6 â”‚ â† Only few are non-zero!
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤               â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤
    â”‚ MAX â”‚ MAX â”‚               â”‚ 0 â”‚ 0 â”‚.4 â”‚ 0 â”‚ 0 â”‚
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â–¼
              FINAL FEATURE VECTOR
              [0.8, 0.6, 0.4, 0.0, ...]
               â†‘    â†‘    â†‘    â†‘
            Edge  Texture Corner None

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            MATHEMATICAL FRAMEWORK                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸ”¢ CORE OPTIMIZATION PROBLEM:

1. DICTIONARY LEARNING:
   min_{D,Î±} Î£áµ¢ [Â½||xáµ¢ - DÎ±áµ¢||â‚‚Â² + Î»||Î±áµ¢||â‚]
   
   Where:
   â€¢ xáµ¢ = input patch i
   â€¢ D = dictionary matrix (learned features)  
   â€¢ Î±áµ¢ = sparse coefficients for patch i
   â€¢ Î» = sparsity penalty (bigger Î» = more sparse)

2. SPARSE ENCODING (given learned D):
   Î±áµ¢* = argmin_Î± [Â½||xáµ¢ - DÎ±||â‚‚Â² + Î»||Î±||â‚]
   
   This is LASSO regression - balances reconstruction quality vs sparsity

3. FEATURE VECTOR CONSTRUCTION:
   f(image) = Pool({Î±â‚*, Î±â‚‚*, ..., Î±â‚™*})
   
   Pool functions: max, mean, or sum across spatial locations

ðŸŽ¯ KEY HYPERPARAMETERS:
â€¢ n_components: Size of dictionary (more = richer representation)
â€¢ patch_size: Resolution of learned features (8x8 typical)
â€¢ sparsity_penalty (Î»): Controls sparsity-quality tradeoff
â€¢ overlap_factor: How much patches overlap (more = better reconstruction)

âš¡ COMPUTATIONAL INSIGHTS:
â€¢ Dictionary learning: Alternates between updating D and Î±
â€¢ Sparse coding: Each patch encoded independently (parallelizable!)
â€¢ Pooling: Provides translation invariance and dimensionality reduction

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                             REAL-WORLD APPLICATIONS                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ðŸŒŸ REVOLUTIONARY APPLICATIONS:

1. ðŸ–¼ï¸  COMPUTER VISION:
   - Image classification: Sparse features â†’ better than hand-crafted SIFT/HOG
   - Object detection: Learn part-based representations automatically
   - Image denoising: Separate signal from noise using sparsity prior
   - Super-resolution: Reconstruct high-res images from sparse representations

2. ðŸ¥ MEDICAL IMAGING:
   - MRI reconstruction: Recover full images from undersampled k-space data
   - CT scan denoising: Reduce radiation dose while maintaining image quality  
   - Histology analysis: Automatically discover cellular patterns and structures
   - Brain imaging: Identify disease-specific activation patterns

3. ðŸŽµ AUDIO & SPEECH:
   - Music source separation: Unmix instruments using learned audio dictionaries
   - Speech recognition: Robust features that work in noisy environments
   - Audio compression: Efficient encoding using sparse representations
   - Sound classification: Environmental sound recognition and analysis

4. ðŸ“Š DATA SCIENCE:
   - Anomaly detection: Identify outliers that can't be sparsely reconstructed
   - Dimensionality reduction: Alternative to PCA with better interpretability
   - Collaborative filtering: Recommend items using sparse user-item patterns
   - Text analysis: Learn semantic word features beyond simple bag-of-words

5. ðŸ§¬ SCIENTIFIC APPLICATIONS:
   - Gene expression analysis: Discover sparse regulatory patterns
   - Climate modeling: Identify key variables driving weather patterns
   - Materials science: Learn atomic structure patterns for property prediction
   - Astronomy: Detect rare astronomical events in survey data

ðŸŽ¯ WHY SPARSE FEATURES WIN:
â€¢ Interpretability: Each feature has clear meaning (unlike deep learning black boxes)
â€¢ Efficiency: Only compute/store non-zero coefficients
â€¢ Robustness: Sparse representations generalize better to new data
â€¢ Biological plausibility: Matches how the visual cortex actually works!

================================================================================

import numpy as np
from typing import Tuple, Dict, Any, Optional, List
from .dictionary_learning import DictionaryLearner
from .sparse_coder import SparseCoder


class SparseFeatureExtractor:
    """
    High-level sparse feature extraction interface
    
    Combines dictionary learning and sparse coding to provide
    a complete feature extraction pipeline for images.
    """
    
    def __init__(
        self,
        n_components: int = 100,
        patch_size: Tuple[int, int] = (8, 8),
        sparsity_penalty: float = 0.1,
        overlap_factor: float = 0.5,
        whitening: bool = True,
        random_seed: Optional[int] = None
    ):
        """
        Initialize Sparse Feature Extractor
        
        Args:
            n_components: Number of dictionary atoms/features
            patch_size: Size of image patches
            sparsity_penalty: L1 regularization strength
            overlap_factor: Patch overlap factor (0=no overlap, 1=full overlap)
            whitening: Whether to apply whitening preprocessing
            random_seed: Random seed for reproducibility
        """
        
        self.n_components = n_components
        self.patch_size = patch_size
        self.sparsity_penalty = sparsity_penalty
        self.overlap_factor = overlap_factor
        self.whitening = whitening
        
        if random_seed is not None:
            np.random.seed(random_seed)
            
        # Components
        self.dictionary_learner = None
        self.sparse_coder = None
        self.is_fitted = False
        
        # Preprocessing parameters
        self.mean_ = None
        self.std_ = None
        self.whitening_matrix_ = None
        
    def _preprocess_images(self, images: np.ndarray, fit: bool = False) -> np.ndarray:
        """Apply preprocessing (normalization, whitening)"""
        
        processed = images.copy()
        
        if fit:
            # Compute statistics
            self.mean_ = np.mean(processed)
            self.std_ = np.std(processed)
            
        # Normalize
        if self.mean_ is not None and self.std_ is not None:
            processed = (processed - self.mean_) / (self.std_ + 1e-8)
            
        if self.whitening and fit:
            # Simple whitening: decorrelate patches
            patches = self._extract_all_patches(processed)
            cov = np.cov(patches.T)
            eigenvals, eigenvecs = np.linalg.eigh(cov)
            # Regularize eigenvalues
            eigenvals = np.maximum(eigenvals, 0.01)
            self.whitening_matrix_ = eigenvecs @ np.diag(1.0 / np.sqrt(eigenvals)) @ eigenvecs.T
            
        return processed
        
    def _extract_all_patches(self, images: np.ndarray) -> np.ndarray:
        """Extract all patches from images with specified overlap"""
        
        if len(images.shape) == 2:
            images = images[np.newaxis, :, :]
            
        patches = []
        patch_h, patch_w = self.patch_size
        step_h = max(1, int(patch_h * (1 - self.overlap_factor)))
        step_w = max(1, int(patch_w * (1 - self.overlap_factor)))
        
        for image in images:
            h, w = image.shape
            for i in range(0, h - patch_h + 1, step_h):
                for j in range(0, w - patch_w + 1, step_w):
                    patch = image[i:i+patch_h, j:j+patch_w]
                    patches.append(patch.flatten())
                    
        return np.array(patches)
        
    def fit(self, images: np.ndarray, max_iterations: int = 1000, verbose: bool = True) -> Dict[str, Any]:
        """
        Fit sparse feature extractor to training images
        
        Args:
            images: Training images (n_images, height, width) or (height, width)
            max_iterations: Maximum dictionary learning iterations
            verbose: Whether to print progress
            
        Returns:
            Training statistics
        """
        
        if verbose:
            print(f"ðŸŽ¯ Fitting Sparse Feature Extractor...")
            
        # Preprocess images
        processed_images = self._preprocess_images(images, fit=True)
        
        # Initialize dictionary learner
        self.dictionary_learner = DictionaryLearner(
            n_components=self.n_components,
            patch_size=self.patch_size,
            sparsity_penalty=self.sparsity_penalty,
            max_iterations=max_iterations
        )
        
        # Learn dictionary
        results = self.dictionary_learner.fit(processed_images, verbose=verbose)
        
        # Initialize sparse coder with learned dictionary
        self.sparse_coder = SparseCoder(
            dictionary=self.dictionary_learner.get_dictionary(),
            sparsity_penalty=self.sparsity_penalty
        )
        
        self.is_fitted = True
        
        if verbose:
            print(f"âœ… Sparse Feature Extractor fitted successfully!")
            
        return results
        
    def transform(self, images: np.ndarray, pooling: str = 'max', 
                 grid_size: Tuple[int, int] = (4, 4)) -> np.ndarray:
        """
        Transform images to sparse feature representation
        
        Args:
            images: Input images
            pooling: Pooling method ('max', 'mean', 'sum')
            grid_size: Spatial pooling grid size
            
        Returns:
            Feature vectors (n_images, n_features)
        """
        
        if not self.is_fitted:
            raise ValueError("Feature extractor must be fitted before transform!")
            
        # Preprocess images
        processed_images = self._preprocess_images(images)
        
        if len(processed_images.shape) == 2:
            processed_images = processed_images[np.newaxis, :, :]
            
        features = []
        
        for image in processed_images:
            # Extract patches and encode
            patches = self._extract_patches_with_positions(image)
            patch_codes = []
            positions = []
            
            for patch, pos in patches:
                code = self.sparse_coder.encode_patch(patch.flatten())
                patch_codes.append(code)
                positions.append(pos)
                
            # Apply spatial pooling
            if len(patch_codes) > 0:
                pooled_features = self._spatial_pooling(
                    patch_codes, positions, image.shape, pooling, grid_size
                )
            else:
                pooled_features = np.zeros(self.n_components * grid_size[0] * grid_size[1])
                
            features.append(pooled_features)
            
        return np.array(features)
        
    def _extract_patches_with_positions(self, image: np.ndarray) -> List[Tuple[np.ndarray, Tuple[int, int]]]:
        """Extract patches with their spatial positions"""
        
        patches_with_pos = []
        patch_h, patch_w = self.patch_size
        step_h = max(1, int(patch_h * (1 - self.overlap_factor)))
        step_w = max(1, int(patch_w * (1 - self.overlap_factor)))
        
        h, w = image.shape
        for i in range(0, h - patch_h + 1, step_h):
            for j in range(0, w - patch_w + 1, step_w):
                patch = image[i:i+patch_h, j:j+patch_w]
                patches_with_pos.append((patch, (i, j)))
                
        return patches_with_pos
        
    def _spatial_pooling(self, codes: List[np.ndarray], positions: List[Tuple[int, int]], 
                        image_shape: Tuple[int, int], pooling: str, 
                        grid_size: Tuple[int, int]) -> np.ndarray:
        """Apply spatial pooling to patch codes"""
        
        h, w = image_shape
        grid_h, grid_w = grid_size
        
        # Create spatial grid
        cell_h = h // grid_h
        cell_w = w // grid_w
        
        # Initialize pooled features
        pooled = np.zeros((grid_h, grid_w, self.n_components))
        counts = np.zeros((grid_h, grid_w))
        
        # Assign codes to grid cells
        for code, (pos_i, pos_j) in zip(codes, positions):
            grid_i = min(pos_i // cell_h, grid_h - 1)
            grid_j = min(pos_j // cell_w, grid_w - 1)
            
            if pooling == 'max':
                pooled[grid_i, grid_j] = np.maximum(pooled[grid_i, grid_j], code)
            elif pooling == 'mean' or pooling == 'sum':
                pooled[grid_i, grid_j] += code
                counts[grid_i, grid_j] += 1
                
        # Finalize pooling
        if pooling == 'mean':
            mask = counts > 0
            pooled[mask] = pooled[mask] / counts[mask, np.newaxis]
            
        return pooled.flatten()
        
    def fit_transform(self, images: np.ndarray, **kwargs) -> np.ndarray:
        """Fit extractor and transform images in one step"""
        self.fit(images, **kwargs)
        return self.transform(images)
        
    def get_feature_images(self) -> np.ndarray:
        """Get learned features as images"""
        if not self.is_fitted:
            raise ValueError("Feature extractor must be fitted first!")
            
        return self.dictionary_learner.get_dictionary_images()
        
    def visualize_features(self, figsize: Tuple[int, int] = (12, 8)):
        """Visualize learned features"""
        if not self.is_fitted:
            raise ValueError("Feature extractor must be fitted first!")
            
        import matplotlib.pyplot as plt
        
        feature_images = self.get_feature_images()
        n_features = len(feature_images)
        
        # Determine grid size
        grid_size = int(np.ceil(np.sqrt(n_features)))
        
        fig, axes = plt.subplots(grid_size, grid_size, figsize=figsize)
        axes = axes.flatten() if n_features > 1 else [axes]
        
        for i in range(n_features):
            ax = axes[i]
            ax.imshow(feature_images[i], cmap='gray')
            ax.set_title(f'Feature {i+1}')
            ax.axis('off')
            
        # Hide unused subplots
        for i in range(n_features, len(axes)):
            axes[i].axis('off')
            
        plt.tight_layout()
        plt.show()
        
        print(f"ðŸ“Š Learned {n_features} sparse features")
        print(f"   Feature size: {self.patch_size}")
        print(f"   Sparsity penalty: {self.sparsity_penalty}")