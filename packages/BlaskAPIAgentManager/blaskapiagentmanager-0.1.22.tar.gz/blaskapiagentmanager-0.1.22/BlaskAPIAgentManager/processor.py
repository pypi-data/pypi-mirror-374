import logging
import json
from typing import Dict, Any, List, Optional, Union
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field


from .utils import (
    filter_json_data,
    sort_slice_json_data,
    calculate_statistics,
    extract_json_data,
)
from .prompts import processor_prompt

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FilterJsonDataParams(BaseModel):
    """Parameters for filter_json_data function."""

    key: str = Field(description="The key to search for at any depth")
    values: List[Any] = Field(
        description="A list of values to match against the key (items matching any value will be included)"
    )


class SortSliceJsonDataParams(BaseModel):
    """Parameters for sort_slice_json_data function."""

    key: str = Field(
        description="The key to sort by (will look for this key at any depth)"
    )
    limit: int = Field(
        default=0,
        description="Maximum number of items to return after sorting (0 means no limit)",
    )
    order: str = Field(
        default="ASC",
        description="Sort order, either 'ASC' (ascending) or 'DESC' (descending)",
    )


class CalculateStatisticsParams(BaseModel):
    """Parameters for calculate_statistics function."""

    keys: List[str] = Field(
        description="List of keys to calculate statistics for (e.g., ['ggr', 'ftd', 'value'])"
    )
    operations: List[str] = Field(
        description="List of operations to perform ['sum', 'min', 'max', 'avg', 'delta']"
    )


class ExtractJsonDataParams(BaseModel):
    """Parameters for extract_json_data function."""

    keys: List[str] = Field(
        description="List of keys to extract (can use dot notation for nested keys, e.g., 'parent.child')"
    )


class ProcessorFunction(BaseModel):
    """Model for a processing function call."""

    function: str = Field(description="Name of the function to apply")
    parameters: Union[
        FilterJsonDataParams,
        SortSliceJsonDataParams,
        CalculateStatisticsParams,
        ExtractJsonDataParams,
    ] = Field(description="Parameters for the function")


class ProcessorPlan(BaseModel):
    """Model for a sequence of processing function calls."""

    function_calls: List[ProcessorFunction]


class ProcessorTool:
    """Tool for processing API responses using utility functions.

    This class implements a two-stage processing approach:
    1. Planning Stage: Analyze a sample/structure of the API response to determine
       what processing steps are needed (get_processing_plan)
    2. Execution Stage: Apply those processing steps to the full API response data
       (execute_processing_plan)

    This approach allows for more efficient processing of large API responses by first
    determining the processing strategy on a smaller sample.

    For convenience, the process_api_response method combines both stages, but when
    you need to use different data for planning vs. execution, it's better to call
    the two methods separately.
    """

    def __init__(self, llm=None):
        """Initialize the ProcessorTool.

        Args:
            llm: Language model for generating function call plans
        """
        self.llm = llm
        self.output_parser = JsonOutputParser(pydantic_object=ProcessorPlan)
        self.function_map = {
            "filter_json_data": filter_json_data,
            "sort_slice_json_data": sort_slice_json_data,
            "calculate_statistics": calculate_statistics,
            "extract_json_data": extract_json_data,
        }

        self.param_type_map = {
            "filter_json_data": FilterJsonDataParams,
            "sort_slice_json_data": SortSliceJsonDataParams,
            "calculate_statistics": CalculateStatisticsParams,
            "extract_json_data": ExtractJsonDataParams,
        }

    def get_processing_plan(
        self,
        query: str,
        api_parameters: Dict[str, Any],
        api_response: Dict[str, Any],
        reason: str = "",
        dependencies: str = "",
        explanation: str = "",
        previous_processed_results: str = "{}",
        country_ids: Optional[Dict[str, int]] = None,
    ) -> List[ProcessorFunction]:
        """Generate a processing plan based on API parameters and response structure.

        This is the first step in the two-stage processing approach. It analyzes the
        API response structure (typically a sample or single unit from the full response)
        to determine what processing steps should be applied.

        The plan generated by this method is then executed on the full API response data
        using the execute_processing_plan method.

        Args:
            query: The query used to generate the processing plan
            api_parameters: Parameters used in the API call
            api_response: API response structure example (typically a sample of the full response)
            reason: The reason for this API call from the planner
            dependencies: Dependencies for this API call from the planner
            explanation: Explanation for this API call from the planner
            previous_processed_results: JSON string of previously processed results
            country_ids: Country IDs mapping from the country extractor

        Returns:
            List of ProcessorFunction objects representing the processing plan
        """
        try:
            chain = processor_prompt | self.llm | self.output_parser

            chain_input = {
                "query": query,
                "api_parameters": json.dumps(api_parameters, indent=2),
                "api_response": json.dumps(api_response, indent=2),
                "reason": reason,
                "dependencies": dependencies,
                "explanation": explanation,
                "previous_processed_results": previous_processed_results,
                "country_ids": json.dumps(country_ids or {}, indent=2),
            }

            parsed_output = chain.invoke(chain_input)

            if isinstance(parsed_output, dict):
                try:
                    plan = ProcessorPlan(**parsed_output)
                except Exception as validation_error:
                    logger.error(
                        f"Failed to validate processing plan: {validation_error}"
                    )
                    logger.error(f"Invalid plan structure received: {parsed_output}")
                    return []
            elif isinstance(parsed_output, ProcessorPlan):
                plan = parsed_output
            else:
                logger.error(
                    f"Unexpected output type from parser: {type(parsed_output)}"
                )
                logger.error(f"Output content: {parsed_output}")
                return []

            logger.info(
                f"Generated processing plan with {len(plan.function_calls)} steps"
            )
            return plan.function_calls
        except Exception as e:
            logger.error(f"Error generating processing plan: {str(e)}")
            return []

    def execute_processing_plan(
        self, processing_plan: List[ProcessorFunction], api_response: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute the processing plan on the API response.

        This is the second step in the two-stage processing approach. It takes the
        processing plan generated by get_processing_plan and applies it to the full
        API response data.

        Args:
            processing_plan: List of ProcessorFunction objects generated by get_processing_plan
            api_response: Full API response data to process

        Returns:
            Processed API response with execution trace
        """
        if not processing_plan:
            logger.info("No processing steps to execute, returning original response")
            return {"processed_data": api_response}

        # Validate processing plan for common issues
        # self._validate_processing_plan(processing_plan)

        processed_data = api_response

        for step in processing_plan:
            function_name = step.function
            parameters = (
                step.parameters.model_dump()
                if hasattr(step.parameters, "model_dump")
                else step.parameters
            )

            logger.info(f"Executing processing step: {function_name}")

            if function_name not in self.function_map:
                logger.warning(f"Unknown function: {function_name}, skipping")
                continue

            function = self.function_map[function_name]

            if isinstance(parameters, dict):
                execution_params = parameters.copy()
            else:
                execution_params = (
                    parameters.model_dump()
                    if hasattr(parameters, "model_dump")
                    else dict(parameters)
                )
            execution_params["data"] = processed_data

            try:
                processed_data = function(**execution_params)
                if processed_data is None:
                    logger.warning(
                        f"Function {function_name} returned None. Input data: {type(execution_params.get('data', 'N/A'))}, Parameters: {parameters}"
                    )
                    # Log available keys for debugging
                    input_data = execution_params.get("data", None)
                    if isinstance(input_data, dict):
                        logger.warning(
                            f"Available keys in input data: {list(input_data.keys())}"
                        )
                    elif (
                        isinstance(input_data, list)
                        and input_data
                        and isinstance(input_data[0], dict)
                    ):
                        logger.warning(
                            f"Available keys in first list item: {list(input_data[0].keys())}"
                        )
                    # Stop further processing if a step returns None
                    break
            except Exception as e:
                logger.error(f"Error executing {function_name}: {str(e)}")
                logger.error(f"Parameters: {parameters}")
                logger.error(
                    f"Input data type: {type(execution_params.get('data', 'N/A'))}"
                )

        return {"processed_data": processed_data}

    def process_api_response(
        self,
        query: str,
        api_parameters: Dict[str, Any],
        api_response: Dict[str, Any],
        reason: str = "",
        dependencies: str = "",
        explanation: str = "",
        previous_processed_results: str = "{}",
        country_ids: Optional[Dict[str, int]] = None,
    ) -> Dict[str, Any]:
        """Process an API response based on API parameters and planner information.

        This is a convenience method that combines both steps of the two-stage processing approach:
        1. Generates a processing plan using get_processing_plan
        2. Executes the processing plan on the same API response using execute_processing_plan

        Note: When you need to use a structure sample for planning but process the full result,
        it's better to call get_processing_plan and execute_processing_plan separately.

        Args:
            query: The original user query
            api_parameters: Parameters used in the API call
            api_response: API response to process (used for both planning and execution)
            reason: The reason for this API call from the planner
            dependencies: Dependencies for this API call from the planner
            explanation: Explanation for this API call from the planner
            previous_processed_results: JSON string of previously processed results
            country_ids: Country IDs mapping from the country extractor

        Returns:
            Processed API response with execution trace
        """
        processing_plan = self.get_processing_plan(
            query,
            api_parameters,
            api_response,
            reason,
            dependencies,
            explanation,
            previous_processed_results,
            country_ids,
        )
        processed_result = self.execute_processing_plan(processing_plan, api_response)

        return processed_result

    # def _validate_processing_plan(
    #     self, processing_plan: List[ProcessorFunction]
    # ) -> None:
    #     """Validate the processing plan for common issues that lead to null results."""

    #     # Check if we have extraction followed by filtering
    #     extract_keys = set()
    #     filter_keys = set()

    #     for step in processing_plan:
    #         if step.function == "extract_json_data":
    #             if hasattr(step.parameters, "keys"):
    #                 extract_keys.update(step.parameters.keys)
    #             elif isinstance(step.parameters, dict) and "keys" in step.parameters:
    #                 extract_keys.update(step.parameters["keys"])

    #         elif step.function == "filter_json_data":
    #             if hasattr(step.parameters, "key"):
    #                 filter_keys.add(step.parameters.key)
    #             elif isinstance(step.parameters, dict) and "key" in step.parameters:
    #                 filter_keys.add(step.parameters["key"])

    #     # Check for potential issues
    #     missing_keys = filter_keys - extract_keys
    #     if missing_keys:
    #         logger.warning(
    #             f"Processing plan validation: Filtering by keys {missing_keys} "
    #             f"but these keys are not extracted. This will likely cause null results."
    #         )
    #         logger.warning(f"Extracted keys: {extract_keys}")
    #         logger.warning(f"Filter keys: {filter_keys}")

    #     # Additional validation for calculate_statistics
    #     for step in processing_plan:
    #         if step.function == "calculate_statistics":
    #             if hasattr(step.parameters, "keys"):
    #                 stat_keys = step.parameters.keys
    #             elif isinstance(step.parameters, dict) and "keys" in step.parameters:
    #                 stat_keys = step.parameters["keys"]
    #             else:
    #                 continue

    #             missing_stat_keys = set(stat_keys) - extract_keys
    #             if missing_stat_keys:
    #                 logger.warning(
    #                     f"Processing plan validation: Calculating statistics for keys {missing_stat_keys} "
    #                     f"but these keys are not extracted."
    #                 )
