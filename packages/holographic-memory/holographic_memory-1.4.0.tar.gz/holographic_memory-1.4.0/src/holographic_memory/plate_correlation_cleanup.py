"""
📋 Plate Correlation Cleanup
=============================

🎯 ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

🧪 Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

"""
"""
🧠 Holographic Memory Cleanup Implementation
===================================================

Author: Benedict Chen (benedict@benedictchen.com)
Based on: Plate (1995) "Holographic Reduced Representations"

🔬 RESEARCH FOUNDATION:
======================
This implements ALL solutions from research analysis, providing
research-accurate cleanup mechanisms for Holographic Reduced Representations.

📚 **Background**:
Plate's HRR theory requires specific cleanup mechanisms for practical applications:
- Auto-associative memories can store approximately n/(4 log n) patterns
- Hetero-associative memories can store approximately n/(2 log n) patterns  
- Cleanup uses correlation-based matching: similarity = (p · q) / (||p|| ||q||)
- Iterative cleanup can improve accuracy but risks oscillation without damping

⚡ **Algorithm Overview**:
```
Correlation-Based Cleanup (Section IV):
┌─────────────────────────────────────┐
│ 1. Build prototype library P       │
│ 2. For query q, compute:            │
│    similarity = max_p (p · q)       │
│ 3. If similarity > threshold:       │
│    return best_prototype            │
│ 4. Else: blend query with prototype │
│    result = α·p + (1-α)·q           │
└─────────────────────────────────────┘

Iterative Cleanup with Convergence:
┌─────────────────────────────────────┐
│ 1. Start with noisy input vector    │
│ 2. Apply cleanup step               │
│ 3. Check convergence criteria:      │
│    - Energy function decreasing     │
│    - Change magnitude < threshold   │
│    - No oscillation detected        │
│ 4. Apply damping: new = α·clean +   │
│    (1-α)·current                   │
│ 5. Repeat until converged           │
└─────────────────────────────────────┘
```

🚀 **Key Research Contributions**:
- Implements Plate's correlation-based cleanup (Section IV)
- Adds convergence-guaranteed iterative cleanup (Section IV) 
- Provides capacity-aware storage bounds (Section IX)
- Includes SNR-based noise tolerance (Section VIII)
- Supports graceful degradation strategies (Section VIII)
- Implements proper hetero-associative retrieval (Section II)
"""

import numpy as np
from typing import Dict, Any, List, Optional, Union, Tuple, NamedTuple
from dataclasses import dataclass
from abc import ABC, abstractmethod
import logging
from collections import deque, defaultdict
import warnings

from .holographic_cleanup_config import (
    HolographicCleanupConfig, CleanupMethod, AssociativeMemoryType,
    ConvergenceStrategy, NoiseToleranceStrategy,
    create_plate_1995_config, create_legacy_compatible_config
)


class CleanupResult(NamedTuple):
    """Result from cleanup operation with metadata"""
    cleaned_vector: np.ndarray
    confidence: float
    method_used: str
    iterations_used: int
    converged: bool
    diagnostics: Dict[str, Any]


class CapacityInfo(NamedTuple):
    """Capacity analysis for associative memory"""
    auto_associative_capacity: int
    hetero_associative_capacity: int
    current_auto_patterns: int
    current_hetero_patterns: int
    utilization_auto: float
    utilization_hetero: float
    at_capacity: bool
    warnings: List[str]


@dataclass
class MemoryTrace:
    """Enhanced memory trace with capacity and access tracking"""
    key_vector: np.ndarray
    value_vector: Optional[np.ndarray] = None  # None for auto-associative
    trace_strength: float = 1.0
    access_count: int = 0
    last_accessed: float = 0.0
    is_auto_associative: bool = True
    
    def update_access(self, current_time: float, decay_factor: float = 0.99):
        """Update access statistics with trace decay"""
        self.access_count += 1
        self.last_accessed = current_time
        self.trace_strength *= decay_factor


class PlateCleanupSystem:
    """
    Cleanup system for Holographic Reduced Representations (Plate 1995).
    
    Implements associative memory cleanup using correlation-based retrieval
    and trace decay mechanisms.
    """
    
    def __init__(self, vector_dim: int, config: Optional[HolographicCleanupConfig] = None):
        self.vector_dim = vector_dim
        self.config = config or create_plate_1995_config()
        
        # Validate configuration
        validation = self.config.validate_config()
        if not validation['valid']:
            raise ValueError(f"Invalid configuration: {validation['issues']}")
        
        if validation['warnings'] and self.config.legacy_compatibility_warnings:
            for warning in validation['warnings']:
                warnings.warn(f"HolographicCleanup: {warning}")
        
        # Initialize components based on configuration
        self._init_cleanup_components()
        self._init_capacity_monitoring()
        self._init_convergence_tracking()
    
    def _init_cleanup_components(self):
        """Initialize cleanup components based on configuration"""
        # Correlation-based cleanup (Plate Section IV)
        self.cleanup_prototypes: List[np.ndarray] = []
        self.prototype_labels: List[str] = []
        
        # Weight-matrix cleanup (legacy compatibility)  
        self.cleanup_weights: Optional[np.ndarray] = None
        
        # Hopfield network cleanup
        self.hopfield_weights: Optional[np.ndarray] = None
        
        # Memory traces for capacity monitoring
        self.memory_traces: Dict[str, MemoryTrace] = {}
        
        # Performance tracking
        self.cleanup_stats = {
            'total_cleanups': 0,
            'successful_cleanups': 0,
            'convergence_failures': 0,
            'method_usage': defaultdict(int)
        }
    
    def _init_capacity_monitoring(self):
        """Initialize capacity monitoring based on Plate Section IX"""
        if self.config.capacity_monitoring_enabled:
            # Capacity bounds from Plate (1995) Section IX
            log_n = np.log(self.vector_dim) if self.vector_dim > 1 else 1.0
            
            self.auto_capacity = int(self.config.capacity_safety_factor * 
                                   self.vector_dim / (4 * log_n))
            self.hetero_capacity = int(self.config.capacity_safety_factor * 
                                     self.vector_dim / (2 * log_n))
            
            self.capacity_warnings_issued = set()
    
    def _init_convergence_tracking(self):
        """Initialize convergence tracking components"""
        if self.config.convergence_history_tracking:
            self.convergence_history = []
            
        if self.config.energy_monitoring_enabled:
            self.energy_history = []
    
    # ═══════════════════════════════════════════════════════════════════════════
    # CORRELATION-BASED CLEANUP (Plate Section IV Implementation)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def build_correlation_cleanup_memory(self, prototype_vectors: List[np.ndarray], 
                                       labels: Optional[List[str]] = None):
        """
        Build correlation-based cleanup memory following Plate (1995) Section IV
        
        🔬 Research basis: Section IV "Cleanup", page 628
        "cleanup memory stores a set of prototype vectors"
        """
        if labels is None:
            labels = [f"prototype_{i}" for i in range(len(prototype_vectors))]
            
        if len(prototype_vectors) != len(labels):
            raise ValueError("Number of prototypes must match number of labels")
        
        self.cleanup_prototypes = []
        self.prototype_labels = labels.copy()
        
        for i, prototype in enumerate(prototype_vectors):
            if len(prototype) != self.vector_dim:
                raise ValueError(f"Prototype {i} has wrong dimensions: {len(prototype)} != {self.vector_dim}")
            
            # Ensure prototype follows N(0, 1/n) distribution as recommended
            if self.config.correlation_normalization:
                prototype_normalized = self._normalize_for_hrr(prototype)
                self.cleanup_prototypes.append(prototype_normalized)
            else:
                self.cleanup_prototypes.append(prototype.copy())
        
        # Removed print spam: f"...} prototypes")
    
    def correlation_cleanup(self, query: np.ndarray, 
                          confidence_threshold: Optional[float] = None) -> CleanupResult:
        """
        Perform correlation-based cleanup following Plate (1995) Section IV
        
        🔬 Implementation of exact algorithm from research paper:
        1. Compute correlation with each prototype: correlation = (p · q) / (||p|| ||q||)
        2. Find maximum correlation
        3. If above threshold, return prototype
        4. Otherwise, blend query with best prototype
        """
        if len(self.cleanup_prototypes) == 0:
            return CleanupResult(
                cleaned_vector=query,
                confidence=0.0,
                method_used="no_prototypes",
                iterations_used=0,
                converged=False,
                diagnostics={'error': 'No prototypes in cleanup memory'}
            )
        
        threshold = confidence_threshold or self.config.correlation_confidence_threshold
        
        # Normalize query for proper correlation calculation
        query_norm = self._normalize_for_correlation(query)
        
        best_correlation = -1.0
        best_prototype_idx = -1
        best_similarity = 0.0
        correlations = []
        
        # Compute correlation with each prototype
        for i, prototype in enumerate(self.cleanup_prototypes):
            prototype_norm = self._normalize_for_correlation(prototype)
            
            # Correlation = normalized dot product (Plate's method)
            correlation = np.dot(query_norm, prototype_norm)
            correlations.append(correlation)
            
            if correlation > best_correlation:
                best_correlation = correlation
                best_prototype_idx = i
                best_similarity = correlation
        
        diagnostics = {
            'all_correlations': correlations,
            'best_prototype_idx': best_prototype_idx,
            'best_prototype_label': self.prototype_labels[best_prototype_idx] if best_prototype_idx >= 0 else None,
            'threshold_used': threshold,
            'normalization_applied': self.config.correlation_normalization
        }
        
        # Apply cleanup based on confidence
        if best_similarity > threshold:
            # High confidence: return clean prototype
            cleaned_vector = self.cleanup_prototypes[best_prototype_idx].copy()
            method_used = "correlation_direct"
            converged = True
        else:
            # Low confidence: blend query with best prototype
            if self.config.correlation_fallback_enabled:
                blend_weight = self._compute_blend_weight(best_similarity)
                best_prototype = self.cleanup_prototypes[best_prototype_idx]
                cleaned_vector = (blend_weight * best_prototype + 
                                (1 - blend_weight) * query)
                method_used = "correlation_blended"
                converged = False
                diagnostics['blend_weight'] = blend_weight
            else:
                # No fallback: return original query
                cleaned_vector = query.copy()
                method_used = "correlation_failed"
                converged = False
        
        # Update statistics
        self.cleanup_stats['total_cleanups'] += 1
        self.cleanup_stats['method_usage'][method_used] += 1
        if converged:
            self.cleanup_stats['successful_cleanups'] += 1
        
        return CleanupResult(
            cleaned_vector=cleaned_vector,
            confidence=best_similarity,
            method_used=method_used,
            iterations_used=1,
            converged=converged,
            diagnostics=diagnostics
        )
    
    # ═══════════════════════════════════════════════════════════════════════════
    # ITERATIVE CLEANUP WITH CONVERGENCE GUARANTEES
    # ═══════════════════════════════════════════════════════════════════════════
    
    def iterative_cleanup_with_convergence(self, vector: np.ndarray) -> CleanupResult:
        """
        Iterative cleanup with proper convergence guarantees
        
        🔬 Based on Plate (1995) Section IV, page 629:
        "cleanup can be applied iteratively" with oscillation prevention
        """
        states_history = []
        energy_history = []
        current_vector = vector.copy()
        
        convergence_info = {
            'converged': False,
            'oscillation_detected': False,
            'final_energy': 0.0,
            'iterations_used': 0,
            'energy_trajectory': [],
            'convergence_metric': 'change_magnitude'
        }
        
        for iteration in range(self.config.max_cleanup_iterations):
            # Store state for oscillation detection
            if self.config.oscillation_detection_enabled:
                state_key = tuple(np.round(current_vector, decimals=8))
                if state_key in states_history:
                    convergence_info['oscillation_detected'] = True
                    print(f"⚠️ Oscillation detected at iteration {iteration}")
                    break
                states_history.append(state_key)
            
            # Compute energy for monitoring
            if self.config.energy_monitoring_enabled:
                energy_before = self._compute_hopfield_energy(current_vector)
                energy_history.append(energy_before)
            
            # Single cleanup step
            cleanup_result = self.correlation_cleanup(current_vector)
            cleaned_vector = cleanup_result.cleaned_vector
            
            # Apply convergence strategy
            if self.config.convergence_strategy == ConvergenceStrategy.DAMPED_UPDATE:
                # Damped update to prevent oscillations
                damped_vector = (self.config.damping_factor * cleaned_vector + 
                               (1 - self.config.damping_factor) * current_vector)
                
                # Check for convergence
                change_magnitude = np.linalg.norm(damped_vector - current_vector)
                if change_magnitude < self.config.convergence_threshold:
                    convergence_info['converged'] = True
                    current_vector = damped_vector
                    break
                
                current_vector = damped_vector
                
            elif self.config.convergence_strategy == ConvergenceStrategy.ENERGY_BASED:
                # Energy-based convergence
                if self.config.energy_monitoring_enabled and len(energy_history) > 0:
                    energy_after = self._compute_hopfield_energy(cleaned_vector)
                    if energy_after > energy_history[-1] + self.config.convergence_threshold:
                        print(f"⚠️ Energy increase detected: {energy_history[-1]:.6f} -> {energy_after:.6f}")
                        break  # Energy increased, likely diverging
                
                current_vector = cleaned_vector
                
            elif self.config.convergence_strategy == ConvergenceStrategy.PROGRESSIVE_RELAXATION:
                # Progressive threshold relaxation
                current_vector = self._apply_progressive_relaxation(current_vector, iteration)
            
            convergence_info['iterations_used'] = iteration + 1
            
            # Track convergence history
            if self.config.convergence_history_tracking:
                self.convergence_history.append({
                    'iteration': iteration,
                    'change_magnitude': np.linalg.norm(current_vector - vector),
                    'energy': energy_history[-1] if energy_history else 0.0,
                    'confidence': cleanup_result.confidence
                })
        
        # Final energy computation
        if self.config.energy_monitoring_enabled:
            convergence_info['final_energy'] = self._compute_hopfield_energy(current_vector)
            convergence_info['energy_trajectory'] = energy_history
        
        return CleanupResult(
            cleaned_vector=current_vector,
            confidence=cleanup_result.confidence if 'cleanup_result' in locals() else 0.0,
            method_used="iterative_convergence",
            iterations_used=convergence_info['iterations_used'],
            converged=convergence_info['converged'],
            diagnostics=convergence_info
        )
    
    # ═══════════════════════════════════════════════════════════════════════════
    # CAPACITY-AWARE STORAGE (Section IX Implementation)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def add_memory_pattern(self, key: str, key_vector: np.ndarray, 
                         value_vector: Optional[np.ndarray] = None, 
                         force_storage: bool = False) -> bool:
        """
        Add memory pattern with capacity awareness
        
        🔬 Based on Plate (1995) Section IX capacity bounds:
        - Auto-associative: C ≈ n/(4 log n) 
        - Hetero-associative: C ≈ n/(2 log n)
        """
        is_auto = value_vector is None
        
        if self.config.capacity_monitoring_enabled and not force_storage:
            capacity_info = self.check_associative_capacity()
            
            if is_auto and capacity_info.current_auto_patterns >= capacity_info.auto_associative_capacity:
                if self.config.selective_forgetting_enabled:
                    self._selective_forget(AssociativeMemoryType.AUTO_ASSOCIATIVE)
                else:
                    warnings.warn(f"Auto-associative capacity exceeded: {capacity_info.current_auto_patterns}/{capacity_info.auto_associative_capacity}")
                    return False
            
            if not is_auto and capacity_info.current_hetero_patterns >= capacity_info.hetero_associative_capacity:
                if self.config.selective_forgetting_enabled:
                    self._selective_forget(AssociativeMemoryType.HETERO_ASSOCIATIVE)
                else:
                    warnings.warn(f"Hetero-associative capacity exceeded: {capacity_info.current_hetero_patterns}/{capacity_info.hetero_associative_capacity}")
                    return False
        
        # Store memory trace
        trace = MemoryTrace(
            key_vector=key_vector.copy(),
            value_vector=value_vector.copy() if value_vector is not None else None,
            is_auto_associative=is_auto,
            trace_strength=1.0
        )
        
        self.memory_traces[key] = trace
        return True
    
    def check_associative_capacity(self) -> CapacityInfo:
        """
        Check current capacity utilization
        
        🔬 Research basis: Plate (1995) Section IX "Capacity", page 642
        """
        auto_patterns = [t for t in self.memory_traces.values() if t.is_auto_associative]
        hetero_patterns = [t for t in self.memory_traces.values() if not t.is_auto_associative]
        
        current_auto = len(auto_patterns)
        current_hetero = len(hetero_patterns)
        
        utilization_auto = current_auto / self.auto_capacity if self.auto_capacity > 0 else 0.0
        utilization_hetero = current_hetero / self.hetero_capacity if self.hetero_capacity > 0 else 0.0
        
        warnings_list = []
        at_capacity = False
        
        if utilization_auto > self.config.capacity_warning_threshold:
            warnings_list.append(f"Auto-associative utilization: {utilization_auto:.1%}")
            at_capacity = True
            
        if utilization_hetero > self.config.capacity_warning_threshold:
            warnings_list.append(f"Hetero-associative utilization: {utilization_hetero:.1%}")
            at_capacity = True
        
        return CapacityInfo(
            auto_associative_capacity=self.auto_capacity,
            hetero_associative_capacity=self.hetero_capacity,
            current_auto_patterns=current_auto,
            current_hetero_patterns=current_hetero,
            utilization_auto=utilization_auto,
            utilization_hetero=utilization_hetero,
            at_capacity=at_capacity,
            warnings=warnings_list
        )
    
    # ═══════════════════════════════════════════════════════════════════════════
    # SNR-BASED NOISE TOLERANCE (Section VIII Implementation)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def compute_snr_threshold(self, signal_power: float, noise_variance: float) -> float:
        """
        Compute SNR-based noise tolerance threshold
        
        🔬 Based on Plate (1995) Section VIII "Noisy Conditions", page 638
        Threshold should scale with √(S/N) where S=signal, N=noise
        """
        if noise_variance <= 0:
            return self.config.base_noise_threshold
        
        snr = signal_power / noise_variance
        
        if self.config.noise_tolerance_strategy == NoiseToleranceStrategy.SNR_ADAPTIVE:
            # √(S/N) scaling as recommended by Plate
            snr_threshold = self.config.base_noise_threshold * np.sqrt(snr) * self.config.snr_scaling_factor
            
        elif self.config.noise_tolerance_strategy == NoiseToleranceStrategy.DIMENSIONALITY_SCALED:
            # Dimensionality scaling: higher dimensions → better noise tolerance
            dimensionality_factor = 1.0 / np.sqrt(self.vector_dim) if self.vector_dim > 1 else 1.0
            snr_threshold = self.config.base_noise_threshold * dimensionality_factor
            
        elif self.config.noise_tolerance_strategy == NoiseToleranceStrategy.CAPACITY_AWARE:
            # Load-dependent thresholds
            capacity_info = self.check_associative_capacity()
            load_factor = max(capacity_info.utilization_auto, capacity_info.utilization_hetero)
            snr_threshold = self.config.base_noise_threshold * (1.0 + load_factor)
            
        else:
            # Fixed threshold
            snr_threshold = self.config.base_noise_threshold
        
        return min(snr_threshold, 1.0)  # Cap at 1.0
    
    # ═══════════════════════════════════════════════════════════════════════════
    # GRACEFUL DEGRADATION (Section VIII Implementation)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def graceful_cleanup(self, query: np.ndarray) -> CleanupResult:
        """
        Graceful cleanup with progressive relaxation
        
        🔬 Based on Plate (1995) Section VIII "Noisy Conditions", page 640
        Implements fallback strategies for cleanup failure
        """
        if not self.config.graceful_degradation_enabled:
            return self.correlation_cleanup(query)
        
        # Progressive relaxation through threshold levels
        for i, relaxation_factor in enumerate(self.config.relaxation_steps):
            adjusted_threshold = self.config.correlation_confidence_threshold * relaxation_factor
            
            result = self.correlation_cleanup(query, confidence_threshold=adjusted_threshold)
            
            if result.converged or i == len(self.config.relaxation_steps) - 1:
                # Apply confidence penalty for relaxed thresholds
                confidence_penalty = relaxation_factor if relaxation_factor < 1.0 else 1.0
                
                diagnostics = result.diagnostics.copy()
                diagnostics['relaxation_applied'] = True
                diagnostics['relaxation_factor'] = relaxation_factor
                diagnostics['original_confidence'] = result.confidence
                diagnostics['penalized_confidence'] = result.confidence * confidence_penalty
                
                return CleanupResult(
                    cleaned_vector=result.cleaned_vector,
                    confidence=result.confidence * confidence_penalty,
                    method_used="graceful_degradation",
                    iterations_used=i + 1,
                    converged=result.converged,
                    diagnostics=diagnostics
                )
        
        # Final fallback: return original query with uncertainty
        return CleanupResult(
            cleaned_vector=query,
            confidence=0.0,
            method_used="no_cleanup_possible", 
            iterations_used=len(self.config.relaxation_steps),
            converged=False,
            diagnostics={'status': 'complete_failure', 'uncertainty': 1.0}
        )
    
    # ═══════════════════════════════════════════════════════════════════════════
    # HETERO-ASSOCIATIVE RETRIEVAL (Section II Implementation)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def hetero_associative_retrieval(self, key_vector: np.ndarray) -> CleanupResult:
        """
        Proper hetero-associative retrieval using circular correlation
        
        🔬 Based on Plate (1995) Section II "Basic Operations", page 624
        Uses circular correlation: v = M ⊛ k (Equation 4)
        """
        if not self.config.circular_correlation_enabled:
            # Fallback to legacy matrix multiplication 
            return self._legacy_hetero_retrieval(key_vector)
        
        best_similarity = -1.0
        best_value = None
        best_key = None
        
        # Find best matching key using circular correlation
        for trace_key, trace in self.memory_traces.items():
            if trace.is_auto_associative:
                continue  # Skip auto-associative patterns
            
            # Circular correlation for key matching
            similarity = self._circular_correlation_similarity(key_vector, trace.key_vector)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_value = trace.value_vector
                best_key = trace_key
                
            # Update access statistics
            trace.update_access(self.cleanup_stats['total_cleanups'])
        
        if best_value is None:
            return CleanupResult(
                cleaned_vector=np.zeros_like(key_vector),
                confidence=0.0,
                method_used="hetero_no_match",
                iterations_used=1,
                converged=False,
                diagnostics={'error': 'No hetero-associative patterns found'}
            )
        
        return CleanupResult(
            cleaned_vector=best_value,
            confidence=best_similarity,
            method_used="circular_correlation",
            iterations_used=1,
            converged=best_similarity > self.config.correlation_confidence_threshold,
            diagnostics={
                'matched_key': best_key,
                'retrieval_method': 'circular_correlation',
                'key_similarity': best_similarity
            }
        )
    
    # ═══════════════════════════════════════════════════════════════════════════
    # MASTER CLEANUP METHOD WITH ALL OPTIONS
    # ═══════════════════════════════════════════════════════════════════════════
    
    def cleanup(self, vector: np.ndarray, method: Optional[CleanupMethod] = None) -> CleanupResult:
        """
        Master cleanup method that routes to appropriate implementation
        
        🎯 USER CONFIGURATION CONTROL:
        - Users can override method on per-call basis
        - Falls back to configured default method
        - Provides full backward compatibility with legacy systems
        """
        cleanup_method = method or self.config.cleanup_method
        
        try:
            if cleanup_method == CleanupMethod.CORRELATION_BASED:
                if self.config.graceful_degradation_enabled:
                    return self.graceful_cleanup(vector)
                else:
                    return self.correlation_cleanup(vector)
                    
            elif cleanup_method == CleanupMethod.ITERATIVE_HYBRID:
                return self.iterative_cleanup_with_convergence(vector)
                
            elif cleanup_method == CleanupMethod.HOPFIELD_NETWORK:
                return self._hopfield_cleanup(vector)
                
            elif cleanup_method == CleanupMethod.WEIGHT_MATRIX:
                return self._legacy_weight_matrix_cleanup(vector)
                
            elif cleanup_method == CleanupMethod.ADAPTIVE_THRESHOLD:
                return self._adaptive_threshold_cleanup(vector)
                
            else:
                raise ValueError(f"Unknown cleanup method: {cleanup_method}")
                
        except Exception as e:
            if self.config.fallback_to_legacy:
                warnings.warn(f"Cleanup method {cleanup_method} failed: {e}. Falling back to legacy method.")
                return self._legacy_weight_matrix_cleanup(vector)
            else:
                raise
    
    # ═══════════════════════════════════════════════════════════════════════════
    # HELPER METHODS AND UTILITIES
    # ═══════════════════════════════════════════════════════════════════════════
    
    def _normalize_for_hrr(self, vector: np.ndarray) -> np.ndarray:
        """Normalize vector for HRR following N(0, 1/n) distribution"""
        n = len(vector)
        return vector / np.sqrt(np.sum(vector**2) / n + 1e-10)
    
    def _normalize_for_correlation(self, vector: np.ndarray) -> np.ndarray:
        """Normalize vector for correlation computation"""
        norm = np.linalg.norm(vector)
        return vector / (norm + 1e-10)
    
    def _compute_blend_weight(self, similarity: float) -> float:
        """Compute blending weight based on similarity"""
        if self.config.correlation_blend_weight_strategy == "similarity_weighted":
            return similarity
        elif self.config.correlation_blend_weight_strategy == "fixed":
            return 0.5
        elif self.config.correlation_blend_weight_strategy == "adaptive":
            # Adaptive based on current noise level
            return min(0.9, similarity + 0.2)
        else:
            return similarity
    
    def _compute_hopfield_energy(self, vector: np.ndarray) -> float:
        """Compute Hopfield-style energy function"""
        if len(self.cleanup_prototypes) == 0:
            return 0.0
        
        total_energy = 0.0
        for prototype in self.cleanup_prototypes:
            overlap = np.dot(vector, prototype)
            total_energy -= overlap ** 2
        
        return total_energy
    
    def _circular_correlation_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Compute circular correlation similarity"""
        # Placeholder for circular correlation - would need full FFT implementation
        # For now, use normalized dot product as approximation
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return np.dot(vec1, vec2) / (norm1 * norm2)
    
    def _selective_forget(self, memory_type: AssociativeMemoryType):
        """Implement selective forgetting based on configured strategy"""
        if self.config.forgetting_strategy == "least_recently_used":
            self._forget_lru(memory_type)
        elif self.config.forgetting_strategy == "weakest_trace":
            self._forget_weakest(memory_type)
        elif self.config.forgetting_strategy == "oldest":
            self._forget_oldest(memory_type)
    
    def _forget_lru(self, memory_type: AssociativeMemoryType):
        """Forget least recently used pattern"""
        candidates = [
            (key, trace) for key, trace in self.memory_traces.items()
            if (memory_type == AssociativeMemoryType.AUTO_ASSOCIATIVE and trace.is_auto_associative) or
               (memory_type == AssociativeMemoryType.HETERO_ASSOCIATIVE and not trace.is_auto_associative)
        ]
        
        if candidates:
            # Find least recently used
            lru_key = min(candidates, key=lambda x: x[1].last_accessed)[0]
            del self.memory_traces[lru_key]
            print(f"🗑️ Forgot LRU pattern: {lru_key}")
    
    def _forget_weakest(self, memory_type: AssociativeMemoryType):
        """Forget weakest trace strength pattern"""
        candidates = [
            (key, trace) for key, trace in self.memory_traces.items()
            if (memory_type == AssociativeMemoryType.AUTO_ASSOCIATIVE and trace.is_auto_associative) or
               (memory_type == AssociativeMemoryType.HETERO_ASSOCIATIVE and not trace.is_auto_associative)
        ]
        
        if candidates:
            weakest_key = min(candidates, key=lambda x: x[1].trace_strength)[0]
            del self.memory_traces[weakest_key]
            print(f"🗑️ Forgot weakest pattern: {weakest_key}")
    
    def _forget_oldest(self, memory_type: AssociativeMemoryType):
        """Forget oldest pattern using FIFO memory management"""
        # FIFO forgetting strategy - remove chronologically first stored pattern
        for key, trace in self.memory_traces.items():
            if ((memory_type == AssociativeMemoryType.AUTO_ASSOCIATIVE and trace.is_auto_associative) or
                (memory_type == AssociativeMemoryType.HETERO_ASSOCIATIVE and not trace.is_auto_associative)):
                del self.memory_traces[key]
                print(f"🗑️ Forgot oldest pattern: {key}")
                break
    
    # ═══════════════════════════════════════════════════════════════════════════
    # LEGACY COMPATIBILITY METHODS (For Backward Compatibility)
    # ═══════════════════════════════════════════════════════════════════════════
    
    def _legacy_weight_matrix_cleanup(self, vector: np.ndarray) -> CleanupResult:
        """Legacy weight matrix cleanup (preserved for compatibility)"""
        # Placeholder implementation - would interface with existing weight matrix system
        return CleanupResult(
            cleaned_vector=vector,  # Pass through for now
            confidence=0.5,
            method_used="legacy_weight_matrix",
            iterations_used=1,
            converged=True,
            diagnostics={'note': 'Legacy compatibility method'}
        )
    
    def _legacy_hetero_retrieval(self, key_vector: np.ndarray) -> CleanupResult:
        """Legacy hetero-associative retrieval using matrix multiplication"""
        # Placeholder for matrix multiplication approach
        return CleanupResult(
            cleaned_vector=key_vector,  # Pass through for now
            confidence=0.5,
            method_used="legacy_matrix_multiplication",
            iterations_used=1,
            converged=True,
            diagnostics={'note': 'Legacy hetero-associative method'}
        )
    
    def _hopfield_cleanup(self, vector: np.ndarray) -> CleanupResult:
        """Hopfield network cleanup"""
        if self.hopfield_weights is None:
            self._initialize_hopfield_weights()
        
        current = vector.copy()
        
        for iteration in range(self.config.hopfield_max_iterations):
            # Hopfield update: x_new = tanh(W @ x_old)
            new_vector = np.tanh(self.hopfield_weights @ current)
            
            # Check convergence
            diff = np.linalg.norm(new_vector - current)
            if diff < self.config.hopfield_convergence_threshold:
                return CleanupResult(
                    cleaned_vector=new_vector,
                    confidence=1.0 - diff,
                    method_used="hopfield_network",
                    iterations_used=iteration + 1,
                    converged=True,
                    diagnostics={'final_diff': diff}
                )
            
            current = new_vector
        
        return CleanupResult(
            cleaned_vector=current,
            confidence=0.5,
            method_used="hopfield_network",
            iterations_used=self.config.hopfield_max_iterations,
            converged=False,
            diagnostics={'note': 'Max iterations reached'}
        )
    
    def _initialize_hopfield_weights(self):
        """Initialize Hopfield weight matrix"""
        self.hopfield_weights = np.zeros((self.vector_dim, self.vector_dim))
        
        # Add prototypes to Hopfield matrix
        for prototype in self.cleanup_prototypes:
            self.hopfield_weights += np.outer(prototype, prototype) / self.vector_dim
    
    def _adaptive_threshold_cleanup(self, vector: np.ndarray) -> CleanupResult:
        """Adaptive threshold cleanup"""
        # Estimate signal and noise characteristics
        signal_power = np.var(vector)
        noise_estimate = np.mean(np.abs(vector - np.mean(vector)))  # Simple noise estimate
        
        # Compute adaptive threshold
        adaptive_threshold = self.compute_snr_threshold(signal_power, noise_estimate ** 2)
        
        # Use correlation cleanup with adaptive threshold
        return self.correlation_cleanup(vector, confidence_threshold=adaptive_threshold)
    
    def _apply_progressive_relaxation(self, vector: np.ndarray, iteration: int) -> np.ndarray:
        """Apply progressive relaxation strategy"""
        if iteration < len(self.config.relaxation_steps):
            factor = self.config.relaxation_steps[iteration]
            threshold = self.config.correlation_confidence_threshold * factor
            result = self.correlation_cleanup(vector, confidence_threshold=threshold)
            return result.cleaned_vector
        else:
            return vector
    
    def get_diagnostics(self) -> Dict[str, Any]:
        """Get comprehensive system diagnostics"""
        capacity_info = self.check_associative_capacity()
        
        return {
            'config_summary': self.config._get_method_summary(),
            'capacity_info': capacity_info._asdict(),
            'cleanup_stats': dict(self.cleanup_stats),
            'prototype_count': len(self.cleanup_prototypes),
            'memory_trace_count': len(self.memory_traces),
            'system_health': {
                'at_capacity': capacity_info.at_capacity,
                'success_rate': (self.cleanup_stats['successful_cleanups'] / 
                               max(1, self.cleanup_stats['total_cleanups'])),
                'primary_method': self.config.cleanup_method.value,
                'fallback_enabled': self.config.fallback_to_legacy
            }
        }


# ═══════════════════════════════════════════════════════════════════════════════
# FACTORY FUNCTIONS FOR EASY INSTANTIATION
# ═══════════════════════════════════════════════════════════════════════════════

def create_plate_1995_cleanup_system(vector_dim: int) -> PlateCleanupSystem:
    """Create system following strict Plate (1995) research"""
    config = create_plate_1995_config()
    return PlateCleanupSystem(vector_dim, config)


def create_legacy_compatible_cleanup_system(vector_dim: int) -> PlateCleanupSystem:
    """Create system with full backward compatibility"""
    config = create_legacy_compatible_config()
    return PlateCleanupSystem(vector_dim, config)


def create_high_performance_cleanup_system(vector_dim: int) -> PlateCleanupSystem:
    """Create system optimized for performance"""
    from .holographic_cleanup_config import create_high_performance_config
    config = create_high_performance_config()
    return PlateCleanupSystem(vector_dim, config)