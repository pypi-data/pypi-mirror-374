"""
📋 Capacity Monitor
====================

🎯 ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

🧪 Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

📋 Component Integration:
========================
    ┌──────────┐
    │   This   │
    │Component │ ←→ Other Components
    └──────────┘
         ↑↓
    System Integration

"""
"""
Capacity Monitor for Holographic Reduced Representations

Implements Plate (1995) Section IX "Capacity" (pages 642-648).
Addresses FIXME #5: Missing capacity analysis and bounds with systematic
capacity estimation, degradation detection, and adaptive strategies.

Research-Accurate Implementation of:
- Capacity formula: C ≈ n/(2 log n) for n-dimensional vectors
- Capacity estimation for compositional structures
- Degradation detection when approaching capacity limits  
- Adaptive strategies when capacity exceeded

Based on: Plate (1995) Section IX, page 642; Theorem 1, page 644
"""

import numpy as np
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import logging
import math

from .structure_types import StructureType

logger = logging.getLogger(__name__)


class CapacityStatus(Enum):
    """Capacity status levels"""
    LOW = "low"           # < 50% capacity
    MODERATE = "moderate" # 50-80% capacity
    HIGH = "high"         # 80-95% capacity
    CRITICAL = "critical" # > 95% capacity
    EXCEEDED = "exceeded" # > 100% theoretical capacity


@dataclass
class CapacityAssessment:
    """Comprehensive capacity assessment"""
    theoretical_capacity: int
    current_load: int
    utilization_ratio: float
    status: CapacityStatus
    degradation_metrics: Dict[str, float]
    recommendations: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass 
class DegradationMetrics:
    """Metrics for capacity-related degradation"""
    retrieval_accuracy: float
    false_positive_rate: float
    noise_level: float
    interference_score: float
    confidence_degradation: float


class CapacityMonitor:
    """
    Research-Accurate Capacity Monitor for HRR Systems
    
    Implements Plate (1995) Section IX capacity analysis with:
    - Theoretical capacity bounds: C ≈ n/(2 log n)
    - Real-time degradation monitoring
    - Adaptive capacity management strategies
    - Cross-interference detection between stored items
    
    Key Research Insight: "The capacity of HRR is fundamentally limited by
    the dimensionality and the ability to distinguish between similar patterns"
    """
    
    def __init__(self, 
                 vector_dim: int = 512,
                 monitoring_enabled: bool = True,
                 degradation_threshold: float = 0.8,
                 sample_size_for_testing: int = 100):
        """
        Initialize Capacity Monitor
        
        Args:
            vector_dim: Dimensionality of HRR vectors
            monitoring_enabled: Whether to actively monitor capacity
            degradation_threshold: Threshold for degradation warnings
            sample_size_for_testing: Sample size for degradation testing
        """
        self.vector_dim = vector_dim
        self.monitoring_enabled = monitoring_enabled
        self.degradation_threshold = degradation_threshold
        self.sample_size_for_testing = sample_size_for_testing
        
        # Compute theoretical capacity using Plate's formula
        self.theoretical_capacity = self._compute_theoretical_capacity()
        
        # Storage for monitoring
        self.stored_items = {}  # item_id -> vector
        self.storage_metadata = {}  # item_id -> metadata
        self.storage_order = []  # track insertion order
        
        # Capacity monitoring history
        self.capacity_history = []
        self.degradation_history = []
        
        # Performance baselines (computed on first items)
        self.baseline_metrics = None
        
        # Adaptive strategies state
        self.expansion_history = []
        self.cleanup_history = []
        
        logger.info(f"Initialized capacity monitor: dim={vector_dim}, "
                   f"theoretical_capacity={self.theoretical_capacity}")
    
    def _compute_theoretical_capacity(self) -> int:
        """
        Compute theoretical capacity using Plate (1995) formula
        
        From Theorem 1 (page 644): C ≈ n/(2 log n) where n is vector dimension
        This gives the approximate number of random vectors that can be stored
        before significant interference occurs.
        """
        if self.vector_dim <= 1:
            return 1
        
        # Plate's formula: C ≈ n/(2 log n)
        theoretical = self.vector_dim / (2 * math.log(self.vector_dim))
        
        # Round down for conservative estimate
        return max(1, int(theoretical))
    
    def register_stored_item(self, 
                           item_id: str,
                           vector: np.ndarray, 
                           structure_type: Optional[StructureType] = None,
                           metadata: Optional[Dict[str, Any]] = None):
        """
        Register a newly stored item for capacity monitoring
        
        Args:
            item_id: Unique identifier for the stored item
            vector: The stored vector
            structure_type: Type of compositional structure (if applicable)
            metadata: Additional metadata about the item
        """
        if not self.monitoring_enabled:
            return
        
        # Store normalized vector for consistent monitoring
        normalized_vector = vector / (np.linalg.norm(vector) + 1e-10)
        self.stored_items[item_id] = normalized_vector
        
        # Store metadata
        if metadata is None:
            metadata = {}
        metadata.update({
            "insertion_time": len(self.storage_order),
            "structure_type": structure_type.value if structure_type else None,
            "original_norm": np.linalg.norm(vector)
        })
        self.storage_metadata[item_id] = metadata
        
        # Track insertion order
        self.storage_order.append(item_id)
        
        # Trigger capacity assessment if needed
        current_load = len(self.stored_items)
        if current_load % 10 == 0 or current_load > self.theoretical_capacity * 0.8:
            self._update_capacity_monitoring()
        
        logger.debug(f"Registered item {item_id} (total items: {current_load})")
    
    def remove_stored_item(self, item_id: str):
        """Remove item from capacity monitoring"""
        if item_id in self.stored_items:
            del self.stored_items[item_id]
            del self.storage_metadata[item_id]
            if item_id in self.storage_order:
                self.storage_order.remove(item_id)
            
            logger.debug(f"Removed item {item_id} from capacity monitoring")
    
    def assess_current_capacity(self, 
                               include_degradation_test: bool = True) -> CapacityAssessment:
        """
        Comprehensive capacity assessment
        
        Args:
            include_degradation_test: Whether to run degradation testing
            
        Returns:
            CapacityAssessment with current status and recommendations
        """
        current_load = len(self.stored_items)
        utilization_ratio = current_load / self.theoretical_capacity
        
        # Determine capacity status
        if utilization_ratio >= 1.0:
            status = CapacityStatus.EXCEEDED
        elif utilization_ratio >= 0.95:
            status = CapacityStatus.CRITICAL
        elif utilization_ratio >= 0.8:
            status = CapacityStatus.HIGH
        elif utilization_ratio >= 0.5:
            status = CapacityStatus.MODERATE
        else:
            status = CapacityStatus.LOW
        
        # Run degradation analysis if requested
        degradation_metrics = {}
        if include_degradation_test and current_load > 5:
            degradation_metrics = self._assess_degradation()
        
        # Generate recommendations
        recommendations = self._generate_capacity_recommendations(status, degradation_metrics)
        
        assessment = CapacityAssessment(
            theoretical_capacity=self.theoretical_capacity,
            current_load=current_load,
            utilization_ratio=utilization_ratio,
            status=status,
            degradation_metrics=degradation_metrics,
            recommendations=recommendations,
            metadata={
                "vector_dim": self.vector_dim,
                "monitoring_enabled": self.monitoring_enabled,
                "assessment_time": len(self.capacity_history)
            }
        )
        
        # Store in history
        self.capacity_history.append(assessment)
        
        logger.info(f"Capacity assessment: {current_load}/{self.theoretical_capacity} "
                   f"({utilization_ratio:.1%}) - {status.value}")
        
        return assessment
    
    def _assess_degradation(self) -> Dict[str, float]:
        """
        Assess capacity-related degradation through empirical testing
        
        Tests actual retrieval performance to detect interference effects
        that indicate approaching capacity limits.
        """
        if len(self.stored_items) < 10:
            return {"insufficient_data": True}
        
        # Sample items for testing
        test_items = self._sample_items_for_testing()
        
        # Metrics to assess
        retrieval_accuracies = []
        false_positives = []
        noise_levels = []
        interference_scores = []
        
        for item_id, original_vector in test_items.items():
            # Test 1: Retrieval accuracy (how well can we recover the original?)
            retrieval_accuracy = self._test_retrieval_accuracy(item_id, original_vector)
            retrieval_accuracies.append(retrieval_accuracy)
            
            # Test 2: False positive rate (how often do we get wrong matches?)
            false_positive_rate = self._test_false_positive_rate(item_id, original_vector)
            false_positives.append(false_positive_rate)
            
            # Test 3: Noise level (how much noise has accumulated?)
            noise_level = self._test_noise_level(original_vector)
            noise_levels.append(noise_level)
            
            # Test 4: Interference score (how much do other items interfere?)
            interference_score = self._test_interference_score(item_id, original_vector)
            interference_scores.append(interference_score)
        
        # Aggregate metrics
        degradation_metrics = {
            "retrieval_accuracy": np.mean(retrieval_accuracies),
            "false_positive_rate": np.mean(false_positives),
            "noise_level": np.mean(noise_levels),
            "interference_score": np.mean(interference_scores),
            "confidence_degradation": self._compute_confidence_degradation(retrieval_accuracies)
        }
        
        # Compare against baseline if available
        if self.baseline_metrics is None and len(self.stored_items) <= 20:
            self.baseline_metrics = degradation_metrics.copy()
            degradation_metrics["baseline_established"] = True
        
        # Store degradation history
        self.degradation_history.append(degradation_metrics)
        
        logger.debug(f"Degradation metrics: {degradation_metrics}")
        
        return degradation_metrics
    
    def _sample_items_for_testing(self) -> Dict[str, np.ndarray]:
        """Sample items for degradation testing"""
        test_size = min(self.sample_size_for_testing, len(self.stored_items))
        
        if test_size <= 0:
            return {}
        
        # Sample across different insertion times for representative test
        if len(self.stored_items) > test_size:
            # Sample evenly across storage history
            indices = np.linspace(0, len(self.storage_order) - 1, test_size, dtype=int)
            test_item_ids = [self.storage_order[i] for i in indices]
        else:
            test_item_ids = list(self.stored_items.keys())
        
        return {item_id: self.stored_items[item_id] for item_id in test_item_ids}
    
    def _test_retrieval_accuracy(self, item_id: str, original_vector: np.ndarray) -> float:
        """Test how accurately we can retrieve an item"""
        # Simulate retrieval by finding best match to original
        best_similarity = -1.0
        
        for other_id, stored_vector in self.stored_items.items():
            similarity = np.dot(original_vector, stored_vector)
            if similarity > best_similarity:
                best_similarity = similarity
                best_match_id = other_id
        
        # Accuracy is 1.0 if we retrieve the correct item, less if not
        if best_match_id == item_id:
            return max(0, best_similarity)
        else:
            return 0.0  # Wrong item retrieved
    
    def _test_false_positive_rate(self, item_id: str, original_vector: np.ndarray) -> float:
        """Test false positive rate in retrieval"""
        # Create a noisy version of the original
        noise_level = 0.1
        noisy_vector = original_vector + np.random.normal(0, noise_level, len(original_vector))
        noisy_vector = noisy_vector / (np.linalg.norm(noisy_vector) + 1e-10)
        
        # Find best matches
        similarities = []
        for other_id, stored_vector in self.stored_items.items():
            similarity = np.dot(noisy_vector, stored_vector)
            similarities.append((other_id, similarity))
        
        # Sort by similarity
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Check if correct item is top match
        if similarities[0][0] == item_id:
            return 0.0  # No false positive
        else:
            return 1.0  # False positive occurred
    
    def _test_noise_level(self, original_vector: np.ndarray) -> float:
        """Test accumulated noise level in storage"""
        # Statistical test for vector quality
        vector_mean = abs(np.mean(original_vector))
        vector_std = np.std(original_vector)
        expected_std = 1.0 / np.sqrt(len(original_vector))
        
        # Higher deviations from expected HRR properties indicate noise
        mean_deviation = min(1.0, vector_mean * 5)  # Should be ~0
        std_deviation = min(1.0, abs(vector_std - expected_std) / expected_std)
        
        return (mean_deviation + std_deviation) / 2
    
    def _test_interference_score(self, item_id: str, original_vector: np.ndarray) -> float:
        """Test interference from other stored items"""
        # Compute cross-correlations with all other items
        interferences = []
        
        for other_id, other_vector in self.stored_items.items():
            if other_id != item_id:
                # Cross-correlation indicates potential interference
                cross_correlation = abs(np.dot(original_vector, other_vector))
                interferences.append(cross_correlation)
        
        if not interferences:
            return 0.0
        
        # High interference = many high correlations with other items
        return np.mean(interferences)
    
    def _compute_confidence_degradation(self, retrieval_accuracies: List[float]) -> float:
        """Compute overall confidence degradation"""
        if not retrieval_accuracies:
            return 0.0
        
        mean_accuracy = np.mean(retrieval_accuracies)
        
        # Compare against baseline if available
        if self.baseline_metrics and "retrieval_accuracy" in self.baseline_metrics:
            baseline_accuracy = self.baseline_metrics["retrieval_accuracy"]
            degradation = max(0, baseline_accuracy - mean_accuracy)
            return degradation
        else:
            # Compare against ideal (1.0)
            return max(0, 1.0 - mean_accuracy)
    
    def _generate_capacity_recommendations(self, 
                                         status: CapacityStatus,
                                         degradation_metrics: Dict[str, float]) -> List[str]:
        """Generate actionable recommendations based on capacity status"""
        recommendations = []
        
        if status == CapacityStatus.EXCEEDED:
            recommendations.extend([
                "CRITICAL: Theoretical capacity exceeded - implement immediate expansion",
                "Consider dimensionality increase to expand capacity",
                "Implement aggressive cleanup of least-used items", 
                "Apply hierarchical storage with reduced representations"
            ])
        
        elif status == CapacityStatus.CRITICAL:
            recommendations.extend([
                "WARNING: Approaching capacity limits",
                "Plan capacity expansion strategy",
                "Monitor degradation metrics closely",
                "Consider cleanup of older items"
            ])
        
        elif status == CapacityStatus.HIGH:
            recommendations.extend([
                "High capacity utilization - monitor for degradation",
                "Prepare capacity expansion plan",
                "Consider optimizing storage efficiency"
            ])
        
        # Degradation-specific recommendations
        if degradation_metrics:
            if degradation_metrics.get("retrieval_accuracy", 1.0) < 0.8:
                recommendations.append("Poor retrieval accuracy - consider cleanup or expansion")
            
            if degradation_metrics.get("false_positive_rate", 0.0) > 0.2:
                recommendations.append("High false positive rate - increase vector orthogonality")
            
            if degradation_metrics.get("noise_level", 0.0) > 0.3:
                recommendations.append("High noise levels detected - apply vector cleanup")
            
            if degradation_metrics.get("interference_score", 0.0) > 0.5:
                recommendations.append("High interference - consider more orthogonal encodings")
        
        return recommendations
    
    def _update_capacity_monitoring(self):
        """Update ongoing capacity monitoring"""
        if not self.monitoring_enabled:
            return
        
        # Run periodic capacity assessment
        assessment = self.assess_current_capacity(include_degradation_test=True)
        
        # Log warnings for concerning trends
        if assessment.status in [CapacityStatus.HIGH, CapacityStatus.CRITICAL, CapacityStatus.EXCEEDED]:
            logger.warning(f"Capacity status: {assessment.status.value} "
                          f"({assessment.utilization_ratio:.1%})")
            
            for recommendation in assessment.recommendations:
                logger.warning(f"Recommendation: {recommendation}")
    
    def expand_capacity(self, new_vector_dim: int) -> Dict[str, Any]:
        """
        Expand capacity by increasing vector dimensionality
        
        This is an adaptive strategy when approaching capacity limits.
        
        Args:
            new_vector_dim: New vector dimensionality (should be > current)
            
        Returns:
            Dict with expansion results and statistics
        """
        if new_vector_dim <= self.vector_dim:
            raise ValueError(f"New dimension {new_vector_dim} must be > current {self.vector_dim}")
        
        old_capacity = self.theoretical_capacity
        old_dim = self.vector_dim
        
        # Update dimensions and recompute capacity
        self.vector_dim = new_vector_dim
        self.theoretical_capacity = self._compute_theoretical_capacity()
        
        expansion_result = {
            "old_dimension": old_dim,
            "new_dimension": new_vector_dim,
            "old_capacity": old_capacity,
            "new_capacity": self.theoretical_capacity,
            "capacity_increase": self.theoretical_capacity - old_capacity,
            "expansion_factor": self.theoretical_capacity / old_capacity,
            "current_utilization": len(self.stored_items) / self.theoretical_capacity
        }
        
        # Record expansion in history
        self.expansion_history.append(expansion_result)
        
        logger.info(f"Capacity expanded: {old_dim}→{new_vector_dim} dimensions, "
                   f"capacity: {old_capacity}→{self.theoretical_capacity}")
        
        return expansion_result
    
    def cleanup_least_used_items(self, cleanup_fraction: float = 0.2) -> Dict[str, Any]:
        """
        Cleanup least-used items to free capacity
        
        Args:
            cleanup_fraction: Fraction of items to remove (0.0 to 1.0)
            
        Returns:
            Dict with cleanup results
        """
        if not 0.0 < cleanup_fraction <= 1.0:
            raise ValueError("cleanup_fraction must be between 0 and 1")
        
        num_items_to_remove = int(len(self.stored_items) * cleanup_fraction)
        if num_items_to_remove == 0:
            return {"items_removed": 0, "message": "No items to remove"}
        
        # For now, remove oldest items (could be enhanced with usage statistics)
        items_to_remove = self.storage_order[:num_items_to_remove]
        
        removed_items = []
        for item_id in items_to_remove:
            if item_id in self.stored_items:
                del self.stored_items[item_id]
                del self.storage_metadata[item_id]
                removed_items.append(item_id)
        
        # Update storage order
        self.storage_order = self.storage_order[num_items_to_remove:]
        
        cleanup_result = {
            "items_removed": len(removed_items),
            "items_remaining": len(self.stored_items),
            "new_utilization": len(self.stored_items) / self.theoretical_capacity,
            "cleanup_fraction_actual": len(removed_items) / (len(removed_items) + len(self.stored_items))
        }
        
        # Record cleanup in history
        self.cleanup_history.append(cleanup_result)
        
        logger.info(f"Cleanup completed: removed {len(removed_items)} items, "
                   f"{len(self.stored_items)} remaining")
        
        return cleanup_result
    
    def get_capacity_statistics(self) -> Dict[str, Any]:
        """Get comprehensive capacity monitoring statistics"""
        current_assessment = self.assess_current_capacity(include_degradation_test=False)
        
        stats = {
            "theoretical_capacity": self.theoretical_capacity,
            "current_load": len(self.stored_items),
            "utilization_ratio": current_assessment.utilization_ratio,
            "capacity_status": current_assessment.status.value,
            "vector_dimension": self.vector_dim,
            "monitoring_enabled": self.monitoring_enabled,
            
            # History statistics
            "assessments_performed": len(self.capacity_history),
            "degradation_tests_performed": len(self.degradation_history),
            "capacity_expansions": len(self.expansion_history),
            "cleanups_performed": len(self.cleanup_history),
            
            # Recent trends
            "recent_utilization_trend": self._compute_utilization_trend(),
            "recent_degradation_trend": self._compute_degradation_trend()
        }
        
        # Add baseline comparison if available
        if self.baseline_metrics:
            stats["baseline_metrics"] = self.baseline_metrics.copy()
        
        return stats
    
    def _compute_utilization_trend(self) -> Optional[float]:
        """Compute recent utilization trend"""
        if len(self.capacity_history) < 3:
            return None
        
        recent_utilizations = [assessment.utilization_ratio 
                              for assessment in self.capacity_history[-5:]]
        
        # Simple linear trend
        x = np.arange(len(recent_utilizations))
        trend_slope = np.polyfit(x, recent_utilizations, 1)[0]
        
        return float(trend_slope)
    
    def _compute_degradation_trend(self) -> Optional[float]:
        """Compute recent degradation trend"""
        if len(self.degradation_history) < 3:
            return None
        
        recent_accuracies = [metrics.get("retrieval_accuracy", 1.0)
                            for metrics in self.degradation_history[-5:]]
        
        # Trend in retrieval accuracy (negative slope = degradation)
        x = np.arange(len(recent_accuracies))
        trend_slope = np.polyfit(x, recent_accuracies, 1)[0]
        
        return float(-trend_slope)  # Negative for degradation direction