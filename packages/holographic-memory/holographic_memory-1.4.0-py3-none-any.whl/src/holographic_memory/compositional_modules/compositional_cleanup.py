"""
📋 Compositional Cleanup
=========================

🎯 ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

🧪 Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

📋 Component Integration:
========================
    ┌──────────┐
    │   This   │
    │Component │ ←→ Other Components
    └──────────┘
         ↑↓
    System Integration

"""
"""
Compositional Cleanup Engine

Implements Plate (1995) Section IV "Cleanup" (pages 628-630) for compositional structures.
Addresses FIXME #2: Inadequate cleanup memory integration with systematic cleanup
architecture for deeply nested structures.

Research-Accurate Implementation of:
- Multi-stage cleanup: local → structural → global cleanup
- Structure-specific cleanup memories for sequences, trees, records
- Iterative cleanup with convergence detection
- Cleanup confidence scoring and fallback strategies

Based on: Plate (1995) Section IV, page 628; "The success of the method depends critically on cleanup"
"""

import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Union
from dataclasses import dataclass
from enum import Enum
import logging

from .structure_types import StructureType

logger = logging.getLogger(__name__)


@dataclass
class CleanupResult:
    """Result of compositional cleanup operation"""
    cleaned_vector: np.ndarray
    confidence: float
    cleanup_stages: List[str]
    iterations: int
    convergence_achieved: bool
    fallback_used: bool = False
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class CleanupStage(Enum):
    """Cleanup stages as defined in Plate (1995)"""
    LOCAL = "local"          # Component-level cleanup
    STRUCTURAL = "structural" # Structure-specific cleanup  
    GLOBAL = "global"        # Cross-structure cleanup
    ITERATIVE = "iterative"  # Convergence-based refinement


class CompositionalCleanupEngine:
    """
    Research-Accurate Compositional Cleanup Engine
    
    Implements Plate (1995) Section IV systematic cleanup architecture:
    "The success of the method depends critically on cleanup" (page 628)
    
    Key Research Contributions:
    1. Multi-stage cleanup pipeline with convergence guarantees
    2. Structure-specific cleanup memories for different composition types
    3. Iterative refinement with oscillation detection
    4. Context-sensitive cleanup that considers structure type
    """
    
    def __init__(self,
                 vector_dim: int = 512,
                 cleanup_threshold: float = 0.3,
                 max_iterations: int = 10,
                 convergence_epsilon: float = 1e-4,
                 enable_iterative: bool = True):
        """
        Initialize Compositional Cleanup Engine
        
        Args:
            vector_dim: Dimensionality of vectors
            cleanup_threshold: Minimum confidence for cleanup success
            max_iterations: Maximum iterations for iterative cleanup
            convergence_epsilon: Convergence threshold for iterative cleanup
            enable_iterative: Whether to enable iterative cleanup refinement
        """
        self.vector_dim = vector_dim
        self.cleanup_threshold = cleanup_threshold
        self.max_iterations = max_iterations
        self.convergence_epsilon = convergence_epsilon
        self.enable_iterative = enable_iterative
        
        # Structure-specific cleanup memories
        self.cleanup_memories = self._initialize_cleanup_memories()
        
        # Cleanup statistics
        self.cleanup_stats = {
            "total_cleanups": 0,
            "successful_cleanups": 0,
            "convergence_failures": 0,
            "fallback_usage": 0,
            "stage_usage": {stage.value: 0 for stage in CleanupStage}
        }
        
        # Oscillation detection
        self.oscillation_memory = {}
        
    def _initialize_cleanup_memories(self) -> Dict[StructureType, Dict[str, np.ndarray]]:
        """Initialize structure-specific cleanup memories"""
        cleanup_memories = {}
        
        for structure_type in StructureType:
            cleanup_memories[structure_type] = {
                "prototypes": {},      # Known good patterns
                "compositions": {},    # Common composition patterns
                "degradations": {}     # Known degradation patterns
            }
        
        # Initialize with fundamental patterns
        self._create_fundamental_patterns(cleanup_memories)
        
        return cleanup_memories
    
    def _create_fundamental_patterns(self, cleanup_memories: Dict):
        """Create fundamental cleanup patterns for each structure type"""
        
        # Sequence patterns
        seq_patterns = {
            "empty_sequence": self._create_pattern_vector("empty_seq"),
            "singleton": self._create_pattern_vector("singleton"),
            "pair": self._create_pattern_vector("pair"),
            "triplet": self._create_pattern_vector("triplet")
        }
        cleanup_memories[StructureType.SEQUENCE]["prototypes"] = seq_patterns
        
        # Tree patterns
        tree_patterns = {
            "leaf_node": self._create_pattern_vector("leaf"),
            "binary_tree": self._create_pattern_vector("binary_tree"),
            "left_heavy": self._create_pattern_vector("left_heavy"),
            "balanced": self._create_pattern_vector("balanced")
        }
        cleanup_memories[StructureType.TREE]["prototypes"] = tree_patterns
        
        # Record patterns
        record_patterns = {
            "single_field": self._create_pattern_vector("single_field"),
            "key_value_pair": self._create_pattern_vector("kv_pair"),
            "multi_field": self._create_pattern_vector("multi_field")
        }
        cleanup_memories[StructureType.RECORD]["prototypes"] = record_patterns
        
        # Set patterns
        set_patterns = {
            "singleton_set": self._create_pattern_vector("singleton_set"),
            "small_set": self._create_pattern_vector("small_set"),
            "large_set": self._create_pattern_vector("large_set")
        }
        cleanup_memories[StructureType.SET]["prototypes"] = set_patterns
    
    def _create_pattern_vector(self, pattern_name: str) -> np.ndarray:
        """Create a reproducible pattern vector"""
        # Use hash for reproducible random generation
        np.random.seed(hash(pattern_name) % (2**32))
        pattern = np.random.normal(0, 1, self.vector_dim)
        pattern = pattern / (np.linalg.norm(pattern) + 1e-10)
        
        # Reset seed
        np.random.seed()
        
        return pattern
    
    def cleanup_compositional_structure(self, 
                                      structure_vector: np.ndarray,
                                      structure_type: Optional[StructureType] = None,
                                      context_hints: Optional[Dict[str, Any]] = None) -> CleanupResult:
        """
        Research-Accurate Multi-Stage Compositional Cleanup
        
        Implements Plate (1995) Section IV cleanup pipeline:
        1. Local cleanup - Clean individual components
        2. Structural cleanup - Apply structure-specific patterns
        3. Global cleanup - Cross-structure consistency
        4. Iterative refinement - Convergence-based improvement
        
        Args:
            structure_vector: Noisy compositional structure to clean
            structure_type: Type hint for structure-specific cleanup
            context_hints: Additional context for cleanup guidance
            
        Returns:
            CleanupResult with cleaned vector and metadata
        """
        if context_hints is None:
            context_hints = {}
        
        self.cleanup_stats["total_cleanups"] += 1
        
        # Normalize input
        input_vector = structure_vector / (np.linalg.norm(structure_vector) + 1e-10)
        current_vector = input_vector.copy()
        
        cleanup_stages = []
        total_iterations = 0
        
        # Stage 1: Local Cleanup
        current_vector, local_confidence = self._local_cleanup(current_vector, context_hints)
        cleanup_stages.append("local")
        self.cleanup_stats["stage_usage"]["local"] += 1
        
        logger.debug(f"Local cleanup confidence: {local_confidence:.3f}")
        
        # Stage 2: Structural Cleanup
        if structure_type is not None:
            current_vector, struct_confidence = self._structural_cleanup(
                current_vector, structure_type, context_hints
            )
            cleanup_stages.append("structural")
            self.cleanup_stats["stage_usage"]["structural"] += 1
            
            logger.debug(f"Structural cleanup confidence: {struct_confidence:.3f}")
        else:
            struct_confidence = local_confidence
        
        # Stage 3: Global Cleanup
        current_vector, global_confidence = self._global_cleanup(current_vector, context_hints)
        cleanup_stages.append("global")
        self.cleanup_stats["stage_usage"]["global"] += 1
        
        logger.debug(f"Global cleanup confidence: {global_confidence:.3f}")
        
        # Stage 4: Iterative Refinement
        convergence_achieved = True
        if self.enable_iterative and global_confidence < 0.8:
            current_vector, iterations, convergence_achieved = self._iterative_cleanup(
                current_vector, structure_type, context_hints
            )
            total_iterations = iterations
            cleanup_stages.append("iterative")
            self.cleanup_stats["stage_usage"]["iterative"] += 1
            
            logger.debug(f"Iterative cleanup: {iterations} iterations, "
                        f"converged: {convergence_achieved}")
        
        # Final confidence assessment
        final_confidence = max(local_confidence, struct_confidence, global_confidence)
        
        # Apply fallback if confidence too low
        fallback_used = False
        if final_confidence < self.cleanup_threshold:
            current_vector, fallback_used = self._apply_fallback_cleanup(
                current_vector, input_vector, structure_type
            )
            final_confidence = max(final_confidence, 0.2)  # Minimum fallback confidence
            self.cleanup_stats["fallback_usage"] += 1
        
        # Update success statistics
        if final_confidence >= self.cleanup_threshold:
            self.cleanup_stats["successful_cleanups"] += 1
        
        if not convergence_achieved:
            self.cleanup_stats["convergence_failures"] += 1
        
        return CleanupResult(
            cleaned_vector=current_vector,
            confidence=final_confidence,
            cleanup_stages=cleanup_stages,
            iterations=total_iterations,
            convergence_achieved=convergence_achieved,
            fallback_used=fallback_used,
            metadata={
                "input_norm": np.linalg.norm(structure_vector),
                "structure_type": structure_type.value if structure_type else None,
                "context_hints": context_hints
            }
        )
    
    def _local_cleanup(self, 
                      vector: np.ndarray, 
                      context_hints: Dict[str, Any]) -> Tuple[np.ndarray, float]:
        """
        Local cleanup - clean individual vector components
        
        Focuses on removing noise and correcting obvious errors at the
        component level before applying structure-specific cleanup.
        """
        # Approach 1: Statistical outlier removal
        vector_std = np.std(vector)
        vector_mean = np.mean(vector)
        outlier_threshold = 3.0 * vector_std
        
        # Clip extreme outliers
        cleaned = np.clip(vector, 
                         vector_mean - outlier_threshold,
                         vector_mean + outlier_threshold)
        
        # Approach 2: Sparsity-based denoising
        if context_hints.get("expected_sparsity"):
            # Keep only strongest components for sparse structures
            sparsity_level = context_hints["expected_sparsity"]
            abs_values = np.abs(cleaned)
            threshold = np.percentile(abs_values, (1 - sparsity_level) * 100)
            cleaned = np.where(abs_values > threshold, cleaned, 0)
        
        # Approach 3: Smooth high-frequency noise
        if len(cleaned) > 10:
            # Simple moving average for noise reduction
            window_size = min(5, len(cleaned) // 10)
            if window_size > 1:
                # Apply mild smoothing
                padded = np.pad(cleaned, window_size//2, mode='edge')
                kernel = np.ones(window_size) / window_size
                smoothed = np.convolve(padded, kernel, mode='valid')
                
                # Blend original with smoothed (preserve sharp features)
                blend_factor = 0.3
                cleaned = (1 - blend_factor) * cleaned + blend_factor * smoothed
        
        # Normalize result
        cleaned = cleaned / (np.linalg.norm(cleaned) + 1e-10)
        
        # Compute confidence based on noise reduction achieved
        noise_reduction = np.linalg.norm(vector - cleaned)
        confidence = max(0.1, 1.0 - noise_reduction)
        
        return cleaned, confidence
    
    def _structural_cleanup(self, 
                           vector: np.ndarray,
                           structure_type: StructureType,
                           context_hints: Dict[str, Any]) -> Tuple[np.ndarray, float]:
        """
        Structure-specific cleanup using known structural patterns
        
        Applies cleanup patterns specific to the type of compositional
        structure (sequence, tree, record, etc.).
        """
        structure_memory = self.cleanup_memories.get(structure_type)
        if not structure_memory:
            return vector, 0.5  # No specific cleanup available
        
        prototypes = structure_memory["prototypes"]
        
        # Find best matching prototype
        best_prototype = None
        best_similarity = -1.0
        
        for prototype_name, prototype_vector in prototypes.items():
            similarity = np.dot(vector, prototype_vector)
            if similarity > best_similarity:
                best_similarity = similarity
                best_prototype = prototype_vector
        
        if best_prototype is not None and best_similarity > 0.2:
            # Blend with best prototype (weighted by similarity)
            blend_weight = min(0.5, best_similarity)
            cleaned = (1 - blend_weight) * vector + blend_weight * best_prototype
            
            # Normalize
            cleaned = cleaned / (np.linalg.norm(cleaned) + 1e-10)
            
            confidence = best_similarity
            
            logger.debug(f"Structural cleanup: matched prototype with "
                        f"similarity {best_similarity:.3f}")
        else:
            cleaned = vector
            confidence = 0.3  # Default confidence when no good prototype match
        
        return cleaned, confidence
    
    def _global_cleanup(self, 
                       vector: np.ndarray,
                       context_hints: Dict[str, Any]) -> Tuple[np.ndarray, float]:
        """
        Global cleanup - ensure cross-structure consistency
        
        Applies cleanup that considers relationships between different
        parts of the compositional structure.
        """
        # Approach 1: Consistency with global statistics
        # Check if vector statistics are reasonable for HRR vectors
        
        vector_norm = np.linalg.norm(vector)
        vector_mean = np.mean(vector)
        vector_std = np.std(vector)
        
        # HRR vectors should be approximately normalized with ~zero mean
        cleaned = vector.copy()
        
        # Ensure proper normalization
        if abs(vector_norm - 1.0) > 0.1:
            cleaned = cleaned / (vector_norm + 1e-10)
        
        # Center the vector (HRR vectors should have approximately zero mean)
        if abs(vector_mean) > 0.2:
            cleaned = cleaned - np.mean(cleaned)
            cleaned = cleaned / (np.linalg.norm(cleaned) + 1e-10)
        
        # Check variance (should be reasonable for random-like HRR vectors)
        expected_std = 1.0 / np.sqrt(len(vector))  # Expected for normalized random vector
        if vector_std < expected_std * 0.5 or vector_std > expected_std * 2.0:
            # Adjust variance by blending with appropriate noise
            noise_scale = max(0.1, expected_std - vector_std)
            noise = np.random.normal(0, noise_scale, len(vector))
            cleaned = 0.9 * cleaned + 0.1 * noise
            cleaned = cleaned / (np.linalg.norm(cleaned) + 1e-10)
        
        # Compute confidence based on how much correction was needed
        correction_magnitude = np.linalg.norm(vector - cleaned)
        confidence = max(0.2, 1.0 - 2 * correction_magnitude)
        
        return cleaned, confidence
    
    def _iterative_cleanup(self, 
                          vector: np.ndarray,
                          structure_type: Optional[StructureType],
                          context_hints: Dict[str, Any]) -> Tuple[np.ndarray, int, bool]:
        """
        Iterative cleanup with convergence detection
        
        Repeatedly applies cleanup until convergence or maximum iterations.
        Includes oscillation detection to prevent infinite loops.
        """
        current_vector = vector.copy()
        previous_vectors = [current_vector.copy()]
        
        converged = False
        iteration = 0
        
        while iteration < self.max_iterations and not converged:
            iteration += 1
            
            # Apply one round of local + structural cleanup
            temp_vector, _ = self._local_cleanup(current_vector, context_hints)
            
            if structure_type is not None:
                temp_vector, _ = self._structural_cleanup(temp_vector, structure_type, context_hints)
            
            temp_vector, _ = self._global_cleanup(temp_vector, context_hints)
            
            # Check for convergence
            change_magnitude = np.linalg.norm(temp_vector - current_vector)
            
            if change_magnitude < self.convergence_epsilon:
                converged = True
                logger.debug(f"Iterative cleanup converged at iteration {iteration}")
            
            # Check for oscillation
            if self._detect_oscillation(temp_vector, previous_vectors):
                logger.debug(f"Oscillation detected at iteration {iteration}, stopping")
                break
            
            current_vector = temp_vector
            previous_vectors.append(current_vector.copy())
            
            # Keep only recent history for oscillation detection
            if len(previous_vectors) > 5:
                previous_vectors.pop(0)
        
        return current_vector, iteration, converged
    
    def _detect_oscillation(self, current_vector: np.ndarray, previous_vectors: List[np.ndarray]) -> bool:
        """Detect if iterative cleanup is oscillating"""
        if len(previous_vectors) < 3:
            return False
        
        # Check if current vector is similar to any recent previous vector
        for prev_vector in previous_vectors[-3:]:
            similarity = np.dot(current_vector, prev_vector)
            if similarity > 0.95:  # Very similar to a recent state
                return True
        
        return False
    
    def _apply_fallback_cleanup(self, 
                               failed_vector: np.ndarray,
                               original_vector: np.ndarray,
                               structure_type: Optional[StructureType]) -> Tuple[np.ndarray, bool]:
        """
        Apply fallback cleanup when standard cleanup fails
        
        Uses conservative strategies to at least ensure basic vector properties.
        """
        # Strategy 1: Blend with original (conservative approach)
        conservative_blend = 0.7 * original_vector + 0.3 * failed_vector
        conservative_blend = conservative_blend / (np.linalg.norm(conservative_blend) + 1e-10)
        
        # Strategy 2: Project onto space of "reasonable" HRR vectors
        # Ensure zero mean and unit norm
        zero_mean = conservative_blend - np.mean(conservative_blend)
        fallback_vector = zero_mean / (np.linalg.norm(zero_mean) + 1e-10)
        
        logger.warning("Applied fallback cleanup due to low confidence")
        
        return fallback_vector, True
    
    def add_cleanup_prototype(self, 
                             structure_type: StructureType,
                             prototype_name: str,
                             prototype_vector: np.ndarray):
        """Add a new prototype to the cleanup memory"""
        if structure_type not in self.cleanup_memories:
            self.cleanup_memories[structure_type] = {"prototypes": {}, "compositions": {}, "degradations": {}}
        
        # Normalize prototype
        normalized_prototype = prototype_vector / (np.linalg.norm(prototype_vector) + 1e-10)
        
        self.cleanup_memories[structure_type]["prototypes"][prototype_name] = normalized_prototype
        
        logger.debug(f"Added cleanup prototype '{prototype_name}' for {structure_type.value}")
    
    def get_cleanup_statistics(self) -> Dict[str, Any]:
        """Get comprehensive cleanup statistics"""
        stats = self.cleanup_stats.copy()
        
        if stats["total_cleanups"] > 0:
            stats["success_rate"] = stats["successful_cleanups"] / stats["total_cleanups"]
            stats["fallback_rate"] = stats["fallback_usage"] / stats["total_cleanups"]
        else:
            stats["success_rate"] = 0.0
            stats["fallback_rate"] = 0.0
        
        stats["cleanup_memory_sizes"] = {
            struct_type.value: len(memory["prototypes"])
            for struct_type, memory in self.cleanup_memories.items()
        }
        
        return stats