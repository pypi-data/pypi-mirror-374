# coding: utf-8

"""
    Sifflet Backend API

    Requirements: <br>    - [Create your access token through the UI](https://docs.siffletdata.com/docs/generate-an-api-token#create-an-api-token) <br>    - Get your tenant name: if you access to Sifflet with `https://abcdef.siffletdata.com`, then your tenant would be `abcdef`

    The version of the OpenAPI document: 1.0.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations

import json
import pprint
from typing import Any, Dict, List, Optional, Set, Union

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    StrictStr,
    ValidationError,
    field_validator,
)
from sifflet_sdk.client.models.big_query_ingestion_time_partitioning_properties import (
    BigQueryIngestionTimePartitioningProperties,
)
from sifflet_sdk.client.models.big_query_integer_range_partitioning_properties import (
    BigQueryIntegerRangePartitioningProperties,
)
from sifflet_sdk.client.models.big_query_partitioning_properties import (
    BigQueryPartitioningProperties,
)
from sifflet_sdk.client.models.big_query_time_unit_column_partitioning_properties import (
    BigQueryTimeUnitColumnPartitioningProperties,
)
from sifflet_sdk.client.models.default_partitioning import DefaultPartitioning
from typing_extensions import Literal, Self

DATASETPARTITIONINGDTOPROPERTIES_ONE_OF_SCHEMAS = [
    "BigQueryIngestionTimePartitioningProperties",
    "BigQueryIntegerRangePartitioningProperties",
    "BigQueryPartitioningProperties",
    "BigQueryTimeUnitColumnPartitioningProperties",
    "DefaultPartitioning",
]


class DatasetPartitioningDtoProperties(BaseModel):
    """
    DatasetPartitioningDtoProperties
    """

    # data type: BigQueryPartitioningProperties
    oneof_schema_1_validator: Optional[BigQueryPartitioningProperties] = None
    # data type: BigQueryIngestionTimePartitioningProperties
    oneof_schema_2_validator: Optional[BigQueryIngestionTimePartitioningProperties] = None
    # data type: BigQueryIntegerRangePartitioningProperties
    oneof_schema_3_validator: Optional[BigQueryIntegerRangePartitioningProperties] = None
    # data type: BigQueryTimeUnitColumnPartitioningProperties
    oneof_schema_4_validator: Optional[BigQueryTimeUnitColumnPartitioningProperties] = None
    # data type: DefaultPartitioning
    oneof_schema_5_validator: Optional[DefaultPartitioning] = None
    actual_instance: Optional[
        Union[
            BigQueryIngestionTimePartitioningProperties,
            BigQueryIntegerRangePartitioningProperties,
            BigQueryPartitioningProperties,
            BigQueryTimeUnitColumnPartitioningProperties,
            DefaultPartitioning,
        ]
    ] = None
    one_of_schemas: Set[str] = {
        "BigQueryIngestionTimePartitioningProperties",
        "BigQueryIntegerRangePartitioningProperties",
        "BigQueryPartitioningProperties",
        "BigQueryTimeUnitColumnPartitioningProperties",
        "DefaultPartitioning",
    }

    model_config = ConfigDict(
        validate_assignment=True,
        protected_namespaces=(),
    )

    discriminator_value_class_map: Dict[str, str] = {}

    def __init__(self, *args, **kwargs) -> None:
        if args:
            if len(args) > 1:
                raise ValueError("If a position argument is used, only 1 is allowed to set `actual_instance`")
            if kwargs:
                raise ValueError("If a position argument is used, keyword arguments cannot be used.")
            super().__init__(actual_instance=args[0])
        else:
            super().__init__(**kwargs)

    @field_validator("actual_instance")
    def actual_instance_must_validate_oneof(cls, v):
        instance = DatasetPartitioningDtoProperties.model_construct()
        error_messages = []
        match = 0
        # validate data type: BigQueryPartitioningProperties
        if not isinstance(v, BigQueryPartitioningProperties):
            error_messages.append(f"Error! Input type `{type(v)}` is not `BigQueryPartitioningProperties`")
        else:
            match += 1
        # validate data type: BigQueryIngestionTimePartitioningProperties
        if not isinstance(v, BigQueryIngestionTimePartitioningProperties):
            error_messages.append(f"Error! Input type `{type(v)}` is not `BigQueryIngestionTimePartitioningProperties`")
        else:
            match += 1
        # validate data type: BigQueryIntegerRangePartitioningProperties
        if not isinstance(v, BigQueryIntegerRangePartitioningProperties):
            error_messages.append(f"Error! Input type `{type(v)}` is not `BigQueryIntegerRangePartitioningProperties`")
        else:
            match += 1
        # validate data type: BigQueryTimeUnitColumnPartitioningProperties
        if not isinstance(v, BigQueryTimeUnitColumnPartitioningProperties):
            error_messages.append(
                f"Error! Input type `{type(v)}` is not `BigQueryTimeUnitColumnPartitioningProperties`"
            )
        else:
            match += 1
        # validate data type: DefaultPartitioning
        if not isinstance(v, DefaultPartitioning):
            error_messages.append(f"Error! Input type `{type(v)}` is not `DefaultPartitioning`")
        else:
            match += 1
        if match > 1:
            # more than 1 match
            raise ValueError(
                "Multiple matches found when setting `actual_instance` in DatasetPartitioningDtoProperties with oneOf schemas: BigQueryIngestionTimePartitioningProperties, BigQueryIntegerRangePartitioningProperties, BigQueryPartitioningProperties, BigQueryTimeUnitColumnPartitioningProperties, DefaultPartitioning. Details: "
                + ", ".join(error_messages)
            )
        elif match == 0:
            # no match
            raise ValueError(
                "No match found when setting `actual_instance` in DatasetPartitioningDtoProperties with oneOf schemas: BigQueryIngestionTimePartitioningProperties, BigQueryIntegerRangePartitioningProperties, BigQueryPartitioningProperties, BigQueryTimeUnitColumnPartitioningProperties, DefaultPartitioning. Details: "
                + ", ".join(error_messages)
            )
        else:
            return v

    @classmethod
    def from_dict(cls, obj: Union[str, Dict[str, Any]]) -> Self:
        return cls.from_json(json.dumps(obj))

    @classmethod
    def from_json(cls, json_str: str) -> Self:
        """Returns the object represented by the json string"""
        instance = cls.model_construct()
        error_messages = []
        match = 0

        # use oneOf discriminator to lookup the data type
        _data_type = json.loads(json_str).get("partitioningType")
        if not _data_type:
            raise ValueError("Failed to lookup data type from the field `partitioningType` in the input.")

        # check if data type is `BigQueryIngestionTimePartitioningProperties`
        if _data_type == "INGESTION_TIME":
            instance.actual_instance = BigQueryIngestionTimePartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `BigQueryIntegerRangePartitioningProperties`
        if _data_type == "INTEGER_RANGE":
            instance.actual_instance = BigQueryIntegerRangePartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `DefaultPartitioning`
        if _data_type == "NO_PARTITIONING":
            instance.actual_instance = DefaultPartitioning.from_json(json_str)
            return instance

        # check if data type is `BigQueryTimeUnitColumnPartitioningProperties`
        if _data_type == "TIME_UNIT_COLUMN":
            instance.actual_instance = BigQueryTimeUnitColumnPartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `BigQueryIngestionTimePartitioningProperties`
        if _data_type == "BigQueryIngestionTimePartitioningProperties":
            instance.actual_instance = BigQueryIngestionTimePartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `BigQueryIntegerRangePartitioningProperties`
        if _data_type == "BigQueryIntegerRangePartitioningProperties":
            instance.actual_instance = BigQueryIntegerRangePartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `BigQueryPartitioningProperties`
        if _data_type == "BigQueryPartitioningProperties":
            instance.actual_instance = BigQueryPartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `BigQueryTimeUnitColumnPartitioningProperties`
        if _data_type == "BigQueryTimeUnitColumnPartitioningProperties":
            instance.actual_instance = BigQueryTimeUnitColumnPartitioningProperties.from_json(json_str)
            return instance

        # check if data type is `DefaultPartitioning`
        if _data_type == "DefaultPartitioning":
            instance.actual_instance = DefaultPartitioning.from_json(json_str)
            return instance

        # deserialize data into BigQueryPartitioningProperties
        try:
            instance.actual_instance = BigQueryPartitioningProperties.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into BigQueryIngestionTimePartitioningProperties
        try:
            instance.actual_instance = BigQueryIngestionTimePartitioningProperties.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into BigQueryIntegerRangePartitioningProperties
        try:
            instance.actual_instance = BigQueryIntegerRangePartitioningProperties.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into BigQueryTimeUnitColumnPartitioningProperties
        try:
            instance.actual_instance = BigQueryTimeUnitColumnPartitioningProperties.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))
        # deserialize data into DefaultPartitioning
        try:
            instance.actual_instance = DefaultPartitioning.from_json(json_str)
            match += 1
        except (ValidationError, ValueError) as e:
            error_messages.append(str(e))

        if match > 1:
            # more than 1 match
            raise ValueError(
                "Multiple matches found when deserializing the JSON string into DatasetPartitioningDtoProperties with oneOf schemas: BigQueryIngestionTimePartitioningProperties, BigQueryIntegerRangePartitioningProperties, BigQueryPartitioningProperties, BigQueryTimeUnitColumnPartitioningProperties, DefaultPartitioning. Details: "
                + ", ".join(error_messages)
            )
        elif match == 0:
            # no match
            raise ValueError(
                "No match found when deserializing the JSON string into DatasetPartitioningDtoProperties with oneOf schemas: BigQueryIngestionTimePartitioningProperties, BigQueryIntegerRangePartitioningProperties, BigQueryPartitioningProperties, BigQueryTimeUnitColumnPartitioningProperties, DefaultPartitioning. Details: "
                + ", ".join(error_messages)
            )
        else:
            return instance

    def to_json(self) -> str:
        """Returns the JSON representation of the actual instance"""
        if self.actual_instance is None:
            return "null"

        if hasattr(self.actual_instance, "to_json") and callable(self.actual_instance.to_json):
            return self.actual_instance.to_json()
        else:
            return json.dumps(self.actual_instance)

    def to_dict(
        self,
    ) -> Optional[
        Union[
            Dict[str, Any],
            BigQueryIngestionTimePartitioningProperties,
            BigQueryIntegerRangePartitioningProperties,
            BigQueryPartitioningProperties,
            BigQueryTimeUnitColumnPartitioningProperties,
            DefaultPartitioning,
        ]
    ]:
        """Returns the dict representation of the actual instance"""
        if self.actual_instance is None:
            return None

        if hasattr(self.actual_instance, "to_dict") and callable(self.actual_instance.to_dict):
            return self.actual_instance.to_dict()
        else:
            # primitive type
            return self.actual_instance

    def to_str(self) -> str:
        """Returns the string representation of the actual instance"""
        return pprint.pformat(self.model_dump())
