# coding: utf-8

"""
    Sifflet Backend API

    Requirements: <br>    - [Create your access token through the UI](https://docs.siffletdata.com/docs/generate-an-api-token#create-an-api-token) <br>    - Get your tenant name: if you access to Sifflet with `https://abcdef.siffletdata.com`, then your tenant would be `abcdef`

    The version of the OpenAPI document: 1.0.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from typing import Any, ClassVar, Dict, List, Optional, Set

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    StrictBool,
    StrictInt,
    StrictStr,
    field_validator,
)
from sifflet_sdk.client.models.tag_dto import TagDto
from sifflet_sdk.client.models.upstream_of_field_dto import UpstreamOfFieldDto
from typing_extensions import Self


class FieldDto(BaseModel):
    """
    FieldDto
    """  # noqa: E501

    created_by: Optional[StrictStr] = Field(default=None, alias="createdBy")
    created_date: Optional[StrictInt] = Field(default=None, alias="createdDate")
    dataset_id: Optional[StrictStr] = Field(default=None, alias="datasetId")
    dataset_name: Optional[StrictStr] = Field(default=None, alias="datasetName")
    datasource_name: Optional[StrictStr] = Field(default=None, alias="datasourceName")
    datasource_type: Optional[StrictStr] = Field(default=None, alias="datasourceType")
    dbt_description: Optional[StrictStr] = Field(default=None, alias="dbtDescription")
    ddl_comment: Optional[StrictStr] = Field(default=None, alias="ddlComment")
    default_value: Optional[StrictStr] = Field(default=None, alias="defaultValue")
    description: Optional[StrictStr] = None
    display_type: Optional[StrictStr] = Field(default=None, alias="displayType")
    entity_type: StrictStr = Field(alias="entityType")
    id: StrictStr
    last_modified_date: Optional[StrictInt] = Field(default=None, alias="lastModifiedDate")
    modified_by: Optional[StrictStr] = Field(default=None, alias="modifiedBy")
    name: Optional[StrictStr] = None
    nullable: Optional[StrictBool] = None
    parent_dataset_field_id: Optional[StrictStr] = Field(default=None, alias="parentDatasetFieldId")
    repeated: Optional[StrictBool] = None
    size: Optional[StrictInt] = None
    subfields: Optional[List[FieldDto]] = None
    tag_names: Optional[List[StrictStr]] = Field(default=None, alias="tagNames")
    tags: Optional[List[TagDto]] = None
    terms: Optional[List[TagDto]] = None
    type: Optional[StrictStr] = None
    upstream_field: Optional[UpstreamOfFieldDto] = Field(default=None, alias="upstreamField")
    __properties: ClassVar[List[str]] = [
        "createdBy",
        "createdDate",
        "datasetId",
        "datasetName",
        "datasourceName",
        "datasourceType",
        "dbtDescription",
        "ddlComment",
        "defaultValue",
        "description",
        "displayType",
        "entityType",
        "id",
        "lastModifiedDate",
        "modifiedBy",
        "name",
        "nullable",
        "parentDatasetFieldId",
        "repeated",
        "size",
        "subfields",
        "tagNames",
        "tags",
        "terms",
        "type",
        "upstreamField",
    ]

    @field_validator("entity_type")
    def entity_type_validate_enum(cls, value):
        """Validates the enum"""
        if value not in set(
            [
                "INTEGRATION",
                "DATASOURCE",
                "DATASOURCE_USAGE",
                "DATASOURCE_INGESTION_RUN",
                "DATASET",
                "DASHBOARD",
                "CHART",
                "COLLECTION",
                "DATASET_FIELD",
                "DAG",
                "DAG_RUN",
                "TRANSFORMATION",
                "TRANSFORMATION_RUN",
                "RULE_RUN",
                "RULE_EXECUTION_SUMMARY",
                "INCIDENT",
                "USER",
                "ACCESS_TOKEN",
                "SIFFLET_RULE",
                "CONFIG",
                "TAG",
                "DOMAIN",
                "ALERTING_HOOK",
                "RULE_MONITORING_RECOMMENDATION",
                "DATAPOINT_QUALIFICATION",
                "DECLARED_ASSET",
                "DECLARED_ASSET_PROPERTIES",
                "WEBHOOK",
                "SIFFLET_AGENT",
                "SIFFLET_AGENT_JOB",
                "SIFFLET_AGENT_DATASOURCE_JOB",
                "SIFFLET_AGENT_DEBUG_JOB",
                "AI_METADATA_PREDICTION",
                "CUSTOM_METADATA",
                "CUSTOM_METADATA_ENTRY",
                "DATA_PRODUCT",
                "METRIC",
                "ASSET_UPDATE_HISTORY_ENTITY",
                "ASSET_USAGE",
                "ASSET_USAGE_HISTORY_RECORD",
                "SAML_REQUEST",
                "SINGLE_USE_TOKEN",
                "FORMATTED_DESCRIPTION",
                "RULE_ROOT_CAUSE_ANALYSIS_RUN",
                "AS_CODE_WORKSPACE",
                "ALERT",
                "SIFFLET_RULE_MODEL_PARAMETERS",
                "CALENDAR",
                "CONNECTION",
                "METADATA_JOB",
                "DATASET_MODEL_PARAMETERS",
                "COLLABORATION_TOOL_ITEM",
                "DBT_IMPACT_ANALYSIS_RUN",
                "INGESTION_STATEMENT_CACHE",
                "LINEAGE",
                "MUTATION_LINK",
                "UPSTREAM_STATE",
            ]
        ):
            raise ValueError(
                "must be one of enum values ('INTEGRATION', 'DATASOURCE', 'DATASOURCE_USAGE', 'DATASOURCE_INGESTION_RUN', 'DATASET', 'DASHBOARD', 'CHART', 'COLLECTION', 'DATASET_FIELD', 'DAG', 'DAG_RUN', 'TRANSFORMATION', 'TRANSFORMATION_RUN', 'RULE_RUN', 'RULE_EXECUTION_SUMMARY', 'INCIDENT', 'USER', 'ACCESS_TOKEN', 'SIFFLET_RULE', 'CONFIG', 'TAG', 'DOMAIN', 'ALERTING_HOOK', 'RULE_MONITORING_RECOMMENDATION', 'DATAPOINT_QUALIFICATION', 'DECLARED_ASSET', 'DECLARED_ASSET_PROPERTIES', 'WEBHOOK', 'SIFFLET_AGENT', 'SIFFLET_AGENT_JOB', 'SIFFLET_AGENT_DATASOURCE_JOB', 'SIFFLET_AGENT_DEBUG_JOB', 'AI_METADATA_PREDICTION', 'CUSTOM_METADATA', 'CUSTOM_METADATA_ENTRY', 'DATA_PRODUCT', 'METRIC', 'ASSET_UPDATE_HISTORY_ENTITY', 'ASSET_USAGE', 'ASSET_USAGE_HISTORY_RECORD', 'SAML_REQUEST', 'SINGLE_USE_TOKEN', 'FORMATTED_DESCRIPTION', 'RULE_ROOT_CAUSE_ANALYSIS_RUN', 'AS_CODE_WORKSPACE', 'ALERT', 'SIFFLET_RULE_MODEL_PARAMETERS', 'CALENDAR', 'CONNECTION', 'METADATA_JOB', 'DATASET_MODEL_PARAMETERS', 'COLLABORATION_TOOL_ITEM', 'DBT_IMPACT_ANALYSIS_RUN', 'INGESTION_STATEMENT_CACHE', 'LINEAGE', 'MUTATION_LINK', 'UPSTREAM_STATE')"
            )
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of FieldDto from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of each item in subfields (list)
        _items = []
        if self.subfields:
            for _item_subfields in self.subfields:
                if _item_subfields:
                    _items.append(_item_subfields.to_dict())
            _dict["subfields"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in tags (list)
        _items = []
        if self.tags:
            for _item_tags in self.tags:
                if _item_tags:
                    _items.append(_item_tags.to_dict())
            _dict["tags"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in terms (list)
        _items = []
        if self.terms:
            for _item_terms in self.terms:
                if _item_terms:
                    _items.append(_item_terms.to_dict())
            _dict["terms"] = _items
        # override the default output from pydantic by calling `to_dict()` of upstream_field
        if self.upstream_field:
            _dict["upstreamField"] = self.upstream_field.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of FieldDto from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "createdBy": obj.get("createdBy"),
                "createdDate": obj.get("createdDate"),
                "datasetId": obj.get("datasetId"),
                "datasetName": obj.get("datasetName"),
                "datasourceName": obj.get("datasourceName"),
                "datasourceType": obj.get("datasourceType"),
                "dbtDescription": obj.get("dbtDescription"),
                "ddlComment": obj.get("ddlComment"),
                "defaultValue": obj.get("defaultValue"),
                "description": obj.get("description"),
                "displayType": obj.get("displayType"),
                "entityType": obj.get("entityType"),
                "id": obj.get("id"),
                "lastModifiedDate": obj.get("lastModifiedDate"),
                "modifiedBy": obj.get("modifiedBy"),
                "name": obj.get("name"),
                "nullable": obj.get("nullable"),
                "parentDatasetFieldId": obj.get("parentDatasetFieldId"),
                "repeated": obj.get("repeated"),
                "size": obj.get("size"),
                "subfields": (
                    [FieldDto.from_dict(_item) for _item in obj["subfields"]]
                    if obj.get("subfields") is not None
                    else None
                ),
                "tagNames": obj.get("tagNames"),
                "tags": [TagDto.from_dict(_item) for _item in obj["tags"]] if obj.get("tags") is not None else None,
                "terms": [TagDto.from_dict(_item) for _item in obj["terms"]] if obj.get("terms") is not None else None,
                "type": obj.get("type"),
                "upstreamField": (
                    UpstreamOfFieldDto.from_dict(obj["upstreamField"]) if obj.get("upstreamField") is not None else None
                ),
            }
        )
        return _obj


# TODO: Rewrite to not use raise_errors
FieldDto.model_rebuild(raise_errors=False)
