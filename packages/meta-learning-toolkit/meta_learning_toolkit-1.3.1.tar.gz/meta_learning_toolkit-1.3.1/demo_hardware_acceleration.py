#!/usr/bin/env python3
"""
Meta-Learning Hardware Acceleration Demo
========================================

Demonstrates modern hardware support including:
- NVIDIA GPU acceleration (RTX 4090, A100, H100)
- Apple Silicon MPS (M1/M2/M3/M4)
- Mixed precision training (FP16/BF16)
- Multi-GPU distributed training
- Optimal batch size detection
- Memory optimization

This demo shows how to use all meta-learning algorithms with hardware acceleration.

Author: Benedict Chen (benedict@benedictchen.com)
"""

import torch
import torch.nn as nn
import numpy as np
import time
from typing import Dict, Any

# Import meta-learning modules with hardware support
from src.meta_learning.meta_learning_modules import (
    # Hardware support
    HardwareManager, HardwareConfig, auto_device, log_hardware_info,
    get_optimal_batch_size, prepare_for_hardware,
    
    # Meta-learning algorithms
    TestTimeComputeScaler, TestTimeComputeConfig,
    PrototypicalLearner, PrototypicalConfig,
    MAML, MAMLConfig,
    ContinualMetaLearner, ContinualConfig
)


def create_sample_meta_learning_task():
    """Create sample data for hardware acceleration testing."""
    print("üéØ Creating sample meta-learning task...")
    
    # 5-way 3-shot classification task
    n_way, k_shot, query_shots = 5, 3, 12
    feature_dim = 64
    
    support_x = torch.randn(n_way, k_shot, feature_dim)
    support_y = torch.arange(n_way).repeat_interleave(k_shot)
    query_x = torch.randn(n_way * query_shots, feature_dim)
    query_y = torch.arange(n_way).repeat(query_shots)
    
    print(f"   Task: {n_way}-way {k_shot}-shot, {len(query_x)} queries")
    print(f"   Feature dim: {feature_dim}")
    
    return support_x, support_y, query_x, query_y


def demo_hardware_detection():
    """Demonstrate automatic hardware detection."""
    print("\nüîç Hardware Detection Demo")
    print("=" * 50)
    
    # Automatic hardware detection
    device = auto_device()
    print(f"üéØ Auto-detected device: {device}")
    
    # Detailed hardware info
    log_hardware_info()
    
    # Create hardware manager
    hw_manager = HardwareManager()
    
    # Get memory statistics
    memory_stats = hw_manager.get_memory_stats()
    print(f"\nüíæ Memory Statistics:")
    for key, value in memory_stats.items():
        if 'gb' in key.lower():
            print(f"   {key}: {value:.2f} GB")
        elif 'percent' in key.lower():
            print(f"   {key}: {value:.1f}%")
        else:
            print(f"   {key}: {value}")
    
    return hw_manager


def demo_prototypical_networks_with_hardware(hw_manager: HardwareManager):
    """Demo Prototypical Networks with hardware acceleration."""
    print("\nüé≤ Prototypical Networks + Hardware Acceleration")
    print("=" * 55)
    
    # Create encoder
    encoder = nn.Sequential(
        nn.Linear(64, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Linear(64, 32)
    )
    
    # Prepare model for hardware
    encoder = hw_manager.prepare_model(encoder)
    print(f"‚úÖ Model prepared for {hw_manager.device}")
    
    # Create prototypical learner with hardware-optimized config
    config = PrototypicalConfig(
        protonet_variant="research_accurate",
        use_uncertainty_aware_distances=True,
        use_temperature_scaling=True
    )
    learner = PrototypicalLearner(encoder, config)
    
    # Create and prepare data
    support_x, support_y, query_x, query_y = create_sample_meta_learning_task()
    support_x, support_y = hw_manager.prepare_data((support_x, support_y))
    query_x, query_y = hw_manager.prepare_data((query_x, query_y))
    
    # Benchmark with hardware acceleration
    print("üöÄ Running with hardware acceleration...")
    
    with hw_manager.autocast_context():
        start_time = time.time()
        
        # Forward pass
        logits = learner(support_x, support_y, query_x)
        predictions = torch.argmax(logits, dim=1)
        accuracy = (predictions == query_y).float().mean()
        
        elapsed = time.time() - start_time
        
    print(f"   ‚ö° Inference time: {elapsed*1000:.2f}ms")
    print(f"   üéØ Accuracy: {accuracy:.3f}")
    print(f"   üìä Output shape: {logits.shape}")
    
    return learner, accuracy


def demo_test_time_compute_with_mixed_precision(hw_manager: HardwareManager):
    """Demo Test-Time Compute Scaling with mixed precision."""
    print("\n‚ö° Test-Time Compute + Mixed Precision")
    print("=" * 45)
    
    # Create base model
    base_model = nn.Sequential(
        nn.Linear(64, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 5)  # 5-way classification
    )
    
    # Prepare for hardware
    base_model = hw_manager.prepare_model(base_model)
    
    # Create test-time compute scaler
    ttc_config = TestTimeComputeConfig(
        compute_strategy="hybrid",
        use_process_reward_model=True,
        use_test_time_training=True,
        max_compute_budget=20
    )
    
    scaler = TestTimeComputeScaler(base_model, ttc_config)
    
    # Prepare data
    support_x, support_y, query_x, query_y = create_sample_meta_learning_task()
    support_x, support_y = hw_manager.prepare_data((support_x, support_y))
    query_x, query_y = hw_manager.prepare_data((query_x, query_y))
    
    print(f"üéØ Mixed precision enabled: {hw_manager.config.use_mixed_precision}")
    print(f"üéØ Compute strategy: {ttc_config.compute_strategy}")
    
    with hw_manager.autocast_context():
        start_time = time.time()
        
        # Test-time compute scaling
        enhanced_logits, metrics = scaler.scale_compute(
            support_x, support_y, query_x
        )
        
        elapsed = time.time() - start_time
    
    print(f"   ‚ö° Scaling time: {elapsed*1000:.2f}ms")
    print(f"   üîß Compute used: {metrics.get('compute_used', 'N/A')}")
    print(f"   üìà Final confidence: {metrics.get('final_confidence', 0):.3f}")
    print(f"   üìä Output shape: {enhanced_logits.shape}")
    
    return enhanced_logits, metrics


def demo_multi_gpu_maml(hw_manager: HardwareManager):
    """Demo MAML with multi-GPU support (if available)."""
    print("\nüîó MAML + Multi-GPU Support")
    print("=" * 35)
    
    gpu_count = torch.cuda.device_count() if torch.cuda.is_available() else 0
    print(f"üéØ Available GPUs: {gpu_count}")
    
    # Create MAML model
    model = nn.Sequential(
        nn.Linear(64, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU(), 
        nn.Linear(64, 5)
    )
    
    # Prepare for hardware (with multi-GPU if available)
    if gpu_count > 1:
        hw_manager.config.use_data_parallel = True
    
    model = hw_manager.prepare_model(model)
    
    # Create MAML learner
    maml_config = MAMLConfig(
        inner_lr=0.01,
        inner_steps=3,
        use_higher_gradients=True
    )
    maml_learner = MAML(model, maml_config)
    
    # Create meta-batch
    meta_batch_size = 4
    support_x_batch = []
    support_y_batch = []
    query_x_batch = []
    query_y_batch = []
    
    for _ in range(meta_batch_size):
        support_x, support_y, query_x, query_y = create_sample_meta_learning_task()
        support_x, support_y = hw_manager.prepare_data((support_x, support_y))
        query_x, query_y = hw_manager.prepare_data((query_x, query_y))
        
        support_x_batch.append(support_x)
        support_y_batch.append(support_y)
        query_x_batch.append(query_x)
        query_y_batch.append(query_y)
    
    print(f"üéØ Meta-batch size: {meta_batch_size}")
    print(f"üéØ Using DataParallel: {hw_manager.config.use_data_parallel}")
    
    with hw_manager.autocast_context():
        start_time = time.time()
        
        # MAML meta-training step
        meta_loss = maml_learner.meta_train_step(
            support_x_batch, support_y_batch,
            query_x_batch, query_y_batch
        )
        
        elapsed = time.time() - start_time
    
    print(f"   ‚ö° Meta-training time: {elapsed*1000:.2f}ms")
    print(f"   üìâ Meta-loss: {meta_loss.item():.4f}")
    
    return meta_loss


def demo_optimal_batch_size():
    """Demo optimal batch size detection."""
    print("\nüìä Optimal Batch Size Detection")
    print("=" * 40)
    
    if not torch.cuda.is_available():
        print("   ‚ö†Ô∏è  CUDA not available, skipping batch size optimization")
        return 32
    
    # Create test model
    test_model = nn.Sequential(
        nn.Linear(64, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 5)
    )
    
    device = torch.device("cuda")
    input_shape = (1, 64)  # Batch size will be determined
    
    print("üîç Finding optimal batch size...")
    start_time = time.time()
    
    optimal_batch = get_optimal_batch_size(test_model, input_shape, device)
    
    elapsed = time.time() - start_time
    
    print(f"   üéØ Optimal batch size: {optimal_batch}")
    print(f"   ‚è±Ô∏è  Detection time: {elapsed:.2f}s")
    
    return optimal_batch


def demo_memory_optimization(hw_manager: HardwareManager):
    """Demo memory optimization features."""
    print("\nüíæ Memory Optimization Demo") 
    print("=" * 35)
    
    print("üìä Initial memory state:")
    initial_stats = hw_manager.get_memory_stats()
    for key, value in initial_stats.items():
        if 'memory' in key:
            print(f"   {key}: {value:.2f} GB")
    
    # Create large model for memory testing
    large_model = nn.Sequential(
        nn.Linear(512, 1024),
        nn.ReLU(),
        nn.Linear(1024, 1024),
        nn.ReLU(),
        nn.Linear(1024, 512),
        nn.ReLU(),
        nn.Linear(512, 5)
    )
    
    # Prepare with memory optimization
    hw_manager.config.memory_efficient = True
    large_model = hw_manager.prepare_model(large_model)
    
    # Create large data batch
    large_batch = torch.randn(128, 512)
    large_batch = hw_manager.prepare_data(large_batch)
    
    print("üß† Processing large batch...")
    
    with hw_manager.autocast_context():
        output = large_model(large_batch)
    
    print("üìä Memory after processing:")
    final_stats = hw_manager.get_memory_stats()
    for key, value in final_stats.items():
        if 'memory' in key:
            print(f"   {key}: {value:.2f} GB")
    
    # Clear cache
    hw_manager.clear_cache()
    print("üßπ Memory cache cleared")
    
    return output


def run_comprehensive_hardware_demo():
    """Run comprehensive hardware acceleration demo."""
    print("üöÄ Meta-Learning Hardware Acceleration Demo")
    print("=" * 60)
    print("Demonstrating modern hardware support for meta-learning algorithms")
    print("Supporting: NVIDIA GPUs, Apple Silicon MPS, Multi-GPU, Mixed Precision")
    print()
    
    # Hardware detection
    hw_manager = demo_hardware_detection()
    
    # Algorithm demos with hardware acceleration
    try:
        # Prototypical Networks
        learner, accuracy = demo_prototypical_networks_with_hardware(hw_manager)
        
        # Test-Time Compute with mixed precision
        enhanced_logits, ttc_metrics = demo_test_time_compute_with_mixed_precision(hw_manager)
        
        # MAML with multi-GPU
        meta_loss = demo_multi_gpu_maml(hw_manager)
        
        # Batch size optimization
        optimal_batch = demo_optimal_batch_size()
        
        # Memory optimization
        large_output = demo_memory_optimization(hw_manager)
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Demo error (expected on some hardware): {e}")
        print("   This is normal - not all features work on all hardware")
    
    # Final benchmark
    print("\nüèÅ Hardware Acceleration Summary")
    print("=" * 45)
    
    device_info = {
        "Device": str(hw_manager.device),
        "Mixed Precision": hw_manager.config.use_mixed_precision,
        "Memory Efficient": hw_manager.config.memory_efficient,
        "Data Parallel": hw_manager.config.use_data_parallel,
    }
    
    for key, value in device_info.items():
        print(f"   {key}: {value}")
    
    # Memory final state
    final_memory = hw_manager.get_memory_stats()
    if 'gpu_memory_allocated' in final_memory:
        print(f"   GPU Memory Used: {final_memory['gpu_memory_allocated']:.2f} GB")
    print(f"   CPU Memory Used: {final_memory['cpu_memory_used']:.2f} GB")
    
    print("\n‚úÖ Hardware acceleration demo completed successfully!")
    print("üéØ All meta-learning algorithms now support modern hardware")
    print("üöÄ Ready for production use with GPU acceleration")


if __name__ == "__main__":
    run_comprehensive_hardware_demo()