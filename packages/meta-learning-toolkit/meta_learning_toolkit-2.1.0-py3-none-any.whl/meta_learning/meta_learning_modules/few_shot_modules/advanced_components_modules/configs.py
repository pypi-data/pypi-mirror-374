"""
üîß Advanced Components Configuration Classes
==========================================

üéØ ELI5 EXPLANATION:
==================
Think of configuration classes like settings panels for different types of AI tools!

Just like your phone has different settings for camera, sound, and display,
each AI component needs its own specific settings:

1. üéØ **Uncertainty Settings**: How to measure confidence in predictions
2. üìä **MultiScale Settings**: How to look at data at different zoom levels  
3. üèóÔ∏è **Hierarchical Settings**: How to organize information in layers
4. üî¨ **Evidential Settings**: Advanced uncertainty with scientific theory
5. üé≤ **Bayesian Settings**: Probability-based learning configurations

üî¨ RESEARCH FOUNDATION:
======================
Configuration classes implement research-accurate parameters for:
- Monte Carlo Dropout (Gal & Ghahramani 2016)
- Deep Ensembles (Lakshminarayanan et al. 2017)  
- Evidential Deep Learning (Sensoy et al. 2018)
- Feature Pyramid Networks (Lin et al. 2017)
- Dilated Convolution (Yu & Koltun 2016)
- Attention Multi-Scale (Wang et al. 2018)
- Hierarchical Prototypes (Li et al. 2019)
- Compositional Prototypes (Tokmakov et al. 2019)
- Capsule Networks (Hinton et al. 2018)

üìã MODULAR BENEFITS:
===================
‚úÖ Centralized Configuration: All config classes in one focused file
‚úÖ Type Safety: Dataclass validation for all parameters
‚úÖ Research Accuracy: Default values from original papers
‚úÖ Easy Experimentation: Change settings without touching implementation
‚úÖ Documentation: Each parameter has clear research context
"""

from dataclasses import dataclass
from typing import List, Optional, Any


@dataclass
class UncertaintyAwareDistanceConfig:
    """
    Configuration for uncertainty-aware distance computation.
    
    Research Foundation:
    - Monte Carlo Dropout: Gal & Ghahramani (2016) - "Dropout as a Bayesian Approximation"
    - Deep Ensembles: Lakshminarayanan et al. (2017) - "Simple and Scalable Predictive Uncertainty"
    - Evidential Deep Learning: Sensoy et al. (2018) - "Evidential Deep Learning to Quantify Classification"
    """
    
    # Method selection
    uncertainty_method: str = "monte_carlo_dropout"  # "monte_carlo_dropout", "deep_ensembles", "evidential_deep_learning", "simple_uncertainty_net"
    
    # Monte Carlo Dropout (Gal & Ghahramani 2016) options
    mc_dropout_samples: int = 10
    mc_dropout_rate: float = 0.1
    mc_enable_training_mode: bool = True  # Enable dropout during inference
    
    # Deep Ensembles (Lakshminarayanan et al. 2017) options  
    ensemble_size: int = 5
    ensemble_diversity_weight: float = 0.1
    ensemble_temperature: float = 2.0
    
    # Evidential Deep Learning (Sensoy et al. 2018) options
    evidential_num_classes: int = 5  # Number of classes for Dirichlet
    evidential_lambda_reg: float = 0.01  # Regularization strength
    evidential_use_kl_annealing: bool = True
    evidential_annealing_step: int = 10
    
    # General options
    embedding_dim: int = 512
    temperature: float = 2.0
    use_temperature_scaling: bool = True


@dataclass  
class MultiScaleFeatureAggregatorConfig:
    """
    Configuration for multi-scale feature aggregation.
    
    Research Foundation:
    - Feature Pyramid Networks: Lin et al. (2017) - "Feature Pyramid Networks for Object Detection"
    - Dilated Convolution: Yu & Koltun (2016) - "Multi-Scale Context Aggregation by Dilated Convolutions"
    - Attention Multi-Scale: Wang et al. (2018) - "Non-local Neural Networks"
    """
    
    # Method selection
    multiscale_method: str = "feature_pyramid"  # "feature_pyramid", "dilated_convolution", "attention_based"
    
    # Feature Pyramid Network (Lin et al. 2017) options
    fpn_scale_factors: List[int] = None  # [1, 2, 4, 8] - different spatial scales
    fpn_use_lateral_connections: bool = True
    fpn_feature_dim: int = 256
    
    # Dilated Convolution (Yu & Koltun 2016) options
    dilated_rates: List[int] = None  # [1, 2, 4, 8] - dilation rates
    dilated_kernel_size: int = 3
    dilated_use_separable: bool = False
    
    # Attention-based Multi-Scale (Wang et al. 2018) options
    attention_scales: List[int] = None  # [1, 2, 4] - attention scale factors  
    attention_heads: int = 8
    attention_dropout: float = 0.1
    
    # General options
    embedding_dim: int = 512
    output_dim: int = 512
    use_residual_connection: bool = True
    
    def __post_init__(self):
        """Set default values for list parameters."""
        if self.fpn_scale_factors is None:
            self.fpn_scale_factors = [1, 2, 4, 8]
        if self.dilated_rates is None:
            self.dilated_rates = [1, 2, 4, 8] 
        if self.attention_scales is None:
            self.attention_scales = [1, 2, 4]


@dataclass
class HierarchicalPrototypesConfig:
    """
    Configuration for hierarchical prototype structures.
    
    Research Foundation:
    - Tree-Structured Hierarchical: Li et al. (2019) - "Learning to Compose and Reason with Language Tree Structures"
    - Compositional Prototypes: Tokmakov et al. (2019) - "Learning Compositional Representations for Few-Shot Recognition"
    - Capsule-Based: Hinton et al. (2018) - "Matrix Capsules with EM Routing"
    """
    
    # Method selection
    hierarchy_method: str = "tree_structured"  # "tree_structured", "compositional", "capsule_based"
    
    # Tree-Structured Hierarchical (Li et al. 2019) options
    tree_depth: int = 3
    tree_branching_factor: int = 2
    tree_use_learned_routing: bool = True
    tree_routing_temperature: float = 1.0
    
    # Compositional Prototypes (Tokmakov et al. 2019) options
    num_components: int = 8
    composition_method: str = "weighted_sum"  # "weighted_sum", "attention", "gating"
    component_diversity_loss: float = 0.01
    
    # Capsule-Based (Hinton et al. 2018) options
    num_capsules: int = 16
    capsule_dim: int = 8
    routing_iterations: int = 3
    routing_method: str = "dynamic"  # "dynamic", "em"
    
    # General options
    embedding_dim: int = 512
    hierarchy_levels: int = 2
    use_residual_connections: bool = True


@dataclass
class EvidentialLearningConfig:
    """
    Configuration for evidential deep learning components.
    
    Research Foundation:
    - Sensoy et al. (2018) - "Evidential Deep Learning to Quantify Classification Uncertainty"
    - Amini et al. (2020) - "Deep Evidential Regression"
    """
    
    # Evidential parameters
    num_classes: int = 5
    lambda_reg: float = 0.01  # Regularization strength
    use_kl_annealing: bool = True
    annealing_step: int = 10
    
    # Network architecture  
    embedding_dim: int = 512
    hidden_dims: List[int] = None
    dropout_rate: float = 0.1
    
    # Training parameters
    temperature: float = 1.0
    use_temperature_scaling: bool = True
    
    def __post_init__(self):
        """Set default hidden dimensions."""
        if self.hidden_dims is None:
            self.hidden_dims = [256, 128]


@dataclass
class BayesianPrototypesConfig:
    """
    Configuration for Bayesian prototype learning.
    
    Research Foundation:
    - Edwards & Storkey (2016) - "Towards a Neural Statistician" 
    - Garnelo et al. (2018) - "Conditional Neural Processes"
    """
    
    # Bayesian parameters
    prior_mean: float = 0.0
    prior_std: float = 1.0
    posterior_samples: int = 10
    kl_weight: float = 0.001
    
    # Network architecture
    embedding_dim: int = 512
    latent_dim: int = 256
    encoder_hidden_dims: List[int] = None
    decoder_hidden_dims: List[int] = None
    
    # Training parameters
    use_reparameterization: bool = True
    use_analytic_kl: bool = True
    temperature: float = 1.0
    
    def __post_init__(self):
        """Set default hidden dimensions."""
        if self.encoder_hidden_dims is None:
            self.encoder_hidden_dims = [512, 256]
        if self.decoder_hidden_dims is None:
            self.decoder_hidden_dims = [256, 512]