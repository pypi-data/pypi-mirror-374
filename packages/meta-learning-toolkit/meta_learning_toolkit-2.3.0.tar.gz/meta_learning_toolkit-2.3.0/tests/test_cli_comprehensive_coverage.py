"""
üí∞ SUPPORT THIS RESEARCH - PLEASE DONATE! üí∞

üôè If this library helps your research or project, please consider donating:
üí≥ PayPal: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
‚≠ê GitHub Sponsors: https://github.com/sponsors/benedictchen

üë®‚Äçüíª Author: Benedict Chen
üí∞ Donations: Help support this work! Buy me a coffee ‚òï, beer üç∫, lamborghini üèéÔ∏è, or private island üèùÔ∏è
üíñ Please consider recurring donations to fully support continued research

Your support enables cutting-edge AI research for everyone! üöÄ

Comprehensive CLI Coverage Tests
===============================

Tests for 100% CLI code coverage, including all edge cases and error conditions.
"""

import pytest
import torch
import torch.nn as nn
import argparse
import sys
import json
import subprocess
import tempfile
import os
from unittest.mock import patch, MagicMock

from meta_learning import cli
import meta_learning as ml


class TestCLIUtilities:
    """Test CLI utility functions."""
    
    def test_make_encoder_identity(self):
        """Test identity encoder creation."""
        encoder = cli.make_encoder("identity", out_dim=64, p_drop=0.1)
        assert isinstance(encoder, nn.Identity)
        
    def test_make_encoder_conv4(self):
        """Test Conv4 encoder creation."""
        encoder = cli.make_encoder("conv4", out_dim=128, p_drop=0.2)
        assert hasattr(encoder, 'forward')
        
    def test_make_encoder_invalid(self):
        """Test invalid encoder raises error."""
        with pytest.raises(ValueError, match="encoder must be"):
            cli.make_encoder("invalid_encoder")
    
    def test_device_auto_cuda_available(self):
        """Test auto device selection when CUDA available."""
        with patch('torch.cuda.is_available', return_value=True):
            device = cli._device("auto")
            assert device.type == "cuda"
    
    def test_device_auto_cuda_unavailable(self):
        """Test auto device selection when CUDA unavailable."""
        with patch('torch.cuda.is_available', return_value=False):
            device = cli._device("auto")
            assert device.type == "cpu"
    
    def test_device_explicit_cpu(self):
        """Test explicit CPU device selection."""
        device = cli._device("cpu")
        assert device.type == "cpu"
        
    def test_device_explicit_cuda(self):
        """Test explicit CUDA device selection."""
        device = cli._device("cuda")
        assert device.type == "cuda"


class TestCLIDatasetBuilding:
    """Test dataset building functionality."""
    
    def test_build_dataset_synthetic(self):
        """Test synthetic dataset creation."""
        args = argparse.Namespace(
            dataset="synthetic",
            encoder="identity",
            emb_dim=64,
            noise=0.1
        )
        dataset = cli._build_dataset(args)
        assert hasattr(dataset, 'sample_support_query')
        
    def test_build_dataset_synthetic_conv4(self):
        """Test synthetic dataset with Conv4 encoder."""
        args = argparse.Namespace(
            dataset="synthetic", 
            encoder="conv4",
            emb_dim=64,
            noise=0.05
        )
        dataset = cli._build_dataset(args)
        assert hasattr(dataset, 'sample_support_query')
    
    def test_build_dataset_cifar_fs(self):
        """Test CIFAR-FS dataset creation."""
        args = argparse.Namespace(
            dataset="cifar_fs",
            data_root="./data",
            split="val",
            manifest=None,
            download=False,
            image_size=32
        )
        # This might fail due to missing data, but tests the code path
        try:
            dataset = cli._build_dataset(args)
        except (FileNotFoundError, RuntimeError):
            # Expected when data not available
            pass
            
    def test_build_dataset_miniimagenet(self):
        """Test MiniImageNet dataset creation."""
        args = argparse.Namespace(
            dataset="miniimagenet",
            data_root="./data", 
            split="val",
            image_size=84
        )
        try:
            dataset = cli._build_dataset(args)
        except (FileNotFoundError, RuntimeError):
            # Expected when data not available
            pass
    
    def test_build_dataset_invalid(self):
        """Test invalid dataset raises error."""
        args = argparse.Namespace(dataset="invalid_dataset")
        with pytest.raises(ValueError, match="unknown dataset"):
            cli._build_dataset(args)


class TestCLIVersionCommand:
    """Test version command."""
    
    def test_cmd_version(self):
        """Test version command prints version."""
        with patch('builtins.print') as mock_print:
            cli.cmd_version(None)
            mock_print.assert_called_once_with(ml.__version__)


class TestCLIEvaluationEdgeCases:
    """Test evaluation command edge cases."""
    
    def test_eval_with_all_advanced_features(self):
        """Test eval with all advanced features enabled."""
        if not ml.INTEGRATED_ADVANCED_AVAILABLE:
            pytest.skip("Advanced features not available")
            
        # Create minimal test arguments
        args = argparse.Namespace(
            seed=42,
            device="cpu",
            distance="cosine",
            tau=2.0,
            prototype_shrinkage=0.2,
            uncertainty="monte_carlo_dropout",
            uncertainty_dropout=0.15,
            uncertainty_samples=5,
            encoder="identity",
            emb_dim=32,
            dropout=0.1,
            optimize_hardware=False,  # Keep on CPU for testing
            dataset="synthetic",
            noise=0.05,
            freeze_bn=True,
            check_leakage=True,
            n_way=3,
            k_shot=2,
            m_query=5,
            episodes=2,
            ttcs=3,
            outdir=None,
            dump_preds=False
        )
        
        # This should run without errors
        try:
            cli.cmd_eval(args)
        except SystemExit:
            # Expected from successful completion
            pass
    
    def test_eval_ttcs_scaling(self):
        """Test Test-Time Compute Scaling functionality."""
        args = argparse.Namespace(
            seed=123,
            device="cpu",
            distance="sqeuclidean", 
            tau=1.0,
            encoder="identity",
            emb_dim=16,
            dropout=0.0,
            dataset="synthetic",
            noise=0.1,
            freeze_bn=False,
            n_way=2,
            k_shot=1,
            m_query=3,
            episodes=1,
            ttcs=5,  # Multiple forward passes
            outdir=None,
            dump_preds=False
        )
        
        # Mock print to capture output
        with patch('builtins.print') as mock_print:
            try:
                cli.cmd_eval(args)
            except SystemExit:
                pass
    
    def test_eval_with_output_directory(self):
        """Test eval with output directory."""
        with tempfile.TemporaryDirectory() as temp_dir:
            args = argparse.Namespace(
                seed=456,
                device="cpu", 
                distance="sqeuclidean",
                tau=1.0,
                encoder="identity",
                emb_dim=8,
                dropout=0.0,
                dataset="synthetic",
                noise=0.1,
                freeze_bn=False,
                n_way=2,
                k_shot=1,
                m_query=2,
                episodes=1,
                ttcs=1,
                outdir=temp_dir,
                dump_preds=True
            )
            
            with patch('builtins.print'):
                try:
                    cli.cmd_eval(args)
                except SystemExit:
                    pass


class TestCLIBenchmarkCommand:
    """Test benchmark command functionality."""
    
    def test_cmd_bench_basic(self):
        """Test basic benchmark functionality."""
        args = argparse.Namespace(
            seed=789,
            device="cpu",
            distance="cosine",
            tau=0.5,
            encoder="identity",
            emb_dim=16,
            dropout=0.0,
            dataset="synthetic",
            noise=0.1,
            freeze_bn=True,
            n_way=3,
            k_shot=1,
            m_query=3,
            episodes=5,
            outdir=None
        )
        
        with patch('builtins.print') as mock_print:
            try:
                cli.cmd_bench(args)
            except SystemExit:
                pass
    
    def test_cmd_bench_with_output(self):
        """Test benchmark with output directory."""
        with tempfile.TemporaryDirectory() as temp_dir:
            args = argparse.Namespace(
                seed=999,
                device="cpu",
                distance="sqeuclidean",
                tau=1.0,
                encoder="conv4",
                emb_dim=32,
                dropout=0.1,
                dataset="synthetic",
                noise=0.05,
                freeze_bn=False,
                n_way=2,
                k_shot=2,
                m_query=4,
                episodes=3,
                outdir=temp_dir
            )
            
            with patch('builtins.print'):
                try:
                    cli.cmd_bench(args)
                except SystemExit:
                    pass


class TestCLIMainFunction:
    """Test main CLI entry point."""
    
    def test_main_version(self):
        """Test main with version command."""
        with patch('builtins.print') as mock_print:
            result = cli.main(["version"])
            # Version command should print and return
            mock_print.assert_called()
    
    def test_main_eval_minimal(self):
        """Test main with minimal eval arguments."""
        with patch('builtins.print'):
            # Should complete successfully 
            result = cli.main([
                "eval", 
                "--episodes", "1",
                "--n-way", "2",
                "--k-shot", "1",
                "--m-query", "2"
            ])
    
    def test_main_bench_minimal(self):
        """Test main with minimal bench arguments."""
        with patch('builtins.print'):
            # Should complete successfully
            result = cli.main([
                "bench",
                "--episodes", "1", 
                "--n-way", "2",
                "--k-shot", "1",
                "--m-query", "2"
            ])
    
    def test_main_no_command(self):
        """Test main with no command raises error."""
        with pytest.raises(SystemExit):
            cli.main([])
    
    def test_main_invalid_command(self):
        """Test main with invalid command raises error."""
        with pytest.raises(SystemExit):
            cli.main(["invalid_command"])


class TestCLIArgumentParsing:
    """Test CLI argument parsing edge cases."""
    
    def test_eval_parser_all_arguments(self):
        """Test evaluation parser with all possible arguments."""
        parser = argparse.ArgumentParser()
        subparsers = parser.add_subparsers(dest="cmd", required=True)
        
        # Recreate the eval parser setup from cli.py
        pe = subparsers.add_parser("eval")
        pe.add_argument("--dataset", choices=["synthetic","cifar_fs","miniimagenet"], default="synthetic")
        pe.add_argument("--split", choices=["train","val","test"], default="val")
        pe.add_argument("--n-way", type=int, default=5)
        pe.add_argument("--k-shot", type=int, default=1)
        pe.add_argument("--m-query", type=int, default=15)
        pe.add_argument("--episodes", type=int, default=200)
        pe.add_argument("--encoder", choices=["identity","conv4"], default="identity")
        pe.add_argument("--emb-dim", type=int, default=64)
        pe.add_argument("--dropout", type=float, default=0.0)
        pe.add_argument("--distance", choices=["sqeuclidean","cosine"], default="sqeuclidean")
        pe.add_argument("--tau", type=float, default=1.0)
        pe.add_argument("--noise", type=float, default=0.1)
        pe.add_argument("--device", choices=["auto","cpu","cuda"], default="auto")
        pe.add_argument("--seed", type=int, default=1234)
        pe.add_argument("--ttcs", type=int, default=1)
        
        # Test parsing with all arguments
        args = parser.parse_args([
            "eval",
            "--dataset", "cifar_fs",
            "--split", "test", 
            "--n-way", "10",
            "--k-shot", "5",
            "--m-query", "20",
            "--episodes", "100",
            "--encoder", "conv4",
            "--emb-dim", "128",
            "--dropout", "0.2",
            "--distance", "cosine",
            "--tau", "2.0",
            "--noise", "0.05",
            "--device", "cuda",
            "--seed", "9999",
            "--ttcs", "10"
        ])
        
        assert args.cmd == "eval"
        assert args.dataset == "cifar_fs"
        assert args.n_way == 10
        assert args.ttcs == 10