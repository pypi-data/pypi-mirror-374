"""
rovibrational_excitation/simulation/runner.py
============================================
ãƒ»ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ sweep â†’ é€æ¬¡ï¼ä¸¦åˆ—å®Ÿè¡Œ
ãƒ»çµæœã‚’ results/<timestamp>_<desc>/â€¦ ã«ä¿å­˜
ãƒ»JSON å¤‰æ›å®‰å…¨åŒ–ï¼é€²æ—ãƒãƒ¼ï¼npz åœ§ç¸®ãªã©æ”¹å–„
ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ»å¾©æ—§æ©Ÿèƒ½è¿½åŠ 

ä¾å­˜ï¼š
    numpy, pandas, (tqdm ã¯ä»»æ„)
"""

from __future__ import annotations

import hashlib
import importlib.util
import itertools
import json
import shutil
import time
import traceback
import types
from collections.abc import Mapping
from datetime import datetime
from multiprocessing import Pool, cpu_count
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

# Unit conversion utilities
from rovibrational_excitation.core.units.parameter_processor import parameter_processor

try:
    from tqdm import tqdm as _tqdm_impl

    def _tqdm(x, **k):  # type: ignore
        return _tqdm_impl(x, **k)
except ImportError:  # é€²æ—ãƒãƒ¼ãŒç„¡ãã¦ã‚‚å‹•ã

    def _tqdm(x, **k):  # type: ignore
        return x


# ---------------------------------------------------------------------
# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹
# ---------------------------------------------------------------------
class CheckpointManager:
    """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã®é€²æ—ã‚’ç®¡ç†ã—ã€é€”ä¸­ã‹ã‚‰å†é–‹å¯èƒ½ã«ã™ã‚‹"""

    def __init__(self, root_dir: Path):
        self.root_dir = root_dir
        self.checkpoint_file = root_dir / "checkpoint.json"
        self.failed_cases_file = root_dir / "failed_cases.json"

    def save_checkpoint(
        self,
        completed_cases: list[dict[str, Any]],
        failed_cases: list[dict[str, Any]],
        total_cases: int,
        start_time: float,
    ):
        """é€²æ—ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        checkpoint_data = {
            "timestamp": datetime.now().isoformat(),
            "start_time": start_time,
            "total_cases": total_cases,
            "completed_cases": len(completed_cases),
            "failed_cases": len(failed_cases),
            "completed_case_hashes": [
                self._case_hash(case) for case in completed_cases
            ],
            "failed_case_data": failed_cases,
        }

        with open(self.checkpoint_file, "w") as f:
            json.dump(_json_safe(checkpoint_data), f, indent=2)

        print(f"âœ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {len(completed_cases)}/{total_cases} å®Œäº†")

    def load_checkpoint(self) -> dict[str, Any] | None:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é€²æ—ã‚’èª­ã¿è¾¼ã¿"""
        if not self.checkpoint_file.exists():
            return None

        try:
            with open(self.checkpoint_file) as f:
                return json.load(f)
        except Exception as e:
            print(f"âš  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return None

    def _case_hash(self, case: dict[str, Any]) -> str:
        """ã‚±ãƒ¼ã‚¹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        # outdirã¨saveã‚’é™¤ã„ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        case_copy = {k: v for k, v in case.items() if k not in ["outdir", "save"]}
        case_str = json.dumps(_json_safe(case_copy), sort_keys=True)
        return hashlib.md5(case_str.encode()).hexdigest()

    def filter_remaining_cases(
        self, all_cases: list[dict[str, Any]]
    ) -> list[dict[str, Any]]:
        """å®Œäº†æ¸ˆã¿ã‚±ãƒ¼ã‚¹ã‚’é™¤ã„ãŸæ®‹ã‚Šã®ã‚±ãƒ¼ã‚¹ã‚’è¿”ã™"""
        checkpoint = self.load_checkpoint()
        if checkpoint is None:
            return all_cases

        completed_hashes = set(checkpoint.get("completed_case_hashes", []))
        remaining_cases = []

        for case in all_cases:
            case_hash = self._case_hash(case)
            if case_hash not in completed_hashes:
                remaining_cases.append(case)

        return remaining_cases

    def is_resumable(self) -> bool:
        """å†é–‹å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.checkpoint_file.exists()


# ---------------------------------------------------------------------
# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãå®Ÿè¡Œé–¢æ•°
# ---------------------------------------------------------------------
def _run_one_safe(
    params: dict[str, Any], max_retries: int = 2
) -> tuple[np.ndarray | None, str | None]:
    """
    1ã‚±ãƒ¼ã‚¹å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰

    Returns:
        (result, error_message): æˆåŠŸæ™‚ã¯(result, None)ã€å¤±æ•—æ™‚ã¯(None, error_message)
    """
    for attempt in range(max_retries + 1):
        try:
            result = _run_one(params)
            return result, None

        except Exception as e:
            error_msg = f"Attempt {attempt + 1}/{max_retries + 1} failed: {str(e)}"
            if attempt < max_retries:
                print(f"âš  {error_msg} (å†è©¦è¡Œä¸­...)")
                time.sleep(2**attempt)  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            else:
                full_error = f"{error_msg}\nTraceback:\n{traceback.format_exc()}"
                print(f"âœ— ã‚±ãƒ¼ã‚¹å¤±æ•—: {full_error}")

                # å¤±æ•—ã‚±ãƒ¼ã‚¹ã®æƒ…å ±ã‚’ä¿å­˜
                if params.get("save", True):
                    outdir = Path(params["outdir"])
                    outdir.mkdir(parents=True, exist_ok=True)
                    with open(outdir / "error.txt", "w") as f:
                        f.write(full_error)
                        f.write(
                            f"\nParameters:\n{json.dumps(_json_safe(params), indent=2)}"
                        )

                return None, full_error

    # ã“ã®è¡Œã«åˆ°é”ã™ã‚‹ã“ã¨ã¯ãªã„ãŒã€å‹ãƒã‚§ãƒƒã‚«ãƒ¼ã®ãŸã‚
    return None, "Unknown error"


def _parallel_run_safe(
    case_list: list[dict[str, Any]],
) -> list[tuple[np.ndarray | None, str | None]]:
    """ä¸¦åˆ—å®Ÿè¡Œç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
    return [_run_one_safe(case) for case in case_list]


# ---------------------------------------------------------------------
# JSONå¤‰æ›ã®å®‰å…¨åŒ–
# è¤‡ç´ æ•°ã«å¯¾å¿œã—ã¦ã„ãªã„ã®ã§ã€å®Ÿéƒ¨ãƒ»è™šéƒ¨ã‚’åˆ†ã‘ã¦è¾æ›¸åŒ–
# list, tuple, np.ndarrayãªã©ã‚‚å†å¸°çš„ã«å¤‰æ›
# ---------------------------------------------------------------------
def _json_safe(obj: Any) -> Any:
    """complex / ndarray ãªã©ã‚’ JSON å¯èƒ½ã¸å†å¸°å¤‰æ›"""
    if isinstance(obj, complex):
        return {"__complex__": True, "r": obj.real, "i": obj.imag}

    if callable(obj):  # é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»ã‚¯ãƒ©ã‚¹ç­‰
        return f"{getattr(obj, '__module__', 'builtins')}.{getattr(obj, '__qualname__', str(obj))}"

    if isinstance(obj, types.ModuleType):
        return obj.__name__

    if isinstance(obj, np.generic):
        return obj.item()  # np.float64 â†’ float ãªã©

    if isinstance(obj, np.ndarray):
        return [_json_safe(v) for v in obj.tolist()]

    if isinstance(obj, list | tuple):
        return [_json_safe(v) for v in obj]

    if isinstance(obj, dict):
        return {k: _json_safe(v) for k, v in obj.items()}

    return obj  # str, int, float, bool, None ãªã©ã¯ãã®ã¾ã¾


# ---------------------------------------------------------------------
# polarization dict â‡„ complex array
# ---------------------------------------------------------------------
def _deserialize_pol(seq) -> np.ndarray:
    def to_complex(d):
        if isinstance(d, dict):
            r = d.get("r", d.get("real", 0))
            i = d.get("i", d.get("imag", 0))
            return complex(r, i)
        elif isinstance(d, float | int | complex):
            return complex(d)
        else:
            raise TypeError(f"Invalid type in polarization sequence: {type(d)}")

    # seqãŒå˜ä¸€ã®å€¤ã®å ´åˆ
    if isinstance(seq, int | float | complex | dict):
        return np.array([to_complex(seq), 0], dtype=complex)

    # seqãŒiterableã®å ´åˆ
    if hasattr(seq, "__iter__") and not isinstance(seq, str | bytes):
        seq_list = list(seq)
        if len(seq_list) == 1:
            # [x] â†’ [x, 0]
            return np.array([to_complex(seq_list[0]), 0], dtype=complex)
        elif len(seq_list) == 2:
            # [x, y] â†’ [x, y]
            return np.array([to_complex(d) for d in seq_list], dtype=complex)
        else:
            raise ValueError(
                f"Polarization must have 1 or 2 elements, got {len(seq_list)}"
            )

    # ä½•ã‚‚è©²å½“ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    return np.array([1.0, 0.0], dtype=complex)


# ---------------------------------------------------------------------
# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
# ---------------------------------------------------------------------
def _load_params_file(path: str) -> dict[str, Any]:
    spec = importlib.util.spec_from_file_location("params", path)
    if spec is None or spec.loader is None:
        raise ImportError(f"Cannot load spec from {path}")
    mod = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(mod)  # type: ignore[arg-type]
    params = {k: getattr(mod, k) for k in dir(mod) if not k.startswith("__")}
    
    # Apply default units and automatic conversion
    print(f"ğŸ“Š Loading parameters from {path}")
    
    # First, apply default units for parameters without explicit units
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå˜ä½ã‚’é©ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¸€æ™‚çš„ã«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
    params_with_defaults = params.copy()
    
    # Then convert all units to standard internal units
    converted_params = parameter_processor.auto_convert_parameters(params_with_defaults)
    
    if params != converted_params:
        print("ğŸ“‹ Unit processing completed.")
    else:
        print("ğŸ“‹ No unit processing needed.")
    
    return converted_params


# ---------------------------------------------------------------------
# sweep ã™ã‚‹ / ã—ãªã„å¤‰æ•°ã‚’åˆ†é›¢ã—ã¦ãƒ‡ãƒ¼ã‚¿ç‚¹å±•é–‹
# ---------------------------------------------------------------------

# å¸¸ã«å›ºå®šå€¤ã¨ã—ã¦æ‰±ã†ã‚­ãƒ¼ï¼ˆåå…‰ãƒ™ã‚¯ãƒˆãƒ«ãªã©ï¼‰
FIXED_VALUE_KEYS = {
    "polarization",
    "initial_states",
    "envelope_func",
}


def _expand_cases(base: dict[str, Any]):
    """
    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸ã‚’ã‚±ãƒ¼ã‚¹å±•é–‹ã™ã‚‹ã€‚

    ã‚¹ã‚¤ãƒ¼ãƒ—åˆ¤å®šãƒ«ãƒ¼ãƒ«:
    1. `_sweep` æ¥å°¾è¾ãŒã‚ã‚‹ã‚­ãƒ¼ â†’ æ˜ç¤ºçš„ã«ã‚¹ã‚¤ãƒ¼ãƒ—å¯¾è±¡
    2. FIXED_VALUE_KEYS ã«å«ã¾ã‚Œã‚‹ã‚­ãƒ¼ â†’ å¸¸ã«å›ºå®šå€¤
    3. ãã®ä»–ã®iterableã§é•·ã•>1 â†’ ã‚¹ã‚¤ãƒ¼ãƒ—å¯¾è±¡
    """
    sweep_keys: list[str] = []
    static: dict[str, Any] = {}
    # _sweepæ¥å°¾è¾ã®ã‚­ãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚° (base_key -> original_key)
    sweep_keys_mapping: dict[str, str] = {}

    for k, v in base.items():
        # æ–‡å­—åˆ—ã‚„ãƒã‚¤ãƒˆåˆ—ã¯å¸¸ã«å›ºå®šå€¤
        if isinstance(v, str | bytes):
            static[k] = v
            continue

        # `_sweep` æ¥å°¾è¾ãŒã‚ã‚‹å ´åˆã¯æ˜ç¤ºçš„ã«ã‚¹ã‚¤ãƒ¼ãƒ—å¯¾è±¡
        if k.endswith("_sweep"):
            if hasattr(v, "__iter__"):
                try:
                    if len(v) > 0:  # ç©ºã§ãªã‘ã‚Œã°ã‚¹ã‚¤ãƒ¼ãƒ—å¯¾è±¡
                        # æ¥å°¾è¾ã‚’å–ã‚Šé™¤ã„ãŸåå‰ã‚’ã‚¹ã‚¤ãƒ¼ãƒ—ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
                        base_key = k[:-6]  # "_sweep"ã‚’é™¤å»
                        sweep_keys.append(base_key)
                        sweep_keys_mapping[base_key] = k
                        continue
                except TypeError:
                    pass
            # `_sweep` æ¥å°¾è¾ã ãŒiterableã§ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
            raise ValueError(f"Parameter '{k}' has '_sweep' suffix but is not iterable")

        # ç‰¹åˆ¥ãªã‚­ãƒ¼ã¯å¸¸ã«å›ºå®šå€¤ã¨ã—ã¦æ‰±ã†
        if k in FIXED_VALUE_KEYS:
            static[k] = v
            continue

        # ãã®ä»–ã®iterableã¯ sweep ã¨ã—ã¦æ‰±ã†ï¼ˆé•·ã•1ã§ã‚‚ã‚¹ã‚«ãƒ©ãƒ¼åŒ–ã®ãŸã‚å±•é–‹å¯¾è±¡ï¼‰
        if hasattr(v, "__iter__"):
            try:
                if len(v) >= 1:
                    sweep_keys.append(k)
                    continue
            except TypeError:  # len() ä¸å¯ (ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ç­‰)
                pass

        # ä¸Šè¨˜ã«è©²å½“ã—ãªã„å ´åˆã¯å›ºå®šå€¤
        static[k] = v

    if not sweep_keys:  # sweep ç„¡ã— â†’ 1 ã‚±ãƒ¼ã‚¹ã®ã¿
        yield static, []
        return

    # å„ã‚¹ã‚¤ãƒ¼ãƒ—ã‚­ãƒ¼ã«å¯¾å¿œã™ã‚‹å€¤ã®iterableã‚’å–å¾—
    iterables = []
    for key in sweep_keys:
        if key in sweep_keys_mapping:
            # _sweepæ¥å°¾è¾ã‚­ãƒ¼ã®å ´åˆã¯å…ƒã®ã‚­ãƒ¼ã‹ã‚‰å€¤ã‚’å–å¾—
            original_key = sweep_keys_mapping[key]
            iterables.append(base[original_key])
        else:
            # é€šå¸¸ã®ã‚­ãƒ¼ã®å ´åˆ
            iterables.append(base[key])

    for combo in itertools.product(*iterables):
        d = static.copy()
        d.update(dict(zip(sweep_keys, combo)))
        yield d, sweep_keys


# --- ãƒ©ãƒ™ãƒ«æ•´å½¢ -------------------------------------------
def _label(v: Any) -> str:
    if isinstance(v, int | float):
        return f"{v:g}"
    return str(v).replace(" ", "").replace("\n", "")


# ---------------------------------------------------------------------
# çµæœãƒ«ãƒ¼ãƒˆä½œæˆ
# ---------------------------------------------------------------------
def _make_root(desc: str) -> Path:
    root = Path("results") / f"{datetime.now():%Y%m%d_%H%M%S}_{desc}"
    root.mkdir(parents=True, exist_ok=True)
    return root


# ---------------------------------------------------------------------
# 1 ã‚±ãƒ¼ã‚¹å®Ÿè¡Œ
# ---------------------------------------------------------------------
def _run_one(params: dict[str, Any]) -> np.ndarray:
    """
    1 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Ÿè¡Œã— population(t) ã‚’è¿”ã™ã€‚
    ç³»ã‚¿ã‚¤ãƒ—ï¼ˆbasis_typeï¼‰ã«å¿œã˜ã¦æ±ç”¨çš„ã«å¯¾å¿œ
    """
    # --- å¿…è¦ãªimportã¯é–¢æ•°å†…ã§ ---
    from rovibrational_excitation.core.electric_field import (
        ElectricField,
        gaussian_fwhm,
    )
    from rovibrational_excitation.core.propagation.schrodinger import SchrodingerPropagator
    from rovibrational_excitation.core.basis import StateVector
    from rovibrational_excitation.core.nondimensional.analysis import analyze_regime

    # --- Electric field å…±é€š ---
    t_E = np.arange(params["t_start"], params["t_end"] + params["dt"], params["dt"])
    E = ElectricField(tlist=t_E)
    E.add_dispersed_Efield(
        envelope_func=params.get("envelope_func", gaussian_fwhm),
        duration=params.get("duration", params.get("pulse_duration", params['t_end']/2)),
        t_center=params.get("t_center", 0.0),
        carrier_freq=params["carrier_freq"],
        amplitude=params["amplitude"],
        polarization=_deserialize_pol(params["polarization"]),
        gdd=params.get("gdd", 0.0),
        tod=params.get("tod", 0.0),
    )
    if params.get("Sinusoidal_modulation", False):
        E.apply_sinusoidal_mod(
            center_freq=params["carrier_freq"],
            amplitude=params["amplitude_sin_mod"],
            carrier_freq=params["carrier_freq_sin_mod"],
            phase_rad=params.get("phase_rad_sin_mod", 0.0),
            type_mod=params.get("type_mod_sin_mod", "phase"),
        )

    # --- ç³»ã‚¿ã‚¤ãƒ—åˆ†å² ---
    basis_type = params.get("basis_type", "linmol").lower()
    if basis_type == "linmol":
        from rovibrational_excitation.core.basis import LinMolBasis
        from rovibrational_excitation.dipole.linmol import LinMolDipoleMatrix
        from rovibrational_excitation.dipole.vib.morse import omega01_domega_to_N
        # Basis
        basis = LinMolBasis(
            params["V_max"],
            params["J_max"],
            use_M=params.get("use_M", True),
            omega=params["omega_rad_phz"],
            delta_omega=params.get("delta_omega_rad_phz", 0.0),
            B=params.get("B_rad_phz", 0.0),
            alpha=params.get("alpha_rad_phz", 0.0),
            output_units="J",
            input_units="rad/fs",
        )
        # åˆæœŸçŠ¶æ…‹
        sv = StateVector(basis)
        for idx in params.get("initial_states", [0]):
            sv.set_state(basis.get_state(idx), 1)
        sv.normalize()
        # Hamiltonian
        delta_omega_rad_phz = params.get("delta_omega_rad_phz", 0.0)
        potential_type = params.get("potential_type", "harmonic")
        if delta_omega_rad_phz == 0.0:
            params.update({"potential_type": "harmonic"})
        if potential_type == "morse":
            omega01_domega_to_N(params["omega_rad_phz"], delta_omega_rad_phz)
        H0 = basis.generate_H0()
            
        
        # Dipole
        dip = LinMolDipoleMatrix(
            basis,
            mu0=params["mu0_Cm"],
            potential_type=params.get("potential_type", "harmonic"),
            backend=params.get("backend", "numpy"),
            dense=params.get("dense", True),
        )
    elif basis_type == "twolevel":
        from rovibrational_excitation.core.basis import TwoLevelBasis
        from rovibrational_excitation.dipole.twolevel import TwoLevelDipoleMatrix
        # Basisï¼ˆenergy_gap ã¨å˜ä½ã¯åˆæœŸåŒ–ã§æŒ‡å®šï¼‰
        basis = TwoLevelBasis(
            energy_gap=params.get("energy_gap", 1.0),
            input_units=params.get("energy_gap_units", "rad/fs"),
            output_units="J",
        )
        # åˆæœŸçŠ¶æ…‹
        sv = StateVector(basis)
        for idx in params.get("initial_states", [0]):
            sv.set_state(basis.get_state(idx), 1)
        sv.normalize()
        # Hamiltonianï¼ˆåŸºåº•ã«è¨­å®šæ¸ˆã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ç”Ÿæˆï¼‰
        H0 = basis.generate_H0()
        # Dipole
        dip = TwoLevelDipoleMatrix(
            basis,
            mu0=params.get("mu0_Cm", params.get("mu0", 1.0)),
            backend=params.get("backend", "numpy"),
        )
    elif basis_type == "vibladder":
        from rovibrational_excitation.core.basis import VibLadderBasis
        from rovibrational_excitation.dipole.viblad import VibLadderDipoleMatrix
        # Basis
        basis = VibLadderBasis(
            params["V_max"],
            omega=params["omega_rad_phz"],
            delta_omega=params.get("delta_omega_rad_phz", 0.0),
        )
        # åˆæœŸçŠ¶æ…‹
        sv = StateVector(basis)
        for idx in params.get("initial_states", [0]):
            sv.set_state(basis.get_state(idx), 1)
        sv.normalize()
        # Hamiltonian
        H0 = basis.generate_H0()
        # Dipole
        dip = VibLadderDipoleMatrix(
            basis,
            mu0=params.get("mu0_Cm", params.get("mu0", 1.0)),
            potential_type=params.get("potential_type", "harmonic"),
            backend=params.get("backend", "numpy"),
        )
    else:
        raise ValueError(f"Unknown basis_type: {basis_type}")

    # ---------- Propagation å…±é€š ----------
    use_nondimensional = params.get("nondimensional", False)
    prop = SchrodingerPropagator()
    psi_t = prop.propagate(
        hamiltonian=H0,
        efield=E,
        dipole_matrix=dip,
        initial_state=sv.data,
        axes=params.get("axes", "xy"),
        return_traj=params.get("return_traj", True),
        return_time_psi=params.get("return_time_psi", True),
        backend=params.get("backend", "numpy"),
        sample_stride=params.get("sample_stride", 1),
        nondimensional=use_nondimensional,
    )

    # ç„¡æ¬¡å…ƒåŒ–ä½¿ç”¨æ™‚ã¯ç‰©ç†ãƒ¬ã‚¸ãƒ¼ãƒ æƒ…å ±ã‚‚ä¿å­˜
    regime_info = None
    if use_nondimensional:
        try:
            from rovibrational_excitation.core.nondimensional.converter import nondimensionalize_system
            # mu_x, mu_yå–å¾—ã¯dipã®å±æ€§ã§åˆ†å²
            mu_x = getattr(dip, "mu_x", None)
            mu_y = getattr(dip, "mu_y", None)
            if mu_x is None or mu_y is None:
                # VibLadderç­‰zã®ã¿ã®å ´åˆã¯zã‚’ä½¿ã†
                mu_x = getattr(dip, "mu_x", getattr(dip, "mu_z", None))
                mu_y = getattr(dip, "mu_y", getattr(dip, "mu_z", None))
            if mu_x is None or mu_y is None:
                raise ValueError("Dipole matrix must have mu_x and mu_y or mu_z attributes for nondimensionalization.")
            _, _, _, _, _, _, scales = nondimensionalize_system(
                H0.get_matrix(units="J"), mu_x, mu_y, E,
                H0_units="energy", time_units="fs"
            )
            regime_info = analyze_regime(scales)
        except Exception as e:
            print(f"Warning: Could not analyze regime: {e}")
            regime_info = {"error": str(e)}
    if isinstance(psi_t, (list, tuple)) and len(psi_t) == 2:
        t_p, psi_t = psi_t
    else:
        t_p = np.array([0.0])  # dummy

    pop_t = np.abs(psi_t) ** 2  # ideally (t, dim)
    # Ensure shape is always (t, dim)
    if isinstance(pop_t, np.ndarray):
        if pop_t.ndim == 0:
            pop_t = np.array([[float(pop_t)]], dtype=float)
        elif pop_t.ndim == 1:
            pop_t = pop_t.reshape(-1, 1)

    # ---------- Save (npz åœ§ç¸®) å…±é€š ----------
    if params.get("save", True):
        outdir = Path(params["outdir"])
        save_data = {
            "t_E": t_E,
            "psi": psi_t,
            "pop": pop_t,
            "E": np.array(E.Efield),
            "t_p": t_p,
        }
        if regime_info is not None:
            save_data["regime_info"] = regime_info
        np.savez_compressed(outdir / "result.npz", **save_data)
        with open(outdir / "parameters.json", "w") as f:
            json.dump(_json_safe(params), f, indent=2)
        if regime_info is not None:
            with open(outdir / "regime_analysis.json", "w") as f:
                json.dump(_json_safe(regime_info), f, indent=2)

    return pop_t


# ---------------------------------------------------------------------
# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä»˜ããƒãƒƒãƒå®Ÿè¡Œ
# ---------------------------------------------------------------------
def run_all_with_checkpoint(
    params: str | Mapping[str, Any],
    *,
    nproc: int | None = None,
    save: bool = True,
    dry_run: bool = False,
    checkpoint_interval: int = 10,
) -> list[Any]:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ä»˜ãã®ãƒãƒƒãƒå®Ÿè¡Œ"""

    # ---------- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ---------------------------------
    if isinstance(params, str):
        base_dict = _load_params_file(params)
        description = base_dict.get("description", Path(params).stem)
        param_file_path = Path(params)
    elif isinstance(params, Mapping):
        print("ğŸ“Š Loading parameters from dict")
        raw_dict = dict(params)
        
        # Apply default units and automatic conversion
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå˜ä½ã‚’é©ç”¨ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¸€æ™‚çš„ã«ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        dict_with_defaults = raw_dict.copy()
        base_dict = parameter_processor.auto_convert_parameters(dict_with_defaults)
        
        if raw_dict != base_dict:
            print("ğŸ“‹ Unit processing completed.")
        else:
            print("ğŸ“‹ No unit processing needed.")
        description = base_dict.get("description", "run")
        param_file_path = None
    else:
        raise TypeError("params must be filepath str or dict-like")

    # ---------- ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª ---------------------------------
    root = _make_root(description) if save else None
    if save and root is not None and param_file_path is not None:
        shutil.copy(param_file_path, root / "params.py")

    # ---------- ã‚±ãƒ¼ã‚¹å±•é–‹ -----------------------------------------
    cases: list[dict[str, Any]] = []
    for case, sweep_keys in _expand_cases(base_dict):
        case["save"] = save
        if save and root is not None:
            rel = Path(*[f"{k}_{_label(case[k])}" for k in sweep_keys])
            outdir = root / rel
            outdir.mkdir(parents=True, exist_ok=True)
            case["outdir"] = str(outdir)
        cases.append(case)

    if dry_run:
        print(f"[Dry-run] would execute {len(cases)} cases")
        return []

    # ---------- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç† -------------------------------
    checkpoint_manager = CheckpointManager(root) if save and root else None

    # ---------- å®Ÿè¡Œ -----------------------------------------------
    start_time = time.perf_counter()
    nproc = min(cpu_count(), nproc or 1)

    print(f"ğŸ“Š å®Ÿè¡Œé–‹å§‹: {len(cases)} ã‚±ãƒ¼ã‚¹ã€{nproc} ãƒ—ãƒ­ã‚»ã‚¹")

    completed_cases = []
    failed_cases = []
    results = []

    # ãƒãƒƒãƒå‡¦ç†ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ã§åˆ†å‰²ï¼‰
    for i in range(0, len(cases), checkpoint_interval):
        batch = cases[i : i + checkpoint_interval]
        batch_num = i // checkpoint_interval + 1
        total_batches = (len(cases) + checkpoint_interval - 1) // checkpoint_interval

        print(
            f"ğŸ”„ ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å®Ÿè¡Œä¸­... ({len(batch)} ã‚±ãƒ¼ã‚¹)"
        )

        # ãƒãƒƒãƒå®Ÿè¡Œ
        if nproc > 1:
            with Pool(nproc) as pool:
                batch_results = list(
                    _tqdm(
                        pool.imap(_run_one_safe, batch),
                        total=len(batch),
                        desc=f"Batch {batch_num}",
                    )
                )
        else:
            batch_results = [
                _run_one_safe(case) for case in _tqdm(batch, desc=f"Batch {batch_num}")
            ]

        # çµæœã‚’åˆ†é¡
        for case, (result, error) in zip(batch, batch_results):
            if error is None:
                completed_cases.append(case)
                results.append(result)
            else:
                failed_case = case.copy()
                failed_case["error"] = error
                failed_cases.append(failed_case)

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ›´æ–°
        if batch_num % 2 == 0 or batch_num == total_batches:
            # æ—¢å­˜ã®å®Œäº†ã‚±ãƒ¼ã‚¹ã‚‚å«ã‚ã¦ä¿å­˜
            all_completed = []
            if checkpoint_manager is not None:
                existing_checkpoint = checkpoint_manager.load_checkpoint()
                if existing_checkpoint:
                    completed_hashes = set(
                        existing_checkpoint.get("completed_case_hashes", [])
                    )
                    for case in cases:
                        case_hash = checkpoint_manager._case_hash(case)
                        if case_hash in completed_hashes:
                            all_completed.append(case)
            all_completed.extend(completed_cases)

            if checkpoint_manager is not None:
                checkpoint_manager.save_checkpoint(
                    all_completed, failed_cases, len(cases), start_time
                )

    # ---------- æœ€çµ‚çµæœæ•´ç† ---------------------------------------
    print(
        f"âœ… å®Ÿè¡Œå®Œäº†: {len(completed_cases)}/{len(cases)} æˆåŠŸ, {len(failed_cases)} å¤±æ•—"
    )

    if failed_cases:
        print(f"âš  å¤±æ•—ã‚±ãƒ¼ã‚¹: {len(failed_cases)} ä»¶")
        for i, failed_case in enumerate(failed_cases[:5]):  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
            error_preview = failed_case.get("error", "Unknown error")[:100]
            print(f"  {i + 1}. {error_preview}...")
        if len(failed_cases) > 5:
            print(f"  ... (ä»– {len(failed_cases) - 5} ä»¶)")

    # ---------- summary.csv ----------------------------------------
    if save and root is not None:
        rows: list[dict[str, Any]] = []
        for case, result in zip(cases, results):
            row = {k: v for k, v in case.items() if k not in ["outdir", "save"]}
            if result is not None:
                vals = result
                if isinstance(vals, np.ndarray):
                    if vals.ndim == 0:
                        vals = np.array([float(vals)])
                    elif vals.ndim == 1:
                        pass
                    else:
                        vals = vals[-1]
                else:
                    vals = [vals]
                row.update({f"pop_{i}": float(p) for i, p in enumerate(vals)})
                row["status"] = "success"
            else:
                row["status"] = "failed"
            rows.append(row)

        df = pd.DataFrame(rows)
        df.to_csv(root / "summary.csv", index=False)

        # æˆåŠŸã‚±ãƒ¼ã‚¹ã®ã¿ã®ã‚µãƒãƒªãƒ¼
        success_df = df[df["status"] == "success"]
        if not success_df.empty:
            success_df.to_csv(root / "summary_success.csv", index=False)

    return [r for r in results if r is not None]


def resume_run(
    results_dir: str | Path,
    *,
    nproc: int | None = None,
    checkpoint_interval: int = 10,
) -> list[Any]:
    """ä¸­æ–­ã•ã‚ŒãŸè¨ˆç®—ã‚’é€”ä¸­ã‹ã‚‰å†é–‹"""

    results_dir = Path(results_dir)
    if not results_dir.exists():
        raise FileNotFoundError(f"çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_dir}")

    checkpoint_manager = CheckpointManager(results_dir)
    if not checkpoint_manager.is_resumable():
        raise ValueError(f"å†é–‹å¯èƒ½ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_dir}")

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    checkpoint = checkpoint_manager.load_checkpoint()
    if checkpoint is None:
        raise ValueError("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")

    print(f"ğŸ“ å†é–‹: {results_dir}")
    print(
        f"ğŸ”„ å‰å›ã®é€²æ—: {checkpoint['completed_cases']}/{checkpoint['total_cases']} å®Œäº†"
    )

    # å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    params_file = results_dir / "params.py"
    if not params_file.exists():
        raise FileNotFoundError(f"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {params_file}")

    base_dict = _load_params_file(str(params_file))
    base_dict.get("description", "resumed_run")

    # å…¨ã‚±ãƒ¼ã‚¹ã‚’å†æ§‹ç¯‰
    all_cases: list[dict[str, Any]] = []
    for case, sweep_keys in _expand_cases(base_dict):
        case["save"] = True
        rel = Path(*[f"{k}_{_label(case[k])}" for k in sweep_keys])
        outdir = results_dir / rel
        outdir.mkdir(parents=True, exist_ok=True)
        case["outdir"] = str(outdir)
        all_cases.append(case)

    # æ®‹ã‚Šã®ã‚±ãƒ¼ã‚¹ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    remaining_cases = checkpoint_manager.filter_remaining_cases(all_cases)

    if not remaining_cases:
        print("âœ… å…¨ã‚±ãƒ¼ã‚¹ãŒæ—¢ã«å®Œäº†ã—ã¦ã„ã¾ã™")
        return []

    print(f"ğŸ”„ æ®‹ã‚Š {len(remaining_cases)} ã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œä¸­...")

    # æ®‹ã‚Šã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    start_time = time.perf_counter()
    nproc = min(cpu_count(), nproc or 1)

    completed_cases = []
    failed_cases = []
    results = []

    # æ—¢å­˜ã®å®Œäº†ãƒ»å¤±æ•—ã‚±ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿
    existing_checkpoint = checkpoint_manager.load_checkpoint()
    if existing_checkpoint:
        existing_failed = existing_checkpoint.get("failed_case_data", [])
        failed_cases.extend(existing_failed)

    # ãƒãƒƒãƒå‡¦ç†
    for i in range(0, len(remaining_cases), checkpoint_interval):
        batch = remaining_cases[i : i + checkpoint_interval]
        batch_num = i // checkpoint_interval + 1
        total_batches = (
            len(remaining_cases) + checkpoint_interval - 1
        ) // checkpoint_interval

        print(
            f"ğŸ”„ ãƒãƒƒãƒ {batch_num}/{total_batches} ã‚’å®Ÿè¡Œä¸­... ({len(batch)} ã‚±ãƒ¼ã‚¹)"
        )

        # ãƒãƒƒãƒå®Ÿè¡Œ
        if nproc > 1:
            with Pool(nproc) as pool:
                batch_results = list(
                    _tqdm(
                        pool.imap(_run_one_safe, batch),
                        total=len(batch),
                        desc=f"Resume Batch {batch_num}",
                    )
                )
        else:
            batch_results = [
                _run_one_safe(case)
                for case in _tqdm(batch, desc=f"Resume Batch {batch_num}")
            ]

        # çµæœã‚’åˆ†é¡
        for case, (result, error) in zip(batch, batch_results):
            if error is None:
                completed_cases.append(case)
                results.append(result)
            else:
                failed_case = case.copy()
                failed_case["error"] = error
                failed_cases.append(failed_case)

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ›´æ–°
        if batch_num % 2 == 0 or batch_num == total_batches:
            # æ—¢å­˜ã®å®Œäº†ã‚±ãƒ¼ã‚¹ã‚‚å«ã‚ã¦ä¿å­˜
            all_completed = []
            existing_checkpoint = checkpoint_manager.load_checkpoint()
            if existing_checkpoint:
                completed_hashes = set(
                    existing_checkpoint.get("completed_case_hashes", [])
                )
                for case in all_cases:
                    case_hash = checkpoint_manager._case_hash(case)
                    if case_hash in completed_hashes:
                        all_completed.append(case)
            all_completed.extend(completed_cases)

            checkpoint_manager.save_checkpoint(
                all_completed, failed_cases, len(all_cases), start_time
            )

    print(f"âœ… å†é–‹å®Œäº†: {len(completed_cases)} æ–°è¦å®Œäº†, {len(failed_cases)} å¤±æ•—")

    # æœ€çµ‚ã‚µãƒãƒªãƒ¼æ›´æ–°
    _update_summary(results_dir, all_cases)

    return results


def _update_summary(results_dir: Path, all_cases: list[dict[str, Any]]):
    """ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°"""
    try:
        rows = []
        for case in all_cases:
            row = {k: v for k, v in case.items() if k not in ["outdir", "save"]}

            outdir = Path(case["outdir"])
            result_file = outdir / "result.npz"

            if result_file.exists():
                try:
                    data = np.load(result_file)
                    if "pop" in data:
                        pop_final = data["pop"][-1]
                        row.update(
                            {f"pop_{i}": float(p) for i, p in enumerate(pop_final)}
                        )
                    row["status"] = "success"
                except Exception:
                    row["status"] = "corrupted"
            else:
                row["status"] = "failed"

            rows.append(row)

        df = pd.DataFrame(rows)
        df.to_csv(results_dir / "summary.csv", index=False)

        # æˆåŠŸã‚±ãƒ¼ã‚¹ã®ã¿
        success_df = df[df["status"] == "success"]
        if not success_df.empty:
            success_df.to_csv(results_dir / "summary_success.csv", index=False)

        print(f"ğŸ“Š ã‚µãƒãƒªãƒ¼æ›´æ–°å®Œäº†: {len(success_df)}/{len(df)} æˆåŠŸ")

    except Exception as e:
        print(f"âš  ã‚µãƒãƒªãƒ¼æ›´æ–°å¤±æ•—: {e}")


# ---------------------------------------------------------------------
# å…ƒã®run_allé–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
# ---------------------------------------------------------------------
def run_all(
    params: str | Mapping[str, Any],
    *,
    nproc: int | None = None,
    save: bool = True,
    dry_run: bool = False,
):
    """å…ƒã®run_allé–¢æ•°ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç„¡ã—ï¼‰"""
    return run_all_with_checkpoint(
        params,
        nproc=nproc,
        save=save,
        dry_run=dry_run,
        checkpoint_interval=len(
            list(
                _expand_cases(
                    _load_params_file(params)
                    if isinstance(params, str)
                    else dict(params)
                )
            )
        )
        + 1,  # å…¨ã¦ä¸€åº¦ã«å®Ÿè¡Œï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç„¡ã—ï¼‰
    )


# ---------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------
if __name__ == "__main__":
    import argparse

    ap = argparse.ArgumentParser(description="Run rovibrational simulation batch")
    ap.add_argument("paramfile", nargs="?", help=".py file with parameter definitions")
    ap.add_argument("-j", "--nproc", type=int, help="processes (default=1)")
    ap.add_argument("--no-save", action="store_true", help="do not write any files")
    ap.add_argument("--dry-run", action="store_true", help="list cases only (no run)")
    ap.add_argument(
        "--resume",
        type=str,
        metavar="RESULTS_DIR",
        help="resume from checkpoint in specified results directory",
    )
    ap.add_argument(
        "--checkpoint-interval",
        type=int,
        default=10,
        help="save checkpoint every N cases (default=10)",
    )
    args = ap.parse_args()

    if args.resume:
        # å†é–‹ãƒ¢ãƒ¼ãƒ‰
        print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹: {args.resume}")
        t0 = time.perf_counter()
        try:
            resume_run(
                args.resume,
                nproc=args.nproc,
                checkpoint_interval=args.checkpoint_interval,
            )
            print(f"âœ… Resumed and finished in {time.perf_counter() - t0:.1f} s")
        except Exception as e:
            print(f"âŒ å†é–‹ã«å¤±æ•—: {e}")
            exit(1)
    else:
        # é€šå¸¸å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        if not args.paramfile:
            ap.error("paramfile is required when not using --resume")

        t0 = time.perf_counter()
        run_all_with_checkpoint(
            args.paramfile,
            nproc=args.nproc,
            save=not args.no_save,
            dry_run=args.dry_run,
            checkpoint_interval=args.checkpoint_interval,
        )
        print(f"Finished in {time.perf_counter() - t0:.1f} s")
