# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.bm_25_operator_type import Bm25OperatorType
from ..types.search_chunk import SearchChunk
from .raw_client import AsyncRawSearchClient, RawSearchClient
from .types.alpha import Alpha

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class SearchClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawSearchClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawSearchClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawSearchClient
        """
        return self._raw_client

    def qna(
        self,
        *,
        question: str,
        session_id: str,
        tenant_id: str,
        context_list: typing.Optional[typing.Sequence[str]] = OMIT,
        search_modes: typing.Optional[typing.Sequence[str]] = OMIT,
        sub_tenant_id: typing.Optional[str] = OMIT,
        highlight_chunks: typing.Optional[bool] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        search_alpha: typing.Optional[float] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        ai_generation: typing.Optional[bool] = OMIT,
        top_n: typing.Optional[int] = OMIT,
        user_name: typing.Optional[str] = OMIT,
        user_instructions: typing.Optional[str] = OMIT,
        multi_step_reasoning: typing.Optional[bool] = OMIT,
        auto_agent_routing: typing.Optional[bool] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        question : str

        session_id : str

        tenant_id : str

        context_list : typing.Optional[typing.Sequence[str]]

        search_modes : typing.Optional[typing.Sequence[str]]

        sub_tenant_id : typing.Optional[str]

        highlight_chunks : typing.Optional[bool]

        stream : typing.Optional[bool]

        search_alpha : typing.Optional[float]

        recency_bias : typing.Optional[float]

        ai_generation : typing.Optional[bool]

        top_n : typing.Optional[int]

        user_name : typing.Optional[str]

        user_instructions : typing.Optional[str]

        multi_step_reasoning : typing.Optional[bool]

        auto_agent_routing : typing.Optional[bool]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.qna(question='question', session_id='session_id', tenant_id='tenant_id', )
        """
        _response = self._raw_client.qna(
            question=question,
            session_id=session_id,
            tenant_id=tenant_id,
            context_list=context_list,
            search_modes=search_modes,
            sub_tenant_id=sub_tenant_id,
            highlight_chunks=highlight_chunks,
            stream=stream,
            search_alpha=search_alpha,
            recency_bias=recency_bias,
            ai_generation=ai_generation,
            top_n=top_n,
            user_name=user_name,
            user_instructions=user_instructions,
            multi_step_reasoning=multi_step_reasoning,
            auto_agent_routing=auto_agent_routing,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    def retrieve(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        alpha: typing.Optional[Alpha] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        num_related_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Search for content within indexed sources using semantic and keyword search capabilities.

        This endpoint performs a search query against the Findr backend, allowing users to retrieve
        relevant content chunks from their indexed documents and sources. The search can be customized
        with various parameters to control the number of results and search behavior.

        Args:
            request (SearchRequest): The search request containing:
                - query (str): Search query string to find relevant content
                - tenant_id (str, optional): Tenant identifier for multi-tenancy
                - sub_tenant_id (str, optional): Sub-tenant identifier, defaults to tenant_id
                - max_chunks (int, optional): Maximum number of content chunks to return
                - alpha (Union[float, str], optional): Search algorithm parameter for result ranking (default: 0.8). Can be float-type (0.0-1.0) or 'auto' for dynamic selection
                - recency_bias (float, optional): Bias towards more recent content (default: 0.5)
                - num_related_chunks (int, optional): Number of related chunks to return (default: 0)
            api_details (dict): Authentication details obtained from API key validation

        Returns:
            SearchData: Success response with search results

        Parameters
        ----------
        query : str

        tenant_id : str

        sub_tenant_id : typing.Optional[str]

        max_chunks : typing.Optional[int]

        alpha : typing.Optional[Alpha]

        recency_bias : typing.Optional[float]

        num_related_chunks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.retrieve(query='query', tenant_id='tenant_id', )
        """
        _response = self._raw_client.retrieve(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            max_chunks=max_chunks,
            alpha=alpha,
            recency_bias=recency_bias,
            num_related_chunks=num_related_chunks,
            request_options=request_options,
        )
        return _response.data

    def full_text_search(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        operator: typing.Optional[Bm25OperatorType] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Full text search endpoint for Cortex customers.
        Performs full text search with configurable operators for precise text matching against the Findr backend.

        This endpoint performs a full text search query against the Findr backend, allowing users to retrieve
        relevant content chunks from their indexed documents and sources using BM25-based text matching.
        The search can be customized with various operators to control the matching behavior.

        Args:
            request (FullTextSearchRequest): The full text search request containing:
                - query (str): Search query string to find relevant content
                - tenant_id (str): Tenant identifier for multi-tenancy
                - sub_tenant_id (str, optional): Sub-tenant identifier, defaults to tenant_id
                - operator (BM25OperatorType, optional): Full text search operator type (OR or AND). Defaults to OR
                - max_chunks (int, optional): Maximum number of content chunks to return (1-1001, defaults to 25)
            api_details (dict): Authentication details obtained from API key validation

        Returns:
            FullTextSearchData: Success response with full text search results

        Parameters
        ----------
        query : str

        tenant_id : str

        sub_tenant_id : typing.Optional[str]

        operator : typing.Optional[Bm25OperatorType]

        max_chunks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        from usecortex-ai import CortexAI

        client = CortexAI(token="YOUR_TOKEN", )
        client.search.full_text_search(query='query', tenant_id='tenant_id', )
        """
        _response = self._raw_client.full_text_search(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            operator=operator,
            max_chunks=max_chunks,
            request_options=request_options,
        )
        return _response.data


class AsyncSearchClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawSearchClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawSearchClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawSearchClient
        """
        return self._raw_client

    async def qna(
        self,
        *,
        question: str,
        session_id: str,
        tenant_id: str,
        context_list: typing.Optional[typing.Sequence[str]] = OMIT,
        search_modes: typing.Optional[typing.Sequence[str]] = OMIT,
        sub_tenant_id: typing.Optional[str] = OMIT,
        highlight_chunks: typing.Optional[bool] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        search_alpha: typing.Optional[float] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        ai_generation: typing.Optional[bool] = OMIT,
        top_n: typing.Optional[int] = OMIT,
        user_name: typing.Optional[str] = OMIT,
        user_instructions: typing.Optional[str] = OMIT,
        multi_step_reasoning: typing.Optional[bool] = OMIT,
        auto_agent_routing: typing.Optional[bool] = OMIT,
        metadata: typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        question : str

        session_id : str

        tenant_id : str

        context_list : typing.Optional[typing.Sequence[str]]

        search_modes : typing.Optional[typing.Sequence[str]]

        sub_tenant_id : typing.Optional[str]

        highlight_chunks : typing.Optional[bool]

        stream : typing.Optional[bool]

        search_alpha : typing.Optional[float]

        recency_bias : typing.Optional[float]

        ai_generation : typing.Optional[bool]

        top_n : typing.Optional[int]

        user_name : typing.Optional[str]

        user_instructions : typing.Optional[str]

        multi_step_reasoning : typing.Optional[bool]

        auto_agent_routing : typing.Optional[bool]

        metadata : typing.Optional[typing.Dict[str, typing.Optional[typing.Any]]]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.qna(question='question', session_id='session_id', tenant_id='tenant_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.qna(
            question=question,
            session_id=session_id,
            tenant_id=tenant_id,
            context_list=context_list,
            search_modes=search_modes,
            sub_tenant_id=sub_tenant_id,
            highlight_chunks=highlight_chunks,
            stream=stream,
            search_alpha=search_alpha,
            recency_bias=recency_bias,
            ai_generation=ai_generation,
            top_n=top_n,
            user_name=user_name,
            user_instructions=user_instructions,
            multi_step_reasoning=multi_step_reasoning,
            auto_agent_routing=auto_agent_routing,
            metadata=metadata,
            request_options=request_options,
        )
        return _response.data

    async def retrieve(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        alpha: typing.Optional[Alpha] = OMIT,
        recency_bias: typing.Optional[float] = OMIT,
        num_related_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Search for content within indexed sources using semantic and keyword search capabilities.

        This endpoint performs a search query against the Findr backend, allowing users to retrieve
        relevant content chunks from their indexed documents and sources. The search can be customized
        with various parameters to control the number of results and search behavior.

        Args:
            request (SearchRequest): The search request containing:
                - query (str): Search query string to find relevant content
                - tenant_id (str, optional): Tenant identifier for multi-tenancy
                - sub_tenant_id (str, optional): Sub-tenant identifier, defaults to tenant_id
                - max_chunks (int, optional): Maximum number of content chunks to return
                - alpha (Union[float, str], optional): Search algorithm parameter for result ranking (default: 0.8). Can be float-type (0.0-1.0) or 'auto' for dynamic selection
                - recency_bias (float, optional): Bias towards more recent content (default: 0.5)
                - num_related_chunks (int, optional): Number of related chunks to return (default: 0)
            api_details (dict): Authentication details obtained from API key validation

        Returns:
            SearchData: Success response with search results

        Parameters
        ----------
        query : str

        tenant_id : str

        sub_tenant_id : typing.Optional[str]

        max_chunks : typing.Optional[int]

        alpha : typing.Optional[Alpha]

        recency_bias : typing.Optional[float]

        num_related_chunks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.retrieve(query='query', tenant_id='tenant_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.retrieve(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            max_chunks=max_chunks,
            alpha=alpha,
            recency_bias=recency_bias,
            num_related_chunks=num_related_chunks,
            request_options=request_options,
        )
        return _response.data

    async def full_text_search(
        self,
        *,
        query: str,
        tenant_id: str,
        sub_tenant_id: typing.Optional[str] = OMIT,
        operator: typing.Optional[Bm25OperatorType] = OMIT,
        max_chunks: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[SearchChunk]:
        """
        Full text search endpoint for Cortex customers.
        Performs full text search with configurable operators for precise text matching against the Findr backend.

        This endpoint performs a full text search query against the Findr backend, allowing users to retrieve
        relevant content chunks from their indexed documents and sources using BM25-based text matching.
        The search can be customized with various operators to control the matching behavior.

        Args:
            request (FullTextSearchRequest): The full text search request containing:
                - query (str): Search query string to find relevant content
                - tenant_id (str): Tenant identifier for multi-tenancy
                - sub_tenant_id (str, optional): Sub-tenant identifier, defaults to tenant_id
                - operator (BM25OperatorType, optional): Full text search operator type (OR or AND). Defaults to OR
                - max_chunks (int, optional): Maximum number of content chunks to return (1-1001, defaults to 25)
            api_details (dict): Authentication details obtained from API key validation

        Returns:
            FullTextSearchData: Success response with full text search results

        Parameters
        ----------
        query : str

        tenant_id : str

        sub_tenant_id : typing.Optional[str]

        operator : typing.Optional[Bm25OperatorType]

        max_chunks : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[SearchChunk]
            Successful Response

        Examples
        --------
        import asyncio

        from usecortex-ai import AsyncCortexAI

        client = AsyncCortexAI(token="YOUR_TOKEN", )
        async def main() -> None:
            await client.search.full_text_search(query='query', tenant_id='tenant_id', )
        asyncio.run(main())
        """
        _response = await self._raw_client.full_text_search(
            query=query,
            tenant_id=tenant_id,
            sub_tenant_id=sub_tenant_id,
            operator=operator,
            max_chunks=max_chunks,
            request_options=request_options,
        )
        return _response.data
