import json
import os
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, cast

from ..content import ContentCleaner, ResearchDocument
from ..llm import LLMProvider
from ..search import GoogleSearchProvider
from ..utils import logger, get_current_date


class DeepResearchAgent:
    """ä¸»è¦ç ”ç©¶ä»£ç†ç±»"""

    def __init__(
        self,
            google_api_key=None,
            google_cx=None,
        openai_api_key=None,
        max_derivation_depth=None,
        max_derivation_width=None,
        llm_model=None,
        llm_temperature=None,
        search_max_results=None,
        search_depth=None,
        report_output_path=None,
    ):
        """åˆå§‹åŒ–DeepResearchAgentï¼Œæ”¯æŒä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®

        Args:
            google_api_key: Google Custom Search APIå¯†é’¥ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡GOOGLE_API_KEYåŠ è½½
            google_cx: Google Custom Search Engine IDï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡GOOGLE_CXåŠ è½½ï¼Œé»˜è®¤ä¸º"c22e0884b26a04213"
            openai_api_key: OpenAI APIå¯†é’¥ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡OPENAI_API_KEYåŠ è½½
            max_derivation_depth: è¡ç”Ÿæ ¸å¿ƒæ–¹é¢çš„æœ€å¤§æ·±åº¦ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_MAX_DERIVATION_DEPTHåŠ è½½ï¼Œé»˜è®¤ä¸º1
            llm_model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_LLM_MODELåŠ è½½ï¼Œé»˜è®¤ä¸º"gpt-4.1-mini"
            llm_temperature: LLMæ¸©åº¦å‚æ•°ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_LLM_TEMPERATUREåŠ è½½ï¼Œé»˜è®¤ä¸º0
            search_max_results: æ¯æ¬¡æœç´¢è¿”å›çš„æœ€å¤§ç»“æœæ•°ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_SEARCH_MAX_RESULTSåŠ è½½ï¼Œé»˜è®¤ä¸º5
            search_depth: æœç´¢æ·±åº¦ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_SEARCH_DEPTHåŠ è½½ï¼Œé»˜è®¤ä¸º"basic"
            report_output_path: æŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼Œå¦‚æœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡RESEARCH_REPORT_PATHåŠ è½½ï¼Œé»˜è®¤ä¸º"/tmp/result.md"
        """
        # åˆå§‹åŒ–é…ç½®
        google_key = google_api_key or os.getenv("GOOGLE_API_KEY")
        google_cx = google_cx or os.getenv("GOOGLE_CX", "c22e0884b26a04213")
        openai_key = openai_api_key or os.getenv("OPENAI_API_KEY")

        # åˆå§‹åŒ–é«˜çº§é…ç½®å‚æ•°
        self.max_derivation_depth = max_derivation_depth or int(os.getenv("RESEARCH_DEPTH", "1"))
        self.max_derivation_width = max_derivation_width or int(os.getenv("RESEARCH_WIDTH", "10"))
        self.llm_model = llm_model or "gpt-4.1-mini"
        self.llm_temperature = float(llm_temperature or 0)
        self.search_max_results = int(search_max_results or 5)
        self.search_depth = search_depth or "basic"
        self.report_output_path = report_output_path or os.getenv(
            "RESEARCH_REPORT_PATH", "/tmp/result.md"
        )

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.search_provider = GoogleSearchProvider(google_key, google_cx)
        self.llm = LLMProvider(openai_key)
        self.cleaner = ContentCleaner()
        self.document = None

    def set_research_question(self, question: str) -> None:
        """è®¾ç½®ç ”ç©¶é—®é¢˜å¹¶åˆå§‹åŒ–æ–‡æ¡£"""
        self.document = ResearchDocument(question)

    def generate_search_strategy(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœç´¢ç­–ç•¥"""
        logger.info("æ­£åœ¨ç”Ÿæˆæœç´¢ç­–ç•¥...")

        system_prompt = f"""å½“å‰æ˜¯{get_current_date()}ã€‚ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶ç­–ç•¥ä¸“å®¶ã€‚
        è¯·æ ¹æ®æˆ‘æä¾›çš„ç ”ç©¶é—®é¢˜ï¼Œåˆ¶å®šå…¨é¢çš„æœç´¢æ–¹æ¡ˆï¼Œä»¥ç¡®ä¿è·å–é«˜è´¨é‡ã€å¤šè§’åº¦çš„ä¿¡æ¯ï¼š

        1. åˆ†æç ”ç©¶é—®é¢˜ï¼Œè¯†åˆ«ä¸è¶…è¿‡{self.max_derivation_width}ä¸ªçš„æ ¸å¿ƒæ–¹é¢ã€‚
        2. ä¸ºæ¯ä¸ªæ–¹é¢åˆ›å»º1-2ä¸ªç²¾ç¡®çš„æœç´¢çŸ­è¯­ï¼Œç¡®ä¿ï¼š
           - åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡å’Œå…³é”®è¯ï¼Œé¿å…æ¨¡ç³Šæœç´¢
           - é’ˆå¯¹æ€§å¼ºï¼Œèƒ½ç›´æ¥è·å–è¯¥æ–¹é¢çš„ä¸“ä¸šä¿¡æ¯
           - é€‚åˆç½‘ç»œæœç´¢å¼•æ“ä½¿ç”¨
           - æ¯ä¸ªçŸ­è¯­ä¹‹é—´æœ‰ä¸åŒçš„è§’åº¦æˆ–é‡ç‚¹ï¼Œä»¥æœ€å¤§åŒ–ä¿¡æ¯è¦†ç›–èŒƒå›´
        3. è¿”å›æ ¼å¼å¿…é¡»ä¸ºæœ‰æ•ˆçš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹ä¸¤ä¸ªé¡¶çº§é”®ï¼š
           - `event`: å­—ç¬¦ä¸²ï¼Œç®€æ˜æ¦‚æ‹¬ç ”ç©¶é—®é¢˜çš„æ ¸å¿ƒä¸»é¢˜
           - `analysis`: å¯¹è±¡ï¼Œé”®ä¸ºå„ä¸ªç ”ç©¶æ–¹é¢çš„åç§°ï¼Œå€¼ä¸ºåŒ…å«å¤šä¸ªæœç´¢çŸ­è¯­çš„æ•°ç»„

        ä¼˜å…ˆå…³æ³¨æœ€æ–°ä¿¡æ¯ã€æƒå¨æ¥æºå’Œå¤šæ ·åŒ–è§‚ç‚¹ã€‚ä»…è¿”å›JSONæ ¼å¼ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"""

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=self.document.question, json_format=True
        )

        logger.success("æœç´¢ç­–ç•¥ç”Ÿæˆå®Œæˆ")

        # å¦‚æœç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        if not result:
            return {
                "event": self.document.question,
                "analysis": {"default": [self.document.question]},
            }

        # ç¡®ä¿analysisä¸­çš„å€¼éƒ½æ˜¯æ•°ç»„æ ¼å¼
        analysis = result.get("analysis", {})
        for key, value in analysis.items():
            if isinstance(value, str):
                analysis[key] = [value]

        result["analysis"] = analysis
        return result

    def search_and_extract(self, search_query: str, max_results: int = 5) -> str:
        """æ‰§è¡Œæœç´¢å¹¶æå–å†…å®¹"""
        logger.info(f"ğŸ” æ­£åœ¨åˆ†æ: {search_query}")
        logger.info("  â”œâ”€ æœç´¢ç½‘ç»œèµ„æº...")

        # æœç´¢è·å–ç»“æœ
        results = self.search_provider.search(
            search_query, max_results=self.search_max_results, search_depth=self.search_depth
        )
        logger.success(f"  â”œâ”€ æœç´¢å®Œæˆï¼Œè·å–åˆ° {len(results)} æ¡ç»“æœ")

        # å¤„ç†ç»“æœ
        logger.info("  â””â”€ æå–å’Œæ¸…æ´—å†…å®¹...")
        cleaned_results = []

        # ä½¿ç”¨å¹¶å‘å¤„ç†å¤šä¸ªç»“æœçš„æ¸…æ´—
        def clean_single_result(result):
            url = result.get("url", "")
            title = result.get("title", "æ— æ ‡é¢˜")
            content = result.get("content", "")
            raw_content = result.get("raw_content", "")

            # ä½¿ç”¨ContentCleaneræ¸…æ´—å†…å®¹
            cleaned_content = self.cleaner.clean_with_trafilatura(url, raw_content, title)

            # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            if not cleaned_content:
                cleaned_content = raw_content or content

            # å¯¹æ¸…æ´—åçš„å†…å®¹è¿›è¡Œæ ¼å¼æ ‡å‡†åŒ–
            return (
                f"æ¥æº: {url}\næ ‡é¢˜: {title}\nå‘å¸ƒæ—¥æœŸ: {result.get('published_date', 'æœªçŸ¥')}\n"
                + f"ç›¸å…³åº¦è¯„åˆ†: {result.get('score', 0)}\nå†…å®¹æ‘˜è¦:\n{cleaned_content[:1000]}...\n\n"
                + f"å®Œæ•´å†…å®¹:\n{cleaned_content}\n\n---\n"
            )

        # å¹¶å‘å¤„ç†æ‰€æœ‰ç»“æœ
        with ThreadPoolExecutor(max_workers=min(len(results), 10)) as executor:
            cleaned_results = list(executor.map(clean_single_result, results))

        combined_result = "".join(cleaned_results)
        logger.success("  â””â”€ å†…å®¹æå–å®Œæˆ")

        # æ·»åŠ åˆ°ç ”ç©¶æ–‡æ¡£
        self.document.add_content(combined_result)

        return combined_result

    def generate_report(self) -> str:
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        logger.info("\nğŸ“ æ­£åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...")

        prompt = f"""ä½œä¸ºä¸“ä¸šç ”ç©¶åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹ææ–™åˆ›å»ºä¸€ä»½é«˜è´¨é‡ç ”ç©¶æŠ¥å‘Šã€‚

### ç ”ç©¶é—®é¢˜
{self.document.question}

### æ£€ç´¢èµ„æ–™
{self.document.get_content()}

### è¾“å‡ºè¦æ±‚
1. **ç»“æ„ä¸æ ¼å¼**
   - åˆ›å»ºä¸€ä¸ªå¼•äººæ³¨ç›®çš„ä¸€çº§æ ‡é¢˜ï¼ˆ#ï¼‰ï¼Œå‡†ç¡®åæ˜ ç ”ç©¶ä¸»é¢˜
   - ä½¿ç”¨ç®€æ˜æ‘˜è¦ä½œä¸ºå¼•è¨€ï¼Œæ¦‚è¿°ä¸»è¦å‘ç°å’Œç»“è®º
   - è¿ç”¨äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰ç»„ç»‡ä¸»è¦éƒ¨åˆ†ï¼Œæ¯éƒ¨åˆ†å›´ç»•ä¸€ä¸ªæ ¸å¿ƒä¸»é¢˜
   - é‡‡ç”¨ä¸‰çº§æ ‡é¢˜ï¼ˆ###ï¼‰è¿›ä¸€æ­¥ç»†åˆ†å¤æ‚ä¸»é¢˜
   - å…¨æ–‡é‡‡ç”¨Markdownæ ¼å¼ï¼Œå……åˆ†åˆ©ç”¨æ ‡é¢˜å±‚çº§ã€åˆ—è¡¨ã€ç²—ä½“å’Œå¼•ç”¨

2. **å†…å®¹è¦æ±‚**
   - ç»¼åˆå¤šä¸ªæ¥æºçš„ä¿¡æ¯ï¼Œé¿å…è¿‡åº¦ä¾èµ–å•ä¸€æ¥æº
   - å¯¹æ¯”ä¸åŒè§‚ç‚¹ï¼Œæä¾›å¹³è¡¡çš„åˆ†æ
   - çº³å…¥ç›¸å…³ç»Ÿè®¡æ•°æ®ã€ç ”ç©¶å‘ç°å’Œä¸“å®¶è§‚ç‚¹ï¼Œå¢å¼ºå¯ä¿¡åº¦
   - æ˜ç¡®æŒ‡å‡ºä¿¡æ¯ç¼ºå£æˆ–æœ‰äº‰è®®çš„é¢†åŸŸ
   - **éå¸¸é‡è¦**ï¼šåœ¨é€‚å½“ä½ç½®ä¿ç•™æ–¹æ‹¬å·ä¸­çš„å¼•ç”¨æ ‡è®°ï¼ˆå¦‚ [1]ã€[2] ç­‰ï¼‰

3. **è´¨é‡æ ‡å‡†**
   - æŠ¥å‘Šç¯‡å¹…å†…å®¹ä¸°å¯Œ
   - ä¸“ä¸šã€å®¢è§‚çš„è¯­æ°”ï¼Œé¿å…è¿‡åº¦ä¿®é¥°è¯­
   - æ¸…æ™°çš„é€»è¾‘æµç¨‹ï¼Œæ®µè½ä¹‹é—´æœ‰è‡ªç„¶è¿‡æ¸¡
   - ä¸è¦åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œè¿™å°†ç”±ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ """

        # è·å–æµå¼è¾“å‡º
        stream = self.llm.generate_stream(
            system_prompt="You are a professional research analyst. Always maintain academic citation standards by preserving citation markers in square brackets.",
            user_prompt=prompt,
        )

        if not stream:
            logger.error("æ— æ³•åˆ›å»ºæµå¼è¾“å‡º")
            return ""

        report = ""
        logger.info("\n--- ç ”ç©¶æŠ¥å‘Šå¼€å§‹ ---\n")

        try:
            for chunk in stream:
                # å®‰å…¨åœ°å¤„ç†æµå—
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta
                    and hasattr(chunk.choices[0].delta, "content")
                    and chunk.choices[0].delta.content
                ):
                    content = chunk.choices[0].delta.content
                    report += content
                    print(content, end="", flush=True)
        except Exception as e:
            logger.error(f"å¤„ç†æµè¾“å‡ºæ—¶å‡ºé”™: {e}")

        logger.info("\n\n--- ç ”ç©¶æŠ¥å‘Šç»“æŸ ---")

        # è§£ææŠ¥å‘Šä¸­ä½¿ç”¨çš„å¼•ç”¨ï¼Œå¹¶æ‰‹åŠ¨æ·»åŠ å‚è€ƒæ–‡çŒ®éƒ¨åˆ†
        final_report = self._append_references_to_report(report)

        # ä¿å­˜æŠ¥å‘Š
        self._save_report(final_report)

        return final_report

    def _save_report(self, report: str, filepath: str = None) -> None:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        output_path = filepath or self.report_output_path
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(report)
        logger.success(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {output_path}")

    def _append_references_to_report(self, report: str) -> str:
        """è§£ææŠ¥å‘Šå¹¶æ·»åŠ å‚è€ƒæ–‡çŒ®éƒ¨åˆ†"""
        logger.info("æ­£åœ¨è§£ææŠ¥å‘Šå¹¶æ·»åŠ å‚è€ƒæ–‡çŒ®...")

        # ä»æŠ¥å‘Šä¸­æå–æ‰€æœ‰å¼•ç”¨æ ‡è®° [1], [2], ç­‰
        import re

        citation_pattern = r"\[(\d+)\]"
        citations_used = set(re.findall(citation_pattern, report))

        if not citations_used:
            logger.warning("æœªåœ¨æŠ¥å‘Šä¸­å‘ç°ä»»ä½•å¼•ç”¨æ ‡è®°")
            return report

        # è·å–å®Œæ•´çš„å¼•ç”¨åˆ—è¡¨
        all_citations = self.document.get_citation_list()

        # ç­›é€‰å‡ºæŠ¥å‘Šä¸­ä½¿ç”¨çš„å¼•ç”¨
        used_references = []
        for citation_id in citations_used:
            try:
                citation_index = int(citation_id) - 1  # è½¬æ¢ä¸º0-ç´¢å¼•
                if 0 <= citation_index < len(all_citations):
                    used_references.append(all_citations[citation_index])
                else:
                    logger.warning(f"å¼•ç”¨ [{citation_id}] è¶…å‡ºèŒƒå›´")
            except ValueError:
                logger.warning(f"æ— æ³•è§£æå¼•ç”¨ID: {citation_id}")

        # æ„å»ºå‚è€ƒæ–‡çŒ®éƒ¨åˆ†
        if used_references:
            used_references.sort(key=lambda x: x.get("citation_id", 0))  # æŒ‰ç…§å¼•ç”¨IDæ’åº
            references_section = "\n\n### å‚è€ƒæ–‡çŒ®\n\n"
            for i, ref in enumerate(used_references):
                ref_id = ref.get("citation_id", i + 1)  # ä½¿ç”¨å¼•ç”¨IDæˆ–ç´¢å¼•ä½œä¸ºæ ‡è¯†
                url = ref.get("url", "")
                title = ref.get("title", "æ— æ ‡é¢˜")

                # æ ¼å¼åŒ–å‚è€ƒæ–‡çŒ®æ¡ç›®
                references_section += f"[{ref_id}] {title}. {url}\n\n"

            # å°†å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æ·»åŠ åˆ°æŠ¥å‘Šæœ«å°¾
            final_report = report + references_section
            logger.success(f"æˆåŠŸæ·»åŠ äº† {len(used_references)} æ¡å‚è€ƒæ–‡çŒ®")
            return final_report
        else:
            return report

    def synthesize_content(
        self, content: str = None, question_suffix: str = None
    ) -> Dict[str, Any]:
        """ç»¼åˆæ•´ç†æ”¶é›†çš„å†…å®¹

        Args:
            content: å¯é€‰ï¼Œè¦å¤„ç†çš„å†…å®¹ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨ä¸»æ–‡æ¡£å†…å®¹ã€‚
            question_suffix: é—®é¢˜åç¼€ï¼Œç”¨äºç”Ÿæˆæ›´å…·ä½“çš„æç¤º
        """
        logger.info("æ­£åœ¨ç»¼åˆæ•´ç†æ”¶é›†çš„å†…å®¹...")

        # ä½¿ç”¨æä¾›çš„å†…å®¹æˆ–ä¸»æ–‡æ¡£å†…å®¹
        document_content = content if content is not None else self.document.get_content()

        prompt = f"""ä½œä¸ºä¿¡æ¯åˆ†æä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹å…³äº"{self.document.question}-{question_suffix or ""}"çš„æ”¶é›†èµ„æ–™è¿›è¡Œç»¼åˆæ•´ç†ä¸æç‚¼ã€‚
        
### åŸå§‹èµ„æ–™
{document_content}

### ä»»åŠ¡è¦æ±‚
1. æå–æ‰€æœ‰èµ„æ–™ä¸­çš„æ ¸å¿ƒä¿¡æ¯ï¼Œè¯†åˆ«å…³é”®äº‹å®ã€æ•°æ®å’Œè§‚ç‚¹
2. åˆå¹¶é‡å¤ä¿¡æ¯ï¼Œè§£å†³äº’ç›¸çŸ›ç›¾çš„è¯´æ³•ï¼ˆå¦‚æœ‰ï¼‰ï¼Œæ ‡æ˜ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦
3. æŒ‰ä¸»é¢˜å¯¹ä¿¡æ¯è¿›è¡Œç»“æ„åŒ–ç»„ç»‡ï¼Œç¡®ä¿é€»è¾‘è¿è´¯
4. è¯†åˆ«ä¿¡æ¯ç©ºç™½ï¼ŒæŒ‡å‡ºå“ªäº›é‡è¦æ–¹é¢ç¼ºä¹å……åˆ†èµ„æ–™
5. ä¿ç•™æ‰€æœ‰é‡è¦çš„åŸå§‹é“¾æ¥å’Œå¼•ç”¨ä¿¡æ¯

### è¾“å‡ºæ ¼å¼
è¿”å›ä¸€ä¸ªç»“æ„åŒ–çš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- `main_themes`: æ•°ç»„ï¼ŒåŒ…å«3-7ä¸ªä¸»è¦ä¸»é¢˜
- `key_facts`: å¯¹è±¡ï¼Œæ¯ä¸ªä¸»é¢˜å¯¹åº”çš„å…³é”®äº‹å®åˆ—è¡¨
- `controversies`: æ•°ç»„ï¼Œå­˜åœ¨äº‰è®®æˆ–çŸ›ç›¾çš„è§‚ç‚¹
- `missing_info`: æ•°ç»„ï¼Œéœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„ä¿¡æ¯ç©ºç™½
- `sources`: æ•°ç»„ï¼Œæ•´ç†åçš„å…³é”®ä¿¡æ¯æ¥æº

ç¡®ä¿è¾“å‡ºå¯ç›´æ¥ç”¨äºç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Šï¼Œå†…å®¹å…¨é¢ã€å‡†ç¡®ä¸”ç»“æ„æ¸…æ™°ã€‚"""

        result = self.llm.generate_completion(
            system_prompt="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¿¡æ¯åˆ†æå¸ˆï¼Œæ“…é•¿ä»å¤§é‡èµ„æ–™ä¸­æå–å…³é”®ä¿¡æ¯å¹¶è¿›è¡Œç»“æ„åŒ–æ•´ç†ã€‚",
            user_prompt=prompt,
            json_format=True,
            max_tokens=2000,
        )

        logger.success("å†…å®¹ç»¼åˆæ•´ç†å®Œæˆ")
        return result

    def run(self, question: str) -> str:
        """æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"""
        logger.info(f'\nğŸš€ å¼€å§‹ç ”ç©¶: "{question}"')

        # è®¾ç½®ç ”ç©¶é—®é¢˜
        self.set_research_question(question)

        # ç”Ÿæˆæœç´¢ç­–ç•¥
        search_strategy = self.generate_search_strategy()
        logger.info(f"ğŸ“Š ç ”ç©¶ä¸»é¢˜: {search_strategy.get('event', question)}")
        logger.info(f"ğŸ“Š åˆ†æç­–ç•¥: {len(search_strategy.get('analysis', {}))} ä¸ªå…³é”®æ–¹é¢")

        # 1. å¹¶å‘æ”¶é›†æ¯ä¸ªæ ¸å¿ƒæ–¹é¢å¯¹åº”çš„æœç´¢ç»“æœ
        aspect_raw_data = self.collect_aspect_raw_data_concurrent(search_strategy, question)

        # 2. å¹¶å‘å¤„ç†æ¯ä¸ªæ ¸å¿ƒæ–¹é¢çš„æ•°æ®
        aspect_contents = self.process_aspect_data_concurrent(aspect_raw_data, question)

        # 3. å¹¶å‘é’ˆå¯¹æ¯ä¸ªæ ¸å¿ƒæ–¹é¢ç”Ÿæˆå›ç­”å¹¶å¤„ç†è¡ç”Ÿé—®é¢˜
        final_contents = self.process_aspects_with_derivation_concurrent(aspect_contents, question)

        # 4. æ±‡æ€»æ‰€æœ‰å†…å®¹åˆ°ä¸»æ–‡æ¡£
        for aspect, content in final_contents.items():
            self.document.add_content(f"## {aspect}\n\n{content}\n\n")

        logger.success("\nâœ… æ‰€æœ‰ç ”ç©¶æ–¹é¢å¤„ç†å®Œæˆ!")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        report = self.generate_report()

        return report

    def collect_aspect_raw_data_concurrent(
        self, search_strategy: Dict[str, Any], question: str
    ) -> Dict[str, List[Dict[str, Any]]]:
        """å¹¶å‘æ”¶é›†æ¯ä¸ªæ ¸å¿ƒæ–¹é¢çš„åŸå§‹æœç´¢ç»“æœ"""
        logger.info(f"\nğŸ“Œ å¼€å§‹å¹¶å‘æ”¶é›†æ ¸å¿ƒæ–¹é¢æ•°æ®")
        aspect_raw_data = {}

        # å®šä¹‰æ”¶é›†å•ä¸ªæ–¹é¢æ•°æ®çš„å‡½æ•°
        def collect_aspect_data(aspect_item):
            aspect, queries = aspect_item
            logger.info(f"  ğŸ” å¼€å§‹æ”¶é›†æ ¸å¿ƒæ–¹é¢æ•°æ®: {aspect}")
            aspect_results = []

            # ç¡®ä¿æŸ¥è¯¢æ˜¯åˆ—è¡¨å½¢å¼
            search_phrases = queries if isinstance(queries, list) else [queries]

            # å¯¹æ¯ä¸ªæœç´¢çŸ­è¯­è¿›è¡ŒæŸ¥è¯¢
            for query in search_phrases:
                logger.info(f"    æœç´¢: {query}")
                # æœç´¢æä¾›è€…å·²å†…ç½®é‡è¯•æœºåˆ¶
                results = self.search_provider.search(query)

                if not results:
                    logger.warning(f"    âš ï¸ æŸ¥è¯¢ '{query}' æœªè¿”å›ç»“æœ")
                    continue

                # æå–URLå’Œcontent
                for result in results:
                    aspect_results.append(
                        {
                            "url": result.get("url", ""),
                            "title": result.get("title", "æ— æ ‡é¢˜"),
                            "content": result.get("content", ""),
                            "raw_content": result.get("raw_content", ""),
                            "published_date": result.get("published_date", "æœªçŸ¥"),
                            "score": result.get("score", 0),
                        }
                    )

                logger.success(f"    è·å–åˆ° {len(results)} æ¡ç»“æœ")

            if not aspect_results:
                logger.warning(f"  âš ï¸ æ ¸å¿ƒæ–¹é¢ '{aspect}' æœªæ‰¾åˆ°ä»»ä½•æ•°æ®")

            logger.success(f"  âœ… æ ¸å¿ƒæ–¹é¢ '{aspect}' æ•°æ®æ”¶é›†å®Œæˆï¼Œå…± {len(aspect_results)} æ¡")
            return aspect, aspect_results

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ–¹é¢çš„æ•°æ®æ”¶é›†
        aspects = list(search_strategy.get("analysis", {}).items())

        with ThreadPoolExecutor(max_workers=min(len(aspects), 10)) as executor:
            results = list(executor.map(collect_aspect_data, aspects))

        # æ•´ç†ç»“æœ
        for aspect, results in results:
            aspect_raw_data[aspect] = results

        logger.success(f"ğŸ“Š æ‰€æœ‰æ ¸å¿ƒæ–¹é¢æ•°æ®æ”¶é›†å®Œæˆï¼Œå…± {len(aspect_raw_data)} ä¸ªæ–¹é¢")
        return aspect_raw_data

    def process_aspect_data_concurrent(
        self, aspect_raw_data: Dict[str, List[Dict[str, Any]]], question: str
    ) -> Dict[str, Dict[str, Any]]:
        """å¹¶å‘å¤„ç†æ¯ä¸ªæ ¸å¿ƒæ–¹é¢çš„æ•°æ®"""
        logger.info(f"\nğŸ“Š å¼€å§‹å¹¶å‘å¤„ç†æ ¸å¿ƒæ–¹é¢æ•°æ®")

        # å®šä¹‰å¤„ç†å•ä¸ªæ–¹é¢çš„å‡½æ•°
        def process_single_aspect(aspect_item):
            aspect, results = aspect_item
            logger.info(f"  å¤„ç†æ ¸å¿ƒæ–¹é¢: {aspect}")

            # ä½¿ç”¨LLMè¿‡æ»¤ä¸ç›¸å…³çš„URL
            filtered_results = self.filter_relevant_results(aspect, results, question)
            logger.info(f"  '{aspect}' è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} æ¡ç›¸å…³ç»“æœ")

            # ä½¿ç”¨trafilaturaè·å–å¹¶æ¸…æ´—å†…å®¹
            cleaned_results = self.fetch_and_clean_content(filtered_results)
            logger.info(f"  '{aspect}' æ¸…æ´—åè·å¾— {len(cleaned_results)} æ¡æœ‰æ•ˆå†…å®¹")

            # åˆ›å»ºä¸´æ—¶æ–‡æ¡£å†…å®¹
            temp_document_content = "\n\n".join(cleaned_results)

            # ç›´æ¥ä½¿ç”¨å½“å‰å®ä¾‹å¤„ç†å†…å®¹ï¼Œä¼ é€’ä¸´æ—¶å†…å®¹
            synthesized = self.synthesize_content(temp_document_content,aspect)
            logger.success(f"  âœ… æ ¸å¿ƒæ–¹é¢ '{aspect}' å¤„ç†å®Œæˆ")

            return aspect, synthesized

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ–¹é¢çš„å¤„ç†
        aspect_items = list(aspect_raw_data.items())

        with ThreadPoolExecutor(max_workers=min(len(aspect_items), 10)) as executor:
            results = list(executor.map(process_single_aspect, aspect_items))

        # æ•´ç†ç»“æœ
        aspect_contents = {}
        for aspect, synthesized in results:
            aspect_contents[aspect] = synthesized

        logger.success(f"ğŸ“Š æ‰€æœ‰æ ¸å¿ƒæ–¹é¢æ•°æ®å¤„ç†å®Œæˆ")
        return aspect_contents

    def process_aspects_with_derivation_concurrent(
        self, aspect_contents: Dict[str, Dict[str, Any]], question: str
    ) -> Dict[str, str]:
        """å¹¶å‘å¤„ç†æ¯ä¸ªæ ¸å¿ƒæ–¹é¢å¹¶å¤„ç†è¡ç”Ÿé—®é¢˜"""
        logger.info(f"\nğŸ”„ å¼€å§‹å¹¶å‘å¤„ç†æ ¸å¿ƒæ–¹é¢åŠå…¶è¡ç”Ÿé—®é¢˜")

        # å®šä¹‰å¤„ç†å•ä¸ªæ–¹é¢çš„å‡½æ•°
        def process_single_aspect_with_derivation(aspect_item):
            aspect, synthesized = aspect_item
            logger.info(f"  å¤„ç†æ ¸å¿ƒæ–¹é¢åŠå…¶è¡ç”Ÿ: {aspect}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¡ç”Ÿé—®é¢˜
            derivation_check = self.generate_aspect_answer_with_derivation_check(
                aspect, synthesized, question
            )

            # Create an empty citations list if we don't have actual citation data
            citations = []

            # If we have stored cleaned results for this aspect, extract citations
            if hasattr(self, "_cleaned_results_dict"):
                for results_id, results_dict in self._cleaned_results_dict.items():
                    for result in results_dict:
                        if "url" in result and "content" in result:
                            citations.append(
                                {
                                    "url": result.get("url", ""),
                                    "title": result.get("title", ""),
                                    "content": result.get("content", ""),
                                    "citation": result.get("citation", ""),
                                }
                            )

            # ç”Ÿæˆé’ˆå¯¹è¯¥æ–¹é¢çš„åŸºç¡€å›ç­”
            base_answer = self.generate_aspect_base_answer(aspect, synthesized, question, citations)

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¡ç”Ÿé—®é¢˜
            if derivation_check.get("derivation_needed", False) and derivation_check.get(
                "derivation_topics"
            ):
                # çœŸæ­£çš„é€’å½’å¤„ç†è¡ç”Ÿé—®é¢˜ï¼Œä»æ·±åº¦0å¼€å§‹
                derivation_results = self._recursive_process_derivation_concurrent(
                    aspect, derivation_check.get("derivation_topics", []), question, depth=0
                )
                # åˆå¹¶å½“å‰æ–¹é¢å’Œè¡ç”Ÿé—®é¢˜çš„å†…å®¹
                final_content = self.combine_aspect_with_derivations(
                    base_answer, derivation_results
                )
            else:
                final_content = base_answer

            logger.success(f"  âœ… æ ¸å¿ƒæ–¹é¢ '{aspect}' åŠå…¶è¡ç”Ÿå¤„ç†å®Œæˆ")
            return aspect, final_content

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æ–¹é¢çš„å¤„ç†
        aspect_items = list(aspect_contents.items())

        with ThreadPoolExecutor(max_workers=min(len(aspect_items), 10)) as executor:
            results = list(executor.map(process_single_aspect_with_derivation, aspect_items))

        # æ•´ç†ç»“æœ
        final_contents = {}
        for aspect, content in results:
            final_contents[aspect] = content

        logger.success(f"ğŸ”„ æ‰€æœ‰æ ¸å¿ƒæ–¹é¢åŠå…¶è¡ç”Ÿå¤„ç†å®Œæˆ")
        return final_contents

    def filter_relevant_results(
        self, aspect: str, results: List[Dict[str, Any]], question: str
    ) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMè¿‡æ»¤ä¸ä¸»é¢˜ç›¸å…³çš„ç»“æœ"""
        if not results:
            return []

        logger.info(f"  æ­£åœ¨è¿‡æ»¤ '{aspect}' æ–¹é¢çš„ç›¸å…³ç»“æœ...")

        # æ„å»ºæ‰¹é‡è¯„ä¼°è¯·æ±‚
        evaluation_items = []
        for i, result in enumerate(results):
            content = result.get("content", "")
            if not content:
                continue

            evaluation_items.append(
                {
                    "id": i,
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "content_preview": content[:500] + ("..." if len(content) > 500 else ""),
                }
            )

        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿åˆ¤æ–­å†…å®¹ä¸ç ”ç©¶ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚
        ä½ å°†æ”¶åˆ°ä¸€ç»„æœç´¢ç»“æœï¼Œæ¯ä¸ªç»“æœåŒ…æ‹¬æ ‡é¢˜ã€URLå’Œå†…å®¹é¢„è§ˆã€‚
        è¯·è¯„ä¼°æ¯ä¸ªç»“æœä¸ç»™å®šç ”ç©¶ä¸»é¢˜çš„ç›¸å…³ç¨‹åº¦ï¼Œåªä¿ç•™çœŸæ­£ç›¸å…³çš„å†…å®¹ã€‚"""

        user_prompt = f"""ç ”ç©¶é—®é¢˜: {question}
        å½“å‰ç ”ç©¶æ–¹é¢: {aspect}
        
        è¯·è¯„ä¼°ä»¥ä¸‹æœç´¢ç»“æœæ˜¯å¦ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³ã€‚å¯¹æ¯ä¸ªç»“æœè¿”å›ä¸€ä¸ªå¸ƒå°”å€¼(true/false)ï¼Œè¡¨ç¤ºæ˜¯å¦åº”è¯¥ä¿ç•™è¯¥ç»“æœ:
        
        {json.dumps(evaluation_items, ensure_ascii=False, indent=2)}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œé”®ä¸ºIDï¼Œå€¼ä¸ºå¸ƒå°”å€¼ï¼Œä¾‹å¦‚: {{"0": true, "1": false}}"""

        try:
            response = self.llm.generate_completion(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_format=True,
                model=self.llm_model,
                temperature=self.llm_temperature,
            )

            filtered_results = []
            if response and isinstance(response, dict):
                for i, result in enumerate(results):
                    if str(i) in response and response[str(i)] is True:
                        filtered_results.append(result)
            else:
                # å¦‚æœLLMè¯„ä¼°å¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰ç»“æœ
                logger.warning("  âš ï¸ LLMè¿‡æ»¤è¯„ä¼°å¤±è´¥ï¼Œä¿ç•™æ‰€æœ‰ç»“æœ")
                filtered_results = results

            return filtered_results

        except Exception as e:
            logger.error(f"  âš ï¸ è¿‡æ»¤ç›¸å…³ç»“æœæ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶ä¿ç•™æ‰€æœ‰ç»“æœ
            return results

    def fetch_and_clean_content(self, results: List[Dict[str, Any]]) -> List[str]:
        """Using trafilatura to concurrently fetch and clean content with citation tracking"""
        if not results:
            return []

        logger.info(f"  æ­£åœ¨å¹¶å‘æ¸…æ´— {len(results)} æ¡å†…å®¹...")

        cleaned_results_dict = []  # Will store the full result dictionaries

        def clean_result(result):
            url = result.get("url", "")
            title = result.get("title", "æ— æ ‡é¢˜")
            raw_content = result.get("raw_content", "")
            published_date = result.get("published_date", "æœªçŸ¥")

            # Use ContentCleaner to clean content
            cleaned_content = self.cleaner.clean_with_trafilatura(url, raw_content, title)

            # Use original content if cleaning fails
            if not cleaned_content:
                cleaned_content = raw_content or result.get("content", "")

            # Track this source in the document (if you've implemented the document class)
            if hasattr(self, "document"):
                self.document.add_content("", url=url, title=title, published_date=published_date)
                citation = self.document.get_citation_for_url(url)
            else:
                citation = ""

            result_dict = {
                "url": url,
                "title": title,
                "published_date": published_date,
                "score": result.get("score", 0),
                "content": cleaned_content,
                "citation": citation,
                "formatted_content": (
                    f"æ¥æº: {url} {citation}\næ ‡é¢˜: {title}\nå‘å¸ƒæ—¥æœŸ: {published_date}\n"
                    + f"ç›¸å…³åº¦è¯„åˆ†: {result.get('score', 0)}\nå†…å®¹æ‘˜è¦:\n{cleaned_content[:1000]}...\n\n"
                    + f"å®Œæ•´å†…å®¹:\n{cleaned_content}\n\n---\n"
                ),
            }

            # Store the full dict for later use
            cleaned_results_dict.append(result_dict)

            # Return only the formatted content string
            return result_dict["formatted_content"]

        # Concurrently process
        with ThreadPoolExecutor(max_workers=min(len(results), 10)) as executor:
            cleaned_results = list(executor.map(clean_result, results))

        logger.success(f"  âœ… å¹¶å‘æ¸…æ´—å®Œæˆï¼Œå¤„ç†äº† {len(cleaned_results)} æ¡å†…å®¹")

        # Store the dictionaries for later reference (if you need access to the structured data)
        if not hasattr(self, "_cleaned_results_dict"):
            self._cleaned_results_dict = {}
        self._cleaned_results_dict[id(results)] = cleaned_results_dict

        return cleaned_results

    def generate_aspect_answer(
        self, aspect: str, synthesized: Dict[str, Any], question: str
    ) -> str:
        """æ ¹æ®ç»¼åˆå†…å®¹ç”Ÿæˆæ ¸å¿ƒæ–¹é¢çš„å›ç­”"""
        logger.info(f"  ç”Ÿæˆ '{aspect}' æ–¹é¢çš„å›ç­”...")

        # æ„å»ºç”¨äºå›ç­”çš„å†…å®¹
        content = ""

        # æ·»åŠ ä¸»é¢˜
        if "main_themes" in synthesized and synthesized["main_themes"]:
            content += "### ä¸»è¦ä¸»é¢˜\n"
            for theme in synthesized["main_themes"]:
                content += f"* {theme}\n"
            content += "\n"

        # æ·»åŠ å…³é”®äº‹å®
        if "key_facts" in synthesized and synthesized["key_facts"]:
            content += "### å…³é”®äº‹å®\n"
            for theme, facts in synthesized["key_facts"].items():
                content += f"**{theme}**:\n"
                for fact in facts:
                    content += f"* {fact}\n"
                content += "\n"

        # æ·»åŠ äº‰è®®ç‚¹
        if "controversies" in synthesized and synthesized["controversies"]:
            content += "### å­˜åœ¨äº‰è®®çš„è§‚ç‚¹\n"
            for controversy in synthesized["controversies"]:
                content += f"* {controversy}\n"
            content += "\n"

        # æ·»åŠ æ¥æº
        if "sources" in synthesized and synthesized["sources"]:
            content += "### ä¿¡æ¯æ¥æº\n"
            for source in synthesized["sources"]:
                content += f"* {source}\n"
            content += "\n"

        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ä»æ•´ç†å¥½çš„èµ„æ–™ä¸­æç‚¼å‡ºæ¸…æ™°ã€å…¨é¢çš„ç­”æ¡ˆã€‚
        è¯·åŸºäºæä¾›çš„æ•´ç†èµ„æ–™ï¼Œå°±ç‰¹å®šç ”ç©¶æ–¹é¢æä¾›è¯¦ç»†ã€å‡†ç¡®çš„å›ç­”ã€‚"""

        user_prompt = f"""ç ”ç©¶é—®é¢˜: {question}
        ç ”ç©¶æ–¹é¢: {aspect}
        
        æ•´ç†èµ„æ–™:
        {content}
        
        è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œå¯¹"{aspect}"è¿™ä¸€ç ”ç©¶æ–¹é¢æä¾›è¯¦å°½ã€å®¢è§‚çš„åˆ†æã€‚è¦æ±‚:
        1. ä»å¤šè§’åº¦åˆ†æè¿™ä¸€ç ”ç©¶æ–¹é¢
        2. å¿…è¦æ—¶å¼•ç”¨å…³é”®äº‹å®å’Œæ•°æ®
        3. å‘ˆç°ä¸åŒè§‚ç‚¹ï¼ˆå¦‚æœ‰ï¼‰
        4. ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€
        5. ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡å†…å®¹ï¼Œåˆç†ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨å’Œå¼ºè°ƒ
        """

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=user_prompt, json_format=False
        )

        result=cast(str,result)
        return result

    def generate_aspect_answer_with_derivation_check(
        self, aspect: str, synthesized: Dict[str, Any], question: str
    ) -> Dict[str, Any]:
        """æ ¹æ®ç»¼åˆå†…å®¹æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¡ç”Ÿé—®é¢˜"""
        logger.info(f"  è¯„ä¼° '{aspect}' æ–¹é¢æ˜¯å¦éœ€è¦è¡ç”Ÿ...")

        # æ„å»ºç”¨äºè¯„ä¼°çš„å†…å®¹
        content = ""

        # æ·»åŠ ä¸»é¢˜
        if "main_themes" in synthesized and synthesized["main_themes"]:
            content += "### ä¸»è¦ä¸»é¢˜\n"
            for theme in synthesized["main_themes"]:
                content += f"* {theme}\n"
            content += "\n"

        # æ·»åŠ å…³é”®äº‹å®
        if "key_facts" in synthesized and synthesized["key_facts"]:
            content += "### å…³é”®äº‹å®\n"
            for theme, facts in synthesized["key_facts"].items():
                content += f"**{theme}**:\n"
                for fact in facts:
                    content += f"* {fact}\n"
                content += "\n"

        # æ·»åŠ äº‰è®®ç‚¹
        if "controversies" in synthesized and synthesized["controversies"]:
            content += "### å­˜åœ¨äº‰è®®çš„è§‚ç‚¹\n"
            for controversy in synthesized["controversies"]:
                content += f"* {controversy}\n"
            content += "\n"

        # æ·»åŠ æ¥æº
        if "sources" in synthesized and synthesized["sources"]:
            content += "### ä¿¡æ¯æ¥æº\n"
            for source in synthesized["sources"]:
                content += f"* {source}\n"
            content += "\n"

        # ä¸»è¦å…³æ³¨ä¿¡æ¯ç©ºç™½
        missing_info = synthesized.get("missing_info", [])
        if missing_info:
            content += "### ä¿¡æ¯ç©ºç™½\n"
            for info in missing_info:
                content += f"* {info}\n"
            content += "\n"

        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­ç ”ç©¶å†…å®¹çš„å®Œæ•´æ€§ã€‚
    è¯·åŸºäºå·²æœ‰èµ„æ–™ï¼Œåˆ¤æ–­æ˜¯å¦é’ˆå¯¹å·²è¯†åˆ«çš„ä¿¡æ¯ç©ºç™½ç‚¹è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚"""

        user_prompt = f"""ç ”ç©¶é—®é¢˜: {question}
    ç ”ç©¶æ–¹é¢: {aspect}
    
    æ•´ç†èµ„æ–™:
    {content}
    
    è¯·ç›´æ¥è¯„ä¼°ï¼Œå¯¹äº"{aspect}"è¿™ä¸€ç ”ç©¶æ–¹é¢ï¼Œç°æœ‰ææ–™æ˜¯å¦è¶³å¤Ÿå…¨é¢ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦é’ˆå¯¹æŸäº›ä¿¡æ¯ç©ºç™½è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
    
    è¯·ä»…è¿”å›æœ‰æ•ˆçš„JSONå¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ:
    - "derivation_needed": å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶
    - "derivation_topics": æ•°ç»„ï¼Œå¦‚æœderivation_neededä¸ºtrueï¼ŒåŒ…å«éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„ä¿¡æ¯ç©ºç™½ç‚¹(æœ€å¤š3ä¸ªï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº)
    - "reason": å­—ç¬¦ä¸²ï¼Œç®€è¦è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦æˆ–ä¸éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶
    """

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=user_prompt, json_format=True
        )

        # ç¡®ä¿è¿”å›çš„æ˜¯å­—å…¸ç±»å‹
        if not result or not isinstance(result, dict):
            logger.warning(f"  âš ï¸ è¯„ä¼° '{aspect}' æ–¹é¢æ˜¯å¦éœ€è¦è¡ç”Ÿæ—¶å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼")
            return {
                "derivation_needed": False,
                "derivation_topics": [],
                "reason": "è¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤ä¸éœ€è¦è¡ç”Ÿ",
            }

        # ç¡®ä¿åŒ…å«å¿…è¦å­—æ®µ
        if "derivation_needed" not in result:
            result["derivation_needed"] = False
        if "derivation_topics" not in result:
            result["derivation_topics"] = []
        if "reason" not in result:
            result["reason"] = "æœªæä¾›ç†ç”±"

        return result

    def check_if_derivation_needed(
        self, aspect: str, answer: str, synthesized: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¡ç”Ÿé—®é¢˜"""
        missing_info = synthesized.get("missing_info", [])
        if not missing_info:
            return {"needed": False}

        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šç ”ç©¶è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè´Ÿè´£åˆ¤æ–­ç ”ç©¶å›ç­”çš„å®Œæ•´æ€§ã€‚
        è¯·åˆ¤æ–­å½“å‰å›ç­”æ˜¯å¦è¶³å¤Ÿå®Œæ•´ï¼Œæˆ–æ˜¯å¦éœ€è¦é’ˆå¯¹å·²è¯†åˆ«å‡ºçš„ä¿¡æ¯ç©ºç™½ç‚¹è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚"""

        user_prompt = f"""ç ”ç©¶æ–¹é¢: {aspect}
        
        å½“å‰å›ç­”:
        {answer}
        
        å·²è¯†åˆ«çš„ä¿¡æ¯ç©ºç™½:
        {json.dumps(missing_info, ensure_ascii=False, indent=2)}
        
        è¯·è¯„ä¼°å½“å‰å›ç­”æ˜¯å¦å·²ç»å……åˆ†è¦†ç›–äº†ä¸»è¦å†…å®¹ï¼Œæˆ–è€…æ˜¯å¦éœ€è¦é’ˆå¯¹æŸäº›ä¿¡æ¯ç©ºç™½è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚
        ä»¥JSONæ ¼å¼è¿”å›ä»¥ä¸‹å†…å®¹:
        1. "needed": å¸ƒå°”å€¼ï¼Œè¡¨ç¤ºæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶
        2. "priorities": æ•°ç»„ï¼Œå¦‚æœneededä¸ºtrueï¼ŒåŒ…å«æŒ‰ä¼˜å…ˆçº§æ’åºçš„éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„ä¿¡æ¯ç©ºç™½ç‚¹ï¼ˆæœ€å¤š3ä¸ªï¼‰
        3. "reason": å­—ç¬¦ä¸²ï¼Œè§£é‡Šä¸ºä»€ä¹ˆéœ€è¦æˆ–ä¸éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶
        """

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=user_prompt, json_format=True
        )

        # å¦‚æœè¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤ä¸éœ€è¦è¡ç”Ÿ
        if not result or not isinstance(result, dict):
            return {"needed": False}

        return result

    def process_derivation_if_needed(
        self, aspect: str, context: str, synthesized: Dict[str, Any], question: str, depth: int = 0
    ) -> Dict[str, str]:
        """å¤„ç†è¡ç”Ÿé—®é¢˜ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§è¡ç”Ÿæ·±åº¦
        if depth >= self.max_derivation_depth:
            logger.info(f"  âš ï¸ '{aspect}' å·²è¾¾åˆ°æœ€å¤§è¡ç”Ÿæ·±åº¦ {self.max_derivation_depth}")
            return {}

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†è¡ç”Ÿé—®é¢˜
        evaluation = self.check_if_derivation_needed(aspect, context, synthesized)

        if not evaluation.get("needed", False):
            logger.info(f"  âœ… '{aspect}' æ–¹é¢çš„å›ç­”å·²è¶³å¤Ÿå®Œå–„ï¼Œæ— éœ€å¤„ç†è¡ç”Ÿé—®é¢˜")
            return {}

        # è·å–éœ€è¦å¤„ç†çš„è¡ç”Ÿé—®é¢˜
        derivation_topics = evaluation.get("priorities", [])
        if not derivation_topics:
            return {}

        logger.info(
            f"  ğŸ”„ '{aspect}' æ–¹é¢éœ€è¦å¤„ç† {len(derivation_topics)} ä¸ªè¡ç”Ÿé—®é¢˜ (æ·±åº¦: {depth})"
        )

        derivation_results = {}
        for topic in derivation_topics:
            derivation_aspect = f"{aspect} - {topic}"
            logger.info(f"    ğŸ“ å¤„ç†è¡ç”Ÿé—®é¢˜: {topic} (æ·±åº¦: {depth})")

            # ä¸ºè¡ç”Ÿé—®é¢˜åˆ›å»ºæœç´¢çŸ­è¯­
            search_queries = self.generate_derivation_queries(question, aspect, topic)

            # æ”¶é›†è¡ç”Ÿé—®é¢˜çš„æ•°æ®
            results = []
            for query in search_queries:
                search_results = self.search_provider.search(query)
                results.extend(search_results)

            # è¿‡æ»¤ç›¸å…³ç»“æœ
            filtered_results = self.filter_relevant_results(derivation_aspect, results, question)

            # æ¸…æ´—å†…å®¹
            cleaned_results = self.fetch_and_clean_content(filtered_results)

            # ä¸ºè¡ç”Ÿé—®é¢˜åˆ›å»ºä¸´æ—¶æ–‡æ¡£
            temp_document = ResearchDocument(f"{question} - {derivation_aspect}")

            # æ·»åŠ æ¸…æ´—åçš„å†…å®¹
            for result in cleaned_results:
                temp_document.add_content(result)

            # åˆ›å»ºä¸€ä¸ªæ–°çš„å¤„ç†å™¨å®ä¾‹æ¥é¿å…æ–‡æ¡£çŠ¶æ€å†²çª
            temp_agent = DeepResearchAgent(
                google_api_key=self.search_provider.api_key, google_cx=self.search_provider.cx,
                openai_api_key=self.llm.client.api_key
            )
            temp_agent.document = temp_document
            temp_agent.max_derivation_depth = self.max_derivation_depth  # ç¡®ä¿ç»§æ‰¿è¡ç”Ÿæ·±åº¦è®¾ç½®

            # å¤„ç†å†…å®¹
            synthesized_derivation = temp_agent.synthesize_content()

            # ç”Ÿæˆè¡ç”Ÿé—®é¢˜çš„å›ç­”
            derivation_answer = temp_agent.generate_aspect_answer(
                derivation_aspect, synthesized_derivation, question
            )

            # å¦‚æœæ·±åº¦è¿˜æ²¡æœ‰è¾¾åˆ°æœ€å¤§å€¼ï¼Œé€’å½’å¤„ç†ä¸‹ä¸€çº§è¡ç”Ÿé—®é¢˜
            if depth + 1 < self.max_derivation_depth:
                sub_derivations = temp_agent.process_derivation_if_needed(
                    derivation_aspect,
                    derivation_answer,
                    synthesized_derivation,
                    question,
                    depth=depth + 1,
                )

                # å¦‚æœæœ‰å­è¡ç”Ÿé—®é¢˜çš„ç»“æœï¼Œå°†å…¶åˆå¹¶åˆ°å½“å‰è¡ç”Ÿé—®é¢˜çš„å›ç­”ä¸­
                if sub_derivations:
                    derivation_results[topic] = temp_agent.combine_aspect_with_derivations(
                        derivation_answer, sub_derivations
                    )
            else:
                # è¾¾åˆ°æœ€å¤§æ·±åº¦æ—¶ï¼Œç›´æ¥è¿”å›è¡ç”Ÿé—®é¢˜çš„å›ç­”
                derivation_results[topic] = derivation_answer

        return derivation_results

    def generate_derivation_queries(self, question: str, aspect: str, topic: str) -> List[str]:
        """ç”Ÿæˆè¡ç”Ÿé—®é¢˜çš„æœç´¢çŸ­è¯­"""
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æœç´¢ç­–ç•¥ä¸“å®¶ï¼Œæ“…é•¿åˆ›å»ºç²¾ç¡®çš„æœç´¢çŸ­è¯­ã€‚
    è¯·ä¸ºç»™å®šçš„ç ”ç©¶é—®é¢˜ã€æ ¸å¿ƒæ–¹é¢å’Œè¡ç”Ÿä¸»é¢˜åˆ›å»º3-5ä¸ªæœ‰æ•ˆçš„æœç´¢çŸ­è¯­ã€‚
    ä½ å¿…é¡»ä»¥æœ‰æ•ˆçš„JSONæ ¼å¼è¿”å›ç»“æœï¼Œå³ä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„ã€‚"""

        user_prompt = f"""ç ”ç©¶é—®é¢˜: {question}
    æ ¸å¿ƒæ–¹é¢: {aspect}
    è¡ç”Ÿä¸»é¢˜: {topic}
    
    è¯·åˆ›å»º1-2ä¸ªé’ˆå¯¹è¿™ä¸ªè¡ç”Ÿä¸»é¢˜çš„ç²¾ç¡®æœç´¢çŸ­è¯­ï¼Œç¡®ä¿:
    1. åŒ…å«è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡å’Œå…³é”®è¯
    2. é’ˆå¯¹æ€§å¼ºï¼Œé€‚åˆæœç´¢å¼•æ“ä½¿ç”¨
    3. æ¯ä¸ªçŸ­è¯­æœ‰ä¸åŒçš„è§’åº¦æˆ–é‡ç‚¹
    
    è¿”å›ä¸€ä¸ªæœç´¢çŸ­è¯­æ•°ç»„ï¼Œæ ¼å¼å¿…é¡»æ˜¯æœ‰æ•ˆçš„JSONæ•°ç»„ï¼Œä¾‹å¦‚:
    ["æœç´¢çŸ­è¯­1", "æœç´¢çŸ­è¯­2"]
    
    ä¸è¦åŒ…å«ä»»ä½•é¢å¤–è§£é‡Šï¼Œåªè¿”å›JSONæ•°ç»„ã€‚
    """

        result = self.llm.generate_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            json_format=True,
        )

        # å¦‚æœLLMè¿”å›æ ¼å¼ä¸å¯¹ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢
        if not isinstance(result, list):
            return [
                f"{question} {aspect} {topic}",
                f"{topic} in {aspect}",
                f"{aspect} {topic} research",
            ]

        return result

    def combine_aspect_with_derivations(
        self, context: str, derivation_results: Dict[str, str], max_tokens: int = 2000
    ) -> str:
        """é€šè¿‡LLMæ™ºèƒ½åˆå¹¶æ–¹é¢å†…å®¹ä¸å…¶è¡ç”Ÿå†…å®¹ï¼Œå¹¶æ§åˆ¶è¾“å‡ºé•¿åº¦

        Args:
            context: åŸå§‹å†…å®¹
            derivation_results: è¡ç”Ÿé—®é¢˜çš„ç»“æœå­—å…¸ï¼Œé”®ä¸ºä¸»é¢˜ï¼Œå€¼ä¸ºå†…å®¹
            max_tokens: ç”Ÿæˆè¾“å‡ºçš„æœ€å¤§tokenæ•°

        Returns:
            åˆå¹¶åçš„å†…å®¹
        """
        # å¦‚æœæ²¡æœ‰è¡ç”Ÿç»“æœï¼Œç›´æ¥è¿”å›åŸæ–‡
        if not derivation_results:
            return context

        # ç¡®ä¿contextæ˜¯å­—ç¬¦ä¸²ç±»å‹
        if not isinstance(context, str):
            logger.warning("  âš ï¸ åœ¨åˆå¹¶è¡ç”Ÿå†…å®¹æ—¶æ”¶åˆ°éå­—ç¬¦ä¸²ç±»å‹çš„contextï¼Œè¿›è¡Œè½¬æ¢")
            context = str(context) if context is not None else ""

        # å‡†å¤‡æ‰€æœ‰ææ–™ï¼Œä»¥ä¾¿LLMæ•´åˆ
        materials = [f"## ä¸»è¦å†…å®¹\n\n{context}"]

        for topic, content in derivation_results.items():
            if content and isinstance(content, str) and content.strip():
                materials.append(f"## å»¶ä¼¸: {topic}\n\n{content}")
            elif content:  # å¦‚æœå†…å®¹ä¸æ˜¯ç©ºçš„ä½†ä¸æ˜¯å­—ç¬¦ä¸²
                materials.append(f"## å»¶ä¼¸: {topic}\n\n{str(content)}")

        all_materials = "\n\n".join(materials)

        # æ„å»ºæç¤ºè¯­
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶å†…å®¹æ•´åˆä¸“å®¶ï¼Œæ“…é•¿å°†ä¸»è¦å†…å®¹ä¸å»¶ä¼¸å†…å®¹èåˆæˆä¸€ä¸ªè¿è´¯ã€å…¨é¢ä½†ç®€æ´çš„æ•´ä½“ã€‚
    è¯·ç»¼åˆæ‰€æœ‰æä¾›çš„ææ–™ï¼Œåˆ›å»ºä¸€ä¸ªç»“æ„æ¸…æ™°çš„ç»Ÿä¸€æ–‡æ¡£ï¼Œä¿æŒé‡è¦ä¿¡æ¯çš„åŒæ—¶é¿å…å†—ä½™ã€‚"""

        user_prompt = f"""è¯·å°†ä»¥ä¸‹ç ”ç©¶å†…å®¹æ•´åˆæˆä¸€ä¸ªè¿è´¯ã€å…¨é¢çš„æ–‡æ¡£ï¼š

{all_materials}

è¦æ±‚:
1. ä¼˜å…ˆä¿ç•™ä¸»è¦å†…å®¹ä¸­çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯
2. å°†å»¶ä¼¸å†…å®¹ä¸­çš„é‡è¦è§è§£å’Œè¡¥å……ä¿¡æ¯èå…¥é€‚å½“ä½ç½®
3. æ¶ˆé™¤é‡å¤å†…å®¹ï¼Œè§£å†³å¯èƒ½å­˜åœ¨çš„çŸ›ç›¾è§‚ç‚¹
4. ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å±‚æ¬¡å’ŒMarkdownæ ¼å¼
5. ä¿æŒä¸“ä¸šã€å®¢è§‚çš„è¯­æ°”
6. ç¡®ä¿æœ€ç»ˆæ–‡æ¡£ç»“æ„æ¸…æ™°ã€é€»è¾‘è¿è´¯

æœ€ç»ˆè¾“å‡ºåº”ä¸ºä¸€ä¸ªMarkdownæ ¼å¼çš„ç»¼åˆæ–‡æ¡£ï¼Œé•¿åº¦é€‚ä¸­ï¼Œå†…å®¹å…¨é¢ã€‚"""

        try:
            result = self.llm.generate_completion(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                json_format=False,
                max_tokens=max_tokens,
                temperature=0.1,  # ä½¿ç”¨è¾ƒä½çš„temperatureä»¥ç¡®ä¿å†…å®¹å¿ å®
            )

            combined_content = cast(str, result)
            # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°ç®€å•åˆå¹¶
            if not combined_content:
                logger.warning("  âš ï¸ LLMåˆå¹¶å†…å®¹å¤±è´¥ï¼Œå›é€€åˆ°ç®€å•åˆå¹¶")
                return self._simple_combine_aspects(context, derivation_results)

            return combined_content

        except Exception as e:
            logger.error(f"  âš ï¸ LLMåˆå¹¶å†…å®¹æ—¶å‡ºé”™: {e}")
            # å‡ºé”™æ—¶å›é€€åˆ°ç®€å•åˆå¹¶
            return self._simple_combine_aspects(context, derivation_results)

    def _simple_combine_aspects(self, context: str, derivation_results: Dict[str, str]) -> str:
        """ç®€å•åˆå¹¶ä¸»è¦å†…å®¹å’Œè¡ç”Ÿå†…å®¹ï¼ˆä½œä¸ºLLMåˆå¹¶çš„å›é€€æ–¹æ¡ˆï¼‰"""
        combined = []
        if context.strip():
            combined.append(context)

        for topic, content in derivation_results.items():
            if isinstance(content, str) and content.strip():
                combined.append(f"### å»¶ä¼¸: {topic}\n\n{content}")
            elif content:
                combined.append(f"### å»¶ä¼¸: {topic}\n\n{str(content)}")

        return "\n\n".join(combined)

    def generate_aspect_base_answer(
        self,
        aspect: str,
        synthesized: Dict[str, Any],
        question: str,
        citations: List[Dict[str, Any]],
    ) -> str:
        """Generate base answer with citations for a research aspect"""
        logger.info(f"  ç”Ÿæˆ '{aspect}' æ–¹é¢çš„åŸºç¡€å›ç­”...")

        # Build content with citations
        content = ""

        # Add themes
        if "main_themes" in synthesized and synthesized["main_themes"]:
            content += "### ä¸»è¦ä¸»é¢˜\n"
            for theme in synthesized["main_themes"]:
                if theme:  # Check if theme is not empty
                    content += f"* {theme}\n"
            content += "\n"

        # Add key facts with citations
        if "key_facts" in synthesized and synthesized["key_facts"]:
            content += "### å…³é”®äº‹å®\n"
            for theme, facts in synthesized["key_facts"].items():
                if theme:  # Check if theme is not empty
                    content += f"**{theme}**:\n"
                    for fact in facts:
                        if fact:  # Check if fact is not empty
                            # Try to find citation for this fact
                            citation_refs = self._find_citations_for_content(fact, citations)
                            citation_text = " " + " ".join(citation_refs) if citation_refs else ""
                            content += f"* {fact}{citation_text}\n"
                    content += "\n"

        # Add controversies with citations
        if "controversies" in synthesized and synthesized["controversies"]:
            content += "### å­˜åœ¨äº‰è®®çš„è§‚ç‚¹\n"
            for controversy in synthesized["controversies"]:
                if controversy:  # Check if controversy is not empty
                    # Try to find citation
                    citation_refs = self._find_citations_for_content(controversy, citations)
                    citation_text = " " + " ".join(citation_refs) if citation_refs else ""
                    content += f"* {controversy}{citation_text}\n"
            content += "\n"

        # Add sources with citation IDs
        if "sources" in synthesized and synthesized["sources"]:
            content += "### ä¿¡æ¯æ¥æº\n"
            for i, source in enumerate(synthesized["sources"]):
                if source:  # Check if source is not empty
                    url = self._extract_url_from_source(str(source))
                    citation = self.document.get_citation_for_url(url) if url else f"[S{i + 1}]"
                    content += f"* {source} {citation}\n"
            content += "\n"

        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ä»æ•´ç†å¥½çš„èµ„æ–™ä¸­æç‚¼å‡ºæ¸…æ™°ã€å…¨é¢çš„ç­”æ¡ˆã€‚
    è¯·åŸºäºæä¾›çš„æ•´ç†èµ„æ–™ï¼Œå¯¹ç‰¹å®šç ”ç©¶æ–¹é¢æä¾›è¯¦å°½ã€å®¢è§‚çš„åˆ†æã€‚
    éå¸¸é‡è¦ï¼šå½“å¼•ç”¨èµ„æ–™ä¸­çš„äº‹å®æˆ–è§‚ç‚¹æ—¶ï¼Œè¯·ä¿ç•™æ–¹æ‹¬å·ä¸­çš„å¼•ç”¨æ ‡è®°ï¼ˆå¦‚[1]ã€[2]ç­‰ï¼‰ã€‚"""

        user_prompt = f"""ç ”ç©¶é—®é¢˜: {question}
    ç ”ç©¶æ–¹é¢: {aspect}

    æ•´ç†èµ„æ–™:
    {content}

    è¯·åŸºäºä»¥ä¸Šèµ„æ–™ï¼Œå¯¹"{aspect}"è¿™ä¸€ç ”ç©¶æ–¹é¢æä¾›è¯¦å°½ã€å®¢è§‚çš„åˆ†æã€‚è¦æ±‚:
    - ä»å¤šè§’åº¦åˆ†æè¿™ä¸€ç ”ç©¶æ–¹é¢
    - å¿…è¦æ—¶å¼•ç”¨å…³é”®äº‹å®å’Œæ•°æ®ï¼Œä¿ç•™åŸå§‹å¼•ç”¨æ ‡è®°ï¼ˆå¦‚[1]ã€[2]ï¼‰
    - å‘ˆç°ä¸åŒè§‚ç‚¹ï¼ˆå¦‚æœ‰ï¼‰
    - ä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šçš„è¯­è¨€
    - ä½¿ç”¨Markdownæ ¼å¼ç»„ç»‡å†…å®¹ï¼Œåˆç†ä½¿ç”¨æ ‡é¢˜ã€åˆ—è¡¨å’Œå¼ºè°ƒ"""

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=user_prompt, json_format=False, max_tokens=2000
        )
        result = cast(str, result)
        return result

    def _find_citations_for_content(
        self, content_text: str, citations: List[Dict[str, Any]]
    ) -> List[str]:
        """Find relevant citations for a piece of content"""
        relevant_citations = []

        # Make sure content_text is a string
        if not isinstance(content_text, str):
            # If it's a dictionary, try to extract text content from common keys
            if isinstance(content_text, dict):
                if "text" in content_text:
                    content_text = content_text["text"]
                elif "content" in content_text:
                    content_text = content_text["content"]
                else:
                    # Convert the entire dict to string as fallback
                    content_text = str(content_text)
            else:
                # Convert to string as fallback for other types
                content_text = str(content_text)

        # Now that content_text is definitely a string, proceed with search
        for citation in citations:
            if "content" not in citation or not citation["content"]:
                continue

            citation_content = citation["content"]
            # Make sure citation_content is a string
            if not isinstance(citation_content, str):
                citation_content = str(citation_content)

            # Check if content appears in citation
            if content_text.lower() in citation_content.lower():
                relevant_citations.append(citation["citation"])

        return relevant_citations

    def _extract_url_from_source(self, source_text: str) -> str:
        """Extract URL from source text if present"""
        # Simple extraction - look for http:// or https://
        if "http://" in source_text:
            start_idx = source_text.find("http://")
            end_idx = source_text.find(" ", start_idx)
            return source_text[start_idx:end_idx] if end_idx > 0 else source_text[start_idx:]
        elif "https://" in source_text:
            start_idx = source_text.find("https://")
            end_idx = source_text.find(" ", start_idx)
            return source_text[start_idx:end_idx] if end_idx > 0 else source_text[start_idx:]
        return ""

    def process_single_aspect(self, aspect_data):
        """Process a single research aspect"""
        aspect, result_list = aspect_data

        logger.info(f"  å¤„ç†ç ”ç©¶æ–¹é¢: '{aspect}'...")

        # æ¸…ç†å†…å®¹
        cleaned_results = self.fetch_and_clean_content(result_list)

        # Extract the content strings from the cleaned results dictionaries
        content_strings = [result.get("formatted_content", "") for result in cleaned_results]

        # Join the content strings
        temp_document_content = "\n\n".join(content_strings)

        # Synthesize content
        synthesized = self.synthesize_content_for_aspect(aspect, temp_document_content)

        # Generate a base answer for this aspect
        aspect_answer = self.generate_aspect_base_answer(
            aspect, synthesized, self.question, cleaned_results
        )

        logger.success(f"  âœ… æˆåŠŸå¤„ç†äº†ç ”ç©¶æ–¹é¢ '{aspect}'")

        return {
            "aspect": aspect,
            "answer": aspect_answer,
            "synthesized": synthesized,
            "sources": [result.get("url") for result in cleaned_results],
            "citations": [
                {
                    "url": result.get("url"),
                    "title": result.get("title"),
                    "content": result.get("content", ""),
                    "citation": result.get("citation", ""),
                }
                for result in cleaned_results
            ],
        }

    def synthesize_content_for_aspect(self, aspect: str, content: str) -> Dict[str, Any]:
        """é’ˆå¯¹ç‰¹å®šç ”ç©¶æ–¹é¢åˆæˆå†…å®¹"""
        logger.info(f"  åˆæˆ '{aspect}' æ–¹é¢çš„å†…å®¹...")

        # ä½¿ç”¨synthesize_contentæ–¹æ³•ï¼Œä¼ å…¥ç‰¹å®šç ”ç©¶æ–¹é¢çš„å†…å®¹
        return self.synthesize_content(content)

    def _recursive_process_derivation_concurrent(
        self, parent_aspect: str, derivation_topics: List[str], question: str, depth: int = 0
    ) -> Dict[str, str]:
        """å¹¶å‘å¤„ç†è¡ç”Ÿä¸»é¢˜"""
        if depth >= self.max_derivation_depth:
            logger.info(f"  å·²è¾¾åˆ°æœ€å¤§è¡ç”Ÿæ·±åº¦ {self.max_derivation_depth}ï¼Œåœæ­¢è¿›ä¸€æ­¥è¡ç”Ÿ")
            return {}

        logger.info(
            f"  å¤„ç† '{parent_aspect}' çš„ {len(derivation_topics)} ä¸ªè¡ç”Ÿä¸»é¢˜ (æ·±åº¦: {depth})"
        )

        # å®šä¹‰å¤„ç†å•ä¸ªè¡ç”Ÿä¸»é¢˜çš„å‡½æ•°
        def process_single_derivation(topic):
            derivation_aspect = f"{parent_aspect} - {topic}"
            logger.info(f"    å¤„ç†è¡ç”Ÿä¸»é¢˜: '{topic}'")

            # ç”Ÿæˆè¡ç”Ÿä¸»é¢˜çš„æœç´¢çŸ­è¯­
            search_queries = self.generate_derivation_queries(question, parent_aspect, topic)
            logger.info(f"    ä¸º '{topic}' ç”Ÿæˆäº† {len(search_queries)} ä¸ªæœç´¢çŸ­è¯­")

            # æ”¶é›†è¡ç”Ÿä¸»é¢˜çš„æœç´¢ç»“æœ
            results = []
            for query in search_queries:
                search_results = self.search_provider.search(
                    query, max_results=max(3, self.search_max_results - 2)
                )
                results.extend(search_results)

            logger.info(f"    '{topic}' æ”¶é›†åˆ° {len(results)} æ¡æœç´¢ç»“æœ")

            # è¿‡æ»¤ç›¸å…³ç»“æœ
            filtered_results = self.filter_relevant_results(derivation_aspect, results, question)
            logger.info(f"    '{topic}' è¿‡æ»¤åå‰©ä½™ {len(filtered_results)} æ¡ç›¸å…³ç»“æœ")

            # æ¸…æ´—å†…å®¹
            cleaned_results = self.fetch_and_clean_content(filtered_results)

            # æå–å†…å®¹å­—ç¬¦ä¸²
            content_strings = []
            for result in cleaned_results:
                if isinstance(result, str):
                    content_strings.append(result)
                elif isinstance(result, dict) and "formatted_content" in result:
                    content_strings.append(result["formatted_content"])

            temp_document_content = "\n\n".join(content_strings)

            # ç»¼åˆå†…å®¹
            synthesized = self.synthesize_content(temp_document_content)

            # å‡†å¤‡å¼•ç”¨åˆ—è¡¨
            citations = []
            if hasattr(self, "_cleaned_results_dict"):
                for results_id, results_dict in self._cleaned_results_dict.items():
                    for result in results_dict:
                        if "url" in result and "content" in result:
                            citations.append(
                                {
                                    "url": result.get("url", ""),
                                    "title": result.get("title", ""),
                                    "content": result.get("content", ""),
                                    "citation": result.get("citation", ""),
                                }
                            )

            # ç”Ÿæˆå›ç­”
            answer = self.generate_aspect_base_answer(
                derivation_aspect, synthesized, question, citations
            )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥è¡ç”Ÿ
            if depth + 1 < self.max_derivation_depth:
                derivation_check = self.generate_aspect_answer_with_derivation_check(
                    derivation_aspect, synthesized, question
                )

                if derivation_check.get("derivation_needed", False) and derivation_check.get(
                    "derivation_topics"
                ):
                    sub_topics = derivation_check.get("derivation_topics", [])
                    logger.info(f"    '{topic}' éœ€è¦è¿›ä¸€æ­¥è¡ç”Ÿ {len(sub_topics)} ä¸ªå­ä¸»é¢˜")

                    # é€’å½’å¤„ç†å­è¡ç”Ÿä¸»é¢˜
                    sub_derivations = self._recursive_process_derivation_concurrent(
                        derivation_aspect, sub_topics, question, depth=depth + 1
                    )

                    # åˆå¹¶å½“å‰å›ç­”ä¸å­è¡ç”Ÿå†…å®¹
                    if sub_derivations:
                        answer = self.combine_aspect_with_derivations(answer, sub_derivations)

            logger.success(f"    âœ… è¡ç”Ÿä¸»é¢˜ '{topic}' å¤„ç†å®Œæˆ")
            return topic, answer

        # å¹¶å‘å¤„ç†æ‰€æœ‰è¡ç”Ÿä¸»é¢˜
        with ThreadPoolExecutor(max_workers=min(len(derivation_topics), 3)) as executor:
            results = list(executor.map(process_single_derivation, derivation_topics))

        # æ•´ç†ç»“æœ
        derivation_results = {}
        for topic, content in results:
            derivation_results[topic] = content

        return derivation_results
