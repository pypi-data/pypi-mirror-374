import os
import json
from typing import Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures

from ..utils import logger, get_current_date
from ..content import ContentCleaner, ResearchDocument
from ..search import TavilySearchProvider
from ..llm import LLMProvider


class DeepSearchAgent:
    """ç®€åŒ–ç‰ˆæœç´¢ä»£ç†"""

    def __init__(self, tavily_api_key=None, openai_api_key=None):
        # åˆå§‹åŒ–é…ç½®
        tavily_key = tavily_api_key or os.getenv("TAVILY_API_KEY")
        openai_key = openai_api_key or os.getenv("OPENAI_API_KEY")

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.search_provider = TavilySearchProvider(tavily_key)
        self.llm = LLMProvider(openai_key)
        self.cleaner = ContentCleaner()
        self.questions = None
        self.document = None
        self.all_content = ""

    def set_research_question(self, question: str) -> None:
        """è®¾ç½®ç ”ç©¶é—®é¢˜"""
        self.questions = question
        self.document = ResearchDocument(question)

    def generate_search_strategy(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœç´¢ç­–ç•¥"""
        logger.info("æ­£åœ¨ç”Ÿæˆæœç´¢ç­–ç•¥...")
        prompt = self.questions

        system_prompt = f"""å½“å‰æ˜¯{get_current_date()}ã€‚ä½ ç°åœ¨æ˜¯ä¸€ä½æœç´¢ç­–ç•¥ä¸“å®¶ã€‚å½“æˆ‘æè¿°ä¸€ä¸ªäº‹ä»¶æˆ–ä¸»é¢˜æ—¶ï¼Œè¯·ä½ æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿”å›ç»“æœï¼š
        1. åˆ†æè¯¥äº‹ä»¶æˆ–ä¸»é¢˜ï¼Œè¯†åˆ«å‡ºä¸ä¹‹ç›¸å…³çš„é‡è¦æ–¹é¢ï¼Œå¦‚èƒŒæ™¯ã€å…³é”®äººç‰©ã€å…³é”®äº‹ä»¶ã€å½±å“ç­‰ï¼ˆå»ºè®®4-5ä¸ªï¼‰ã€‚
        2. é’ˆå¯¹æ¯ä¸ªæ–¹é¢ï¼Œè¯·æä¾›1ä¸ªè¯¦ç»†çš„æœç´¢çŸ­è¯­ã€‚æ¯ä¸ªæœç´¢çŸ­è¯­åº”åŒ…å«è¶³å¤Ÿçš„æè¿°ä¿¡æ¯ï¼Œç¡®ä¿æŸ¥è¯¢å†…å®¹æ˜ç¡®æŒ‡å‘è¯¥äº‹ä»¶æˆ–ä¸»é¢˜çš„å…·ä½“æƒ…å†µï¼Œé¿å…æ­§ä¹‰ã€‚
        3. è¿”å›ç»“æœå¿…é¡»æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼ï¼Œå¹¶ä¸”åªåŒ…å«ä¸¤ä¸ªé¡¶çº§é”®ï¼š`event` å’Œ `analysis`ï¼Œå…¶ä¸­ï¼š
            - `event` çš„å€¼ä¸ºäº‹ä»¶æˆ–ä¸»é¢˜çš„åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚
            - "analysis" çš„å€¼ä¸ºä¸€ä¸ªå¯¹è±¡ï¼Œæ¯ä¸ªé”®ä»£è¡¨ä¸€ä¸ªæ–¹é¢ï¼Œå¯¹åº”çš„å€¼ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œä¸ºè¯¦ç»†çš„æœç´¢çŸ­è¯­ã€‚
        è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸Šè¦æ±‚ï¼Œå¹¶ä»…è¿”å›ç¬¦åˆè¦æ±‚çš„ JSON æ ¼å¼å†…å®¹ã€‚"""

        result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=prompt, json_format=True
        )
        logger.success("æœç´¢ç­–ç•¥ç”Ÿæˆå®Œæˆ")

        # å¦‚æœç»“æœä¸ºç©ºï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
        if not result:
            return {
                "event": self.questions,
                "analysis": {"default": self.questions},
            }

        return result

    def search_and_extract(self, search_query: str, max_results: int = 5) -> str:
        """æ‰§è¡Œæœç´¢å¹¶æå–å†…å®¹"""
        logger.info(f"ğŸ” æ­£åœ¨åˆ†æ: {search_query}")
        logger.info("  â”œâ”€ æœç´¢ç½‘ç»œèµ„æº...")

        # æœç´¢è·å–ç»“æœ
        results = self.search_provider.search(search_query, max_results)
        logger.success(f"  â”œâ”€ æœç´¢å®Œæˆï¼Œè·å–åˆ° {len(results)} æ¡ç»“æœ")

        # å¤„ç†ç»“æœ
        logger.info("  â””â”€ æå–å’Œæ¸…æ´—å†…å®¹...")
        cleaned_results = []

        for result in results:
            url = result.get("url", "")
            title = result.get("title", "æ— æ ‡é¢˜")
            raw_content = result.get("raw_content", "")

            # ä½¿ç”¨ContentCleaneræ¸…æ´—å†…å®¹
            cleaned_content = self.cleaner.clean_with_trafilatura(url, raw_content, title)

            # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            if not cleaned_content:
                cleaned_content = raw_content or result.get("content", "")

            # å¯¹æ¸…æ´—åçš„å†…å®¹è¿›è¡Œæ ¼å¼æ ‡å‡†åŒ–
            record = (
                f"æ¥æº: {url}\næ ‡é¢˜: {title}\nå‘å¸ƒæ—¥æœŸ: {result.get('published_date', 'æœªçŸ¥')}\n"
                + f"ç›¸å…³åº¦è¯„åˆ†: {result.get('score', 0)}\nå†…å®¹æ‘˜è¦:\n{cleaned_content[:1000]}...\n\n"
                + f"å®Œæ•´å†…å®¹:\n{cleaned_content}\n\n---\n"
            )
            cleaned_results.append(record)

        combined_result = "".join(cleaned_results)
        logger.success("  â””â”€ å†…å®¹æå–å®Œæˆ")

        # æ·»åŠ åˆ°ç ”ç©¶æ–‡æ¡£å’Œæ€»å†…å®¹
        self.document.add_content(combined_result)
        self.all_content += combined_result

        return combined_result

    def generate_report(self) -> str:
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        logger.info("\nğŸ“ æ­£åœ¨ç”Ÿæˆç ”ç©¶æŠ¥å‘Š...")

        prompt = f"""ä½ æ˜¯ä¸€ä½é¢†åŸŸç ”ç©¶å‘˜ã€‚è¯·æ ¹æ® **ç ”ç©¶é—®é¢˜** ä¸ **æ£€ç´¢èµ„æ–™**ï¼Œæ’°å†™ä¸€ä»½ç»“æ„æ¸…æ™°ã€ä¿¡æ¯å¯†é›†çš„ Markdown æŠ¥å‘Šã€‚
---
### ç ”ç©¶é—®é¢˜
{self.questions}
### æ£€ç´¢èµ„æ–™
{self.document.get_content(127000)}
---
## è¾“å‡ºè¦æ±‚
1. **æ ‡é¢˜**  
   - ç”¨ä¸€å¥è¯æ¦‚æ‹¬ç ”ç©¶ä¸»é¢˜ï¼Œä½œä¸ºä¸€çº§æ ‡é¢˜ `#`ã€‚
2. **åˆ†æ¡æ€»ç»“**  
   - ä»¥ `##` å¼•å‡ºå¤šä¸ªæ ¸å¿ƒä¸»é¢˜ï¼ˆæŒ‰é€»è¾‘æˆ–é‡è¦æ€§æ’åºï¼‰ã€‚  
   - ä¸»é¢˜ç¤ºä¾‹ï¼šæ¦‚å¿µå®šä¹‰ï¼Œç°çŠ¶æ¦‚è§ˆã€æŠ€æœ¯çªç ´ã€æ¡ˆä¾‹åˆ†æã€æŒ‘æˆ˜ä¸ç©ºç™½ã€æœªæ¥è¶‹åŠ¿ç­‰ã€‚  
   - åœ¨æ¯ä¸ªä¸»é¢˜ä¸‹ï¼Œç”¨æ— åºåˆ—è¡¨ `*` ç»™å‡º 5â€“8 æ¡å…³é”®ä¿¡æ¯ï¼›å¯åœ¨å¿…è¦å¤„æ’å…¥å­åˆ—è¡¨æˆ–è¡Œå†…ç²—ä½“/æ–œä½“å¼ºè°ƒã€‚  
   - æ¯æ¡ä¿¡æ¯ 1â€“3 å¥ï¼Œæ—¢æä¾›äº‹å®æ•°æ®ï¼Œåˆç»™å‡ºç®€è¦åˆ†ææˆ–æ´è§ã€‚
3. **ç¯‡å¹…**  
   - **æ€»å­—æ•° â‰¥ 2000 å­—**ã€‚  
4. **å¼•ç”¨**    
   - æ–‡æœ« `### å‚è€ƒèµ„æ–™` åˆ—å‡ºå®Œæ•´æ¥æºï¼šä½œè€…ï¼ˆå¦‚æœæœ‰ï¼‰/ æ ‡é¢˜ / é“¾æ¥ã€‚
5. **è¯­æ°”ä¸æ ¼å¼**  
   - ä¿æŒå®¢è§‚ã€ç²¾ç‚¼ã€é€»è¾‘æ¸…æ™°ï¼›é¿å…ä¸ç ”ç©¶æ— å…³çš„é—²è°ˆã€‚  
   - ä½¿ç”¨ Markdown æ ‡é¢˜ã€åˆ—è¡¨ã€è¡Œå†…æ ‡è®°ç­‰ï¼Œç¡®ä¿æ˜“è¯»ã€‚
"""

        # è·å–æµå¼è¾“å‡º
        stream = self.llm.generate_stream(
            system_prompt="You are a smart assistant.", user_prompt=prompt
        )

        if not stream:
            logger.error("æ— æ³•åˆ›å»ºæµå¼è¾“å‡º")
            return ""

        report = ""
        logger.info("\n--- ç ”ç©¶æŠ¥å‘Šå¼€å§‹ ---\n")

        try:
            for chunk in stream:
                # å®‰å…¨åœ°å¤„ç†æµå—
                if (
                    hasattr(chunk, "choices")
                    and chunk.choices
                    and chunk.choices[0].delta
                    and hasattr(chunk.choices[0].delta, "content")
                    and chunk.choices[0].delta.content
                ):
                    content = chunk.choices[0].delta.content
                    report += content
                    print(content, end="", flush=True)
        except Exception as e:
            logger.error(f"å¤„ç†æµè¾“å‡ºæ—¶å‡ºé”™: {e}")

        logger.info("\n\n--- ç ”ç©¶æŠ¥å‘Šç»“æŸ ---")

        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)

        return report

    def _save_report(self, report: str, filepath: str = "/tmp/result.md") -> None:
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(report)
        logger.success(f"æŠ¥å‘Šå·²ä¿å­˜åˆ° {filepath}")

    def run(self, question: str) -> str:
        """æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"""
        logger.info(f'\nğŸš€ å¼€å§‹ç ”ç©¶: "{question}"')
        
        # è®¾ç½®ç ”ç©¶é—®é¢˜
        self.set_research_question(question)

        # ç”Ÿæˆæœç´¢ç­–ç•¥
        search_strategy = self.generate_search_strategy()
        logger.info(f"ğŸ“Š ç ”ç©¶ä¸»é¢˜: {search_strategy.get('event', question)}")
        logger.info(f"ğŸ“Š åˆ†æç­–ç•¥: {len(search_strategy.get('analysis', {}))} ä¸ªå…³é”®æ–¹é¢")

        # è·å–æ‰€æœ‰æœç´¢æŸ¥è¯¢
        search_queries = list(search_strategy["analysis"].values())
        logger.info(f"ğŸ” æœç´¢ä»¥ä¸‹å…³é”®æ–¹é¢:")
        for i, query in enumerate(search_queries, 1):
            logger.info(f"  {i}. {query}")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œæœç´¢
        logger.info("\nğŸ”„ å¼€å§‹å¹¶è¡Œå¤„ç†æœç´¢æŸ¥è¯¢...")
        with ThreadPoolExecutor(max_workers=min(10, len(search_queries))) as executor:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_query = {executor.submit(self.search_and_extract, query): query for query in search_queries}
            
            # æ”¶é›†ç»“æœ
            complete_count = 0
            total_count = len(future_to_query)
            for future in concurrent.futures.as_completed(future_to_query):
                query = future_to_query[future]
                try:
                    # è·å–ç»“æœ (ä½†æˆ‘ä»¬å®é™…ä¸Šä¸éœ€è¦è¿”å›å€¼ï¼Œå› ä¸ºsearch_and_extractå·²ç»æ·»åŠ åˆ°self.document)
                    future.result()
                    complete_count += 1
                    logger.success(f"âœ… å·²å®Œæˆ {complete_count}/{total_count} ä¸ªæŸ¥è¯¢")
                except Exception as exc:
                    logger.error(f"âŒ æŸ¥è¯¢ '{query}' ç”Ÿæˆå¼‚å¸¸: {exc}")
        
        logger.success("\nâœ… æ‰€æœ‰æœç´¢æŸ¥è¯¢å¤„ç†å®Œæˆ!")

        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()

        return report

    def get_factual_background_and_entities(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """
        æ ¹æ®æŸ¥è¯¢è·å–äº‹å®èƒŒæ™¯æ€»ç»“å’Œå…³é”®å®ä½“/å…³é”®è¯
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: è·å–çš„æ–°é—»æœç´¢ç»“æœæ•°é‡
            
        Returns:
            åŒ…å«äº‹å®èƒŒæ™¯æ€»ç»“å’Œå…³é”®å®ä½“/å…³é”®è¯çš„å­—å…¸
        """
        logger.info(f"ğŸ” è·å–'{query}'çš„äº‹å®èƒŒæ™¯å’Œå…³é”®å®ä½“...")
        
        # 1. é€šè¿‡Tavily APIæœç´¢æ–°é—»
        logger.info("  â”œâ”€ ä½¿ç”¨Tavilyæœç´¢æ–°é—»...")
        
        # æœç´¢è·å–ç»“æœ
        search_results = self.search_provider.search(
            query, max_results=max_results, search_depth="advanced"
        )
        logger.success(f"  â”œâ”€ å·²è·å– {len(search_results)} æ¡æ–°é—»ç»“æœ")
        
        # 2. æå–å’Œæ¸…æ´—è·å–çš„å†…å®¹
        logger.info("  â”œâ”€ æå–å’Œæ•´åˆå†…å®¹...")
        
        # æ•´ç†æ‰€æœ‰å†…å®¹
        all_content = ""
        sources = []
        
        for idx, result in enumerate(search_results, 1):
            if not (result.get("url") and result.get("raw_content", "")):
                continue
            
            url = result.get("url")
            title = result.get("title", "æ— æ ‡é¢˜")
            
            # ä½¿ç”¨ContentCleaneræ¸…æ´—å†…å®¹
            cleaned_content = self.cleaner.clean_with_trafilatura(url, result.get("raw_content", ""), title)
            
            # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            if not cleaned_content:
                cleaned_content = result.get("raw_content", "")[:2000]  # é™åˆ¶é•¿åº¦
                
            # æ·»åŠ åˆ°æ±‡æ€»å†…å®¹
            all_content += f"æ–°é—» {idx}:\n"
            all_content += f"æ ‡é¢˜: {title}\n"
            all_content += f"å†…å®¹: {cleaned_content}\n\n"
            
            # ä¿å­˜æ¥æºä¿¡æ¯ç”¨äºå¼•ç”¨
            sources.append({
                "title": title,
                "url": url
            })
        
        logger.success(f"  â”œâ”€ å·²æ•´åˆ {len(search_results)} æ¡å†…å®¹ï¼Œå…± {len(all_content)} å­—ç¬¦")
        
        # 3. ä½¿ç”¨LLMç”Ÿæˆäº‹å®èƒŒæ™¯æ€»ç»“
        logger.info("  â”œâ”€ ç”Ÿæˆäº‹å®èƒŒæ™¯æ€»ç»“...")
        
        system_prompt = "You are a helpful assistant skilled at providing structured factual information in JSON format."
        
        user_prompt = f"""
ä½ æ˜¯ä¸€åä¸“ä¸šç ”ç©¶åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹ä¸"{query}"ç›¸å…³çš„æ–°é—»å†…å®¹ï¼Œå®Œæˆä¸¤é¡¹ä»»åŠ¡:

1. æä¾›ä¸€ä»½ç®€æ´ä½†å…¨é¢çš„äº‹å®èƒŒæ™¯æ€»ç»“ (500-800å­—)ï¼š
   - ç¡®ä¿é€»è¾‘ä¸¥è°¨ï¼ŒæŒ‰æ—¶é—´é¡ºåºæˆ–å› æœå…³ç³»å‘ˆç°å…³é”®äº‹ä»¶
   - åªé™ˆè¿°å®¢è§‚äº‹å®ï¼Œä¸åŒ…å«ä¸ªäººè§‚ç‚¹æˆ–çŒœæµ‹
   - æ˜ç¡®æ ‡æ³¨ä¿¡æ¯å­˜åœ¨äº‰è®®ã€ä¸ç¡®å®šæˆ–å°šæœªè¯å®çš„æƒ…å†µ
   - é¿å…é‡å¤ä¿¡æ¯ï¼Œç¡®ä¿æ€»ç»“å†…å®¹å…¨é¢ä¸”ç®€æ´

2. ä»ä»¥ä¸Šæ–°é—»ä¸­æå–æ ¸å¿ƒå®ä½“ï½œå…³é”®è¯ï¼Œç”¨äºåœ¨ç™¾ç§‘ä¸Šæœç´¢ä¿¡æ¯ï¼š
   - æ ¸å¿ƒå®ä½“ï¼šç›´æ¥ç›¸å…³ã€æŒ‡ä»£æ˜ç¡®ä¸”æœ‰è¿›ä¸€æ­¥ç ”ç©¶ä»·å€¼çš„äººç‰©ã€ç»„ç»‡ã€åœ°ç‚¹ç­‰
   - é‡è¦å…³é”®è¯ï¼šä¸ä¸»é¢˜å¯†åˆ‡ç›¸å…³ã€æœ‰åŠ©äºè¿›ä¸€æ­¥ç ”ç©¶çš„æœ¯è¯­å’Œæ¦‚å¿µ

ä¿¡æ¯æ¥æºï¼š
{all_content}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼ˆä½¿ç”¨JSONæ ¼å¼ï¼‰:
{{
  "factual_background": "äº‹å®èƒŒæ™¯æ€»ç»“...",
  "entities": ["å®ä½“1", "å®ä½“2", ...],
  "keywords": ["å…³é”®è¯1", "å…³é”®è¯2", ...]
}}
"""
        
        summary_result = self.llm.generate_completion(
            system_prompt=system_prompt, user_prompt=user_prompt, json_format=True
        )
        
        # å¤„ç†ç»“æœ
        if not summary_result:
            logger.error("  âŒ ç”Ÿæˆäº‹å®èƒŒæ™¯æ€»ç»“å¤±è´¥")
            summary_result = {"factual_background": "", "entities": [], "keywords": []}
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        summary_result["sources"] = sources
        logger.success("  â””â”€ äº‹å®èƒŒæ™¯æ€»ç»“ç”Ÿæˆå®Œæ¯•")
        return summary_result