"""
Unit tests for probabilistic fractional order gradients.

Tests reparameterization and score-function gradient estimators
for probabilistic fractional orders in neural networks.
"""

import pytest
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Normal, Uniform, Beta
import numpy as np

from hpfracc.ml.probabilistic_fractional_orders import (
    ProbabilisticFractionalOrder,
    ReparameterizedFractionalDerivative,
    ScoreFunctionFractionalDerivative,
    create_normal_alpha_layer,
    create_uniform_alpha_layer,
    create_beta_alpha_layer
)


class TestProbabilisticGradients:
    """Test gradient computation for probabilistic fractional orders."""
    
    def test_reparameterization_gradient_flow(self):
        """Test that reparameterization gradients flow correctly."""
        torch.manual_seed(42)
        
        # Create learnable normal distribution
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Normal(0.5, 0.1),
            learnable=True
        )
        
        # Simple input
        x = torch.randn(10, requires_grad=True)
        
        # Forward pass
        epsilon = torch.randn_like(alpha_dist.distribution.loc)
        result = ReparameterizedFractionalDerivative.apply(
            x, alpha_dist, epsilon, "importance", 32
        )
        
        # Backward pass
        loss = result.sum()
        loss.backward()
        
        # Check gradients exist
        assert x.grad is not None
        assert alpha_dist._parameters['loc'].grad is not None
        assert alpha_dist._parameters['scale'].grad is not None
        
        # Check gradient magnitudes are reasonable
        assert torch.isfinite(x.grad).all()
        assert torch.isfinite(alpha_dist._parameters['loc'].grad).all()
        assert torch.isfinite(alpha_dist._parameters['scale'].grad).all()
    
    def test_score_function_gradient_flow(self):
        """Test that score function gradients flow correctly."""
        torch.manual_seed(42)
        
        # Create learnable uniform distribution
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Uniform(0.1, 0.9),
            learnable=True
        )
        
        # Simple input
        x = torch.randn(10, requires_grad=True)
        
        # Forward pass
        result = ScoreFunctionFractionalDerivative.apply(
            x, alpha_dist, "importance", 32
        )
        
        # Backward pass
        loss = result.sum()
        loss.backward()
        
        # Check gradients exist
        assert x.grad is not None
        assert alpha_dist._parameters['low'].grad is not None
        assert alpha_dist._parameters['high'].grad is not None
        
        # Check gradient magnitudes are reasonable
        assert torch.isfinite(x.grad).all()
        assert torch.isfinite(alpha_dist._parameters['low'].grad).all()
        assert torch.isfinite(alpha_dist._parameters['high'].grad).all()
    
    def test_beta_distribution_gradients(self):
        """Test gradient computation for Beta distribution."""
        torch.manual_seed(42)
        
        # Create learnable Beta distribution
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Beta(2.0, 2.0),
            learnable=True
        )
        
        x = torch.randn(8, requires_grad=True)
        
        # Test reparameterization path
        epsilon = torch.randn_like(alpha_dist.distribution.concentration1)
        result = ReparameterizedFractionalDerivative.apply(
            x, alpha_dist, epsilon, "importance", 16
        )
        
        loss = result.sum()
        loss.backward()
        
        # Check gradients exist for Beta parameters
        assert alpha_dist._parameters['concentration1'].grad is not None
        assert alpha_dist._parameters['concentration0'].grad is not None
        assert torch.isfinite(alpha_dist._parameters['concentration1'].grad).all()
        assert torch.isfinite(alpha_dist._parameters['concentration0'].grad).all()
    
    def test_gradient_consistency_across_methods(self):
        """Test that gradients are consistent across different sampling methods."""
        torch.manual_seed(42)
        
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Normal(0.5, 0.1),
            learnable=True
        )
        
        x = torch.randn(10, requires_grad=True)
        
        # Test different methods
        methods = ["importance", "stratified", "control_variate"]
        gradients = {}
        
        for method in methods:
            # Reset gradients
            if x.grad is not None:
                x.grad.zero_()
            for param in alpha_dist._parameters.values():
                if param.grad is not None:
                    param.grad.zero_()
            
            # Forward and backward
            epsilon = torch.randn_like(alpha_dist.distribution.loc)
            result = ReparameterizedFractionalDerivative.apply(
                x, alpha_dist, epsilon, method, 32
            )
            
            loss = result.sum()
            loss.backward()
            
            # Store gradients
            gradients[method] = {
                'x_grad': x.grad.clone(),
                'loc_grad': alpha_dist._parameters['loc'].grad.clone(),
                'scale_grad': alpha_dist._parameters['scale'].grad.clone()
            }
        
        # Check that gradients are finite and non-zero
        for method, grads in gradients.items():
            assert torch.isfinite(grads['x_grad']).all(), f"x_grad not finite for {method}"
            assert torch.isfinite(grads['loc_grad']).all(), f"loc_grad not finite for {method}"
            assert torch.isfinite(grads['scale_grad']).all(), f"scale_grad not finite for {method}"
            
            # Gradients should be non-zero (unless by chance)
            assert not torch.allclose(grads['x_grad'], torch.zeros_like(grads['x_grad']), atol=1e-6)
    
    def test_layer_integration_gradients(self):
        """Test gradient flow through neural network layers."""
        torch.manual_seed(42)
        
        # Create a simple network with probabilistic fractional layer
        class TestNet(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear1 = nn.Linear(10, 5)
                self.frac_layer = create_normal_alpha_layer(0.5, 0.1, learnable=True)
                self.linear2 = nn.Linear(5, 1)
            
            def forward(self, x):
                x = F.relu(self.linear1(x))
                x = self.frac_layer(x)
                return self.linear2(x)
        
        net = TestNet()
        x = torch.randn(4, 10, requires_grad=True)
        
        # Forward pass
        output = net(x)
        loss = output.sum()
        
        # Backward pass
        loss.backward()
        
        # Check all parameters have gradients
        for name, param in net.named_parameters():
            assert param.grad is not None, f"No gradient for {name}"
            assert torch.isfinite(param.grad).all(), f"Non-finite gradient for {name}"
    
    def test_variance_reduction_effectiveness(self):
        """Test that control variates reduce gradient variance."""
        torch.manual_seed(42)
        
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Normal(0.5, 0.1),
            learnable=True
        )
        
        x = torch.randn(10, requires_grad=True)
        
        # Collect gradients from multiple runs
        n_runs = 20
        gradients_importance = []
        gradients_control = []
        
        for _ in range(n_runs):
            # Reset gradients
            if x.grad is not None:
                x.grad.zero_()
            for param in alpha_dist._parameters.values():
                if param.grad is not None:
                    param.grad.zero_()
            
            # Importance sampling
            epsilon = torch.randn_like(alpha_dist.distribution.loc)
            result = ReparameterizedFractionalDerivative.apply(
                x, alpha_dist, epsilon, "importance", 32
            )
            loss = result.sum()
            loss.backward()
            gradients_importance.append(alpha_dist._parameters['loc'].grad.clone())
            
            # Reset gradients
            if x.grad is not None:
                x.grad.zero_()
            for param in alpha_dist._parameters.values():
                if param.grad is not None:
                    param.grad.zero_()
            
            # Control variates
            epsilon = torch.randn_like(alpha_dist.distribution.loc)
            result = ReparameterizedFractionalDerivative.apply(
                x, alpha_dist, epsilon, "control_variate", 32
            )
            loss = result.sum()
            loss.backward()
            gradients_control.append(alpha_dist._parameters['loc'].grad.clone())
        
        # Compute variances
        var_importance = torch.var(torch.stack(gradients_importance))
        var_control = torch.var(torch.stack(gradients_control))
        
        # Control variates should reduce variance (or at least not increase it significantly)
        assert var_control <= var_importance * 1.5, "Control variates should reduce gradient variance"
    
    def test_non_learnable_distribution(self):
        """Test that non-learnable distributions work without gradient computation."""
        torch.manual_seed(42)
        
        # Non-learnable distribution
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Normal(0.5, 0.1),
            learnable=False
        )
        
        x = torch.randn(10, requires_grad=True)
        
        # Forward pass
        result = ReparameterizedFractionalDerivative.apply(
            x, alpha_dist, torch.randn(1), "importance", 32
        )
        
        loss = result.sum()
        loss.backward()
        
        # Only x should have gradients, not distribution parameters
        assert x.grad is not None
        assert torch.isfinite(x.grad).all()
        
        # Distribution should not have learnable parameters
        assert len(alpha_dist._parameters) == 0
    
    def test_gradient_stability_with_different_k(self):
        """Test gradient stability across different sampling sizes."""
        torch.manual_seed(42)
        
        alpha_dist = ProbabilisticFractionalOrder(
            distribution=Normal(0.5, 0.1),
            learnable=True
        )
        
        x = torch.randn(10, requires_grad=True)
        k_values = [8, 16, 32, 64]
        
        for k in k_values:
            # Reset gradients
            if x.grad is not None:
                x.grad.zero_()
            for param in alpha_dist._parameters.values():
                if param.grad is not None:
                    param.grad.zero_()
            
            # Forward and backward
            epsilon = torch.randn_like(alpha_dist.distribution.loc)
            result = ReparameterizedFractionalDerivative.apply(
                x, alpha_dist, epsilon, "importance", k
            )
            
            loss = result.sum()
            loss.backward()
            
            # Check gradients are finite
            assert torch.isfinite(x.grad).all(), f"Non-finite x_grad for k={k}"
            assert torch.isfinite(alpha_dist._parameters['loc'].grad).all(), f"Non-finite loc_grad for k={k}"
            assert torch.isfinite(alpha_dist._parameters['scale'].grad).all(), f"Non-finite scale_grad for k={k}"


if __name__ == "__main__":
    pytest.main([__file__])
