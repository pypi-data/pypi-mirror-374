"""
Variance-aware training hooks for stochastic and probabilistic fractional calculus.

This module provides training utilities that monitor and control variance
in stochastic fractional derivatives and probabilistic fractional orders.
"""

import torch
import torch.nn as nn
import numpy as np
import time
from typing import Dict, List, Optional, Any, Callable, Union
from collections import defaultdict, deque
import logging
from dataclasses import dataclass
from abc import ABC, abstractmethod

from .stochastic_memory_sampling import (
    StochasticFractionalDerivative, ImportanceSampler, 
    StratifiedSampler, ControlVariateSampler
)
from .probabilistic_fractional_orders import (
    ProbabilisticFractionalOrder, ReparameterizedFractionalDerivative,
    ScoreFunctionFractionalDerivative
)


@dataclass
class VarianceMetrics:
    """Container for variance-related metrics."""
    mean: float
    std: float
    variance: float
    coefficient_of_variation: float
    sample_count: int
    timestamp: float


class VarianceMonitor:
    """Monitor variance in stochastic fractional derivatives."""
    
    def __init__(self, window_size: int = 100, log_level: str = "INFO"):
        self.window_size = window_size
        self.logger = logging.getLogger(f"{__name__}.VarianceMonitor")
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # Storage for variance metrics
        self.metrics_history: Dict[str, deque] = defaultdict(lambda: deque(maxlen=window_size))
        self.current_metrics: Dict[str, VarianceMetrics] = {}
        
        # Configuration
        self.variance_threshold = 0.1  # CV threshold for warnings
        self.high_variance_threshold = 0.5  # CV threshold for errors
        
    def update(self, name: str, values: torch.Tensor, timestamp: Optional[float] = None):
        """Update variance metrics for a given component."""
        if timestamp is None:
            timestamp = time.time()
        
        # Convert to numpy for easier computation
        if isinstance(values, torch.Tensor):
            values = values.detach().cpu().numpy()
        
        # Flatten if needed
        values = values.flatten()
        
        # Compute metrics
        mean_val = np.mean(values)
        std_val = np.std(values)
        var_val = np.var(values)
        cv = std_val / (abs(mean_val) + 1e-8)
        
        metrics = VarianceMetrics(
            mean=mean_val,
            std=std_val,
            variance=var_val,
            coefficient_of_variation=cv,
            sample_count=len(values),
            timestamp=timestamp
        )
        
        # Store metrics
        self.current_metrics[name] = metrics
        self.metrics_history[name].append(metrics)
        
        # Log warnings if variance is high
        if cv > self.high_variance_threshold:
            self.logger.error(f"High variance detected in {name}: CV={cv:.3f}")
        elif cv > self.variance_threshold:
            self.logger.warning(f"Elevated variance in {name}: CV={cv:.3f}")
    
    def get_metrics(self, name: str) -> Optional[VarianceMetrics]:
        """Get current metrics for a component."""
        return self.current_metrics.get(name)
    
    def get_history(self, name: str) -> List[VarianceMetrics]:
        """Get historical metrics for a component."""
        return list(self.metrics_history[name])
    
    def get_summary(self) -> Dict[str, Dict[str, float]]:
        """Get summary of all monitored components."""
        summary = {}
        for name, metrics in self.current_metrics.items():
            summary[name] = {
                'mean': metrics.mean,
                'std': metrics.std,
                'variance': metrics.variance,
                'cv': metrics.coefficient_of_variation,
                'samples': metrics.sample_count
            }
        return summary


class StochasticSeedManager:
    """Manage random seeds for stochastic fractional derivatives."""
    
    def __init__(self, base_seed: int = 42):
        self.base_seed = base_seed
        self.current_seed = base_seed
        self.seed_history = []
        
    def set_seed(self, seed: int):
        """Set the current seed."""
        self.current_seed = seed
        torch.manual_seed(seed)
        np.random.seed(seed)
        self.seed_history.append(seed)
    
    def get_next_seed(self) -> int:
        """Get the next seed in sequence."""
        self.current_seed += 1
        return self.current_seed
    
    def reset_to_base(self):
        """Reset to base seed."""
        self.set_seed(self.base_seed)
    
    def set_deterministic_mode(self, deterministic: bool = True):
        """Enable/disable deterministic mode."""
        if deterministic:
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
        else:
            torch.backends.cudnn.deterministic = False
            torch.backends.cudnn.benchmark = True


class VarianceAwareCallback:
    """Callback for variance-aware training."""
    
    def __init__(self, 
                 monitor: VarianceMonitor,
                 seed_manager: StochasticSeedManager,
                 log_interval: int = 10,
                 variance_check_interval: int = 5):
        self.monitor = monitor
        self.seed_manager = seed_manager
        self.log_interval = log_interval
        self.variance_check_interval = variance_check_interval
        
        self.epoch_count = 0
        self.batch_count = 0
        
    def on_epoch_begin(self, epoch: int, **kwargs):
        """Called at the beginning of each epoch."""
        self.epoch_count = epoch
        self.seed_manager.set_seed(self.seed_manager.base_seed + epoch)
        
    def on_batch_begin(self, batch_idx: int, **kwargs):
        """Called at the beginning of each batch."""
        self.batch_count = batch_idx
        
    def on_batch_end(self, batch_idx: int, **kwargs):
        """Called at the end of each batch."""
        if batch_idx % self.variance_check_interval == 0:
            self._check_variance()
    
    def on_epoch_end(self, epoch: int, **kwargs):
        """Called at the end of each epoch."""
        if epoch % self.log_interval == 0:
            self._log_variance_summary()
    
    def _check_variance(self):
        """Check variance metrics and log warnings."""
        summary = self.monitor.get_summary()
        for name, metrics in summary.items():
            if metrics['cv'] > 0.5:
                logging.warning(f"High variance in {name}: CV={metrics['cv']:.3f}")
    
    def _log_variance_summary(self):
        """Log variance summary."""
        summary = self.monitor.get_summary()
        logging.info("Variance Summary:")
        for name, metrics in summary.items():
            logging.info(f"  {name}: mean={metrics['mean']:.4f}, "
                        f"std={metrics['std']:.4f}, cv={metrics['cv']:.3f}")


class AdaptiveSamplingManager:
    """Adaptively adjust sampling parameters based on variance."""
    
    def __init__(self, 
                 initial_k: int = 32,
                 min_k: int = 8,
                 max_k: int = 256,
                 variance_threshold: float = 0.1):
        self.initial_k = initial_k
        self.min_k = min_k
        self.max_k = max_k
        self.variance_threshold = variance_threshold
        
        self.current_k = initial_k
        self.k_history = []
        
    def update_k(self, variance: float, current_k: int) -> int:
        """Update K based on variance."""
        if variance > self.variance_threshold:
            # Increase K to reduce variance
            new_k = min(current_k * 2, self.max_k)
        else:
            # Decrease K to improve efficiency
            new_k = max(current_k // 2, self.min_k)
        
        self.current_k = new_k
        self.k_history.append(new_k)
        return new_k
    
    def get_current_k(self) -> int:
        """Get current K value."""
        return self.current_k


class VarianceAwareTrainer:
    """Enhanced trainer with variance awareness for stochastic fractional calculus."""
    
    def __init__(self,
                 model: nn.Module,
                 optimizer: torch.optim.Optimizer,
                 loss_fn: nn.Module,
                 variance_monitor: Optional[VarianceMonitor] = None,
                 seed_manager: Optional[StochasticSeedManager] = None,
                 adaptive_sampling: Optional[AdaptiveSamplingManager] = None,
                 callbacks: Optional[List[VarianceAwareCallback]] = None):
        
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        
        # Variance-aware components
        self.variance_monitor = variance_monitor or VarianceMonitor()
        self.seed_manager = seed_manager or StochasticSeedManager()
        self.adaptive_sampling = adaptive_sampling or AdaptiveSamplingManager()
        self.callbacks = callbacks or []
        
        # Training state
        self.current_epoch = 0
        self.current_batch = 0
        self.training_losses = []
        self.variance_history = []
        
        # Hook into model for variance monitoring
        self._register_hooks()
    
    def _register_hooks(self):
        """Register forward hooks to monitor variance."""
        def create_hook(name):
            def hook(module, input, output):
                if isinstance(output, torch.Tensor):
                    self.variance_monitor.update(f"{name}_output", output)
            return hook
        
        # Register hooks for stochastic and probabilistic layers
        for name, module in self.model.named_modules():
            if any(keyword in name.lower() for keyword in 
                   ['stochastic', 'probabilistic', 'fractional']):
                module.register_forward_hook(create_hook(name))
    
    def train_epoch(self, dataloader, epoch: int = 0) -> Dict[str, float]:
        """Train for one epoch with variance monitoring."""
        self.current_epoch = epoch
        self.model.train()
        
        # Call epoch begin callbacks
        for callback in self.callbacks:
            callback.on_epoch_begin(epoch)
        
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            self.current_batch = batch_idx
            
            # Call batch begin callbacks
            for callback in self.callbacks:
                callback.on_batch_begin(batch_idx)
            
            # Set seed for this batch
            batch_seed = self.seed_manager.get_next_seed()
            torch.manual_seed(batch_seed)
            
            # Forward pass
            output = self.model(data)
            loss = self.loss_fn(output, target)
            
            # Monitor loss variance
            self.variance_monitor.update("loss", loss.unsqueeze(0))
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            
            # Monitor gradient variance
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    self.variance_monitor.update(f"grad_{name}", param.grad)
            
            self.optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            # Call batch end callbacks
            for callback in self.callbacks:
                callback.on_batch_end(batch_idx)
        
        avg_loss = total_loss / num_batches
        
        # Store training metrics
        self.training_losses.append(avg_loss)
        variance_summary = self.variance_monitor.get_summary()
        self.variance_history.append(variance_summary)
        
        # Call epoch end callbacks
        for callback in self.callbacks:
            callback.on_epoch_end(epoch)
        
        return {
            'loss': avg_loss,
            'variance_summary': variance_summary,
            'epoch': epoch
        }
    
    def train(self, dataloader, num_epochs: int) -> Dict[str, List]:
        """Train for multiple epochs."""
        results = {
            'losses': [],
            'variance_history': [],
            'epochs': []
        }
        
        for epoch in range(num_epochs):
            epoch_results = self.train_epoch(dataloader, epoch)
            
            results['losses'].append(epoch_results['loss'])
            results['variance_history'].append(epoch_results['variance_summary'])
            results['epochs'].append(epoch)
            
            print(f"Epoch {epoch}: Loss = {epoch_results['loss']:.4f}")
            
            # Print variance summary every 10 epochs
            if epoch % 10 == 0:
                print("Variance Summary:")
                for name, metrics in epoch_results['variance_summary'].items():
                    print(f"  {name}: CV = {metrics['cv']:.3f}")
        
        return results
    
    def get_variance_summary(self) -> Dict[str, Dict[str, float]]:
        """Get current variance summary."""
        return self.variance_monitor.get_summary()
    
    def set_sampling_budget(self, k: int):
        """Set sampling budget for stochastic components."""
        self.adaptive_sampling.current_k = k
    
    def enable_deterministic_mode(self, deterministic: bool = True):
        """Enable/disable deterministic mode."""
        self.seed_manager.set_deterministic_mode(deterministic)


def create_variance_aware_trainer(model: nn.Module,
                                 optimizer: torch.optim.Optimizer,
                                 loss_fn: nn.Module,
                                 base_seed: int = 42,
                                 variance_threshold: float = 0.1,
                                 log_interval: int = 10) -> VarianceAwareTrainer:
    """Factory function to create a variance-aware trainer."""
    
    # Create components
    variance_monitor = VarianceMonitor()
    seed_manager = StochasticSeedManager(base_seed)
    adaptive_sampling = AdaptiveSamplingManager(variance_threshold=variance_threshold)
    
    # Create callback
    callback = VarianceAwareCallback(
        monitor=variance_monitor,
        seed_manager=seed_manager,
        log_interval=log_interval
    )
    
    # Create trainer
    trainer = VarianceAwareTrainer(
        model=model,
        optimizer=optimizer,
        loss_fn=loss_fn,
        variance_monitor=variance_monitor,
        seed_manager=seed_manager,
        adaptive_sampling=adaptive_sampling,
        callbacks=[callback]
    )
    
    return trainer


# Example usage and testing functions
def test_variance_aware_training():
    """Test variance-aware training with a simple model."""
    print("Testing variance-aware training...")
    
    # Create a simple model with stochastic fractional layer
    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(10, 5)
            # Note: In practice, you'd use the actual stochastic/probabilistic layers
            # from hpfracc.ml.stochastic_memory_sampling and hpfracc.ml.probabilistic_fractional_orders
        
        def forward(self, x):
            x = self.linear(x)
            return x
    
    # Create model, optimizer, and loss
    model = TestModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.MSELoss()
    
    # Create variance-aware trainer
    trainer = create_variance_aware_trainer(model, optimizer, loss_fn)
    
    # Create dummy data
    data = torch.randn(32, 10)
    target = torch.randn(32, 5)
    dataloader = [(data, target) for _ in range(10)]
    
    # Train for a few epochs
    results = trainer.train(dataloader, num_epochs=3)
    
    print("Training completed!")
    print(f"Final loss: {results['losses'][-1]:.4f}")
    print("Variance summary:")
    for name, metrics in results['variance_history'][-1].items():
        print(f"  {name}: CV = {metrics['cv']:.3f}")
    
    return trainer, results


if __name__ == "__main__":
    trainer, results = test_variance_aware_training()

