{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7744f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "\n",
    "\n",
    "def get_year_from_bibtex(bibtex: str) -> int | None:\n",
    "    \"\"\"Fetches the year attribute from the given BibTex entry. Returns None if no year can be found.\"\"\"\n",
    "    parsed_bibtex = bibtexparser.parse_string(bibtex)\n",
    "\n",
    "    if not parsed_bibtex.entries:\n",
    "        return None\n",
    "\n",
    "    if not (\n",
    "        year := next(\n",
    "            (\n",
    "                entry\n",
    "                for key, entry in parsed_bibtex.entries[0].fields_dict.items()\n",
    "                if key.lower() == \"year\"\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "    ):\n",
    "        return None\n",
    "\n",
    "    if not year.value.isdigit():\n",
    "        return None  # this also catches negative years\n",
    "\n",
    "    return int(year.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe01e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phylodata.paper.bibtex import get_year_from_bibtex\n",
    "\n",
    "\n",
    "def generate_id(title: str, authors: list[str], bibtex: str) -> str:\n",
    "    author = authors[0]\n",
    "    title_excerpt = next((word for word in title.split() if word not in STOP_WORDS), \"\")\n",
    "    year = get_year_from_bibtex(bibtex) or \"\"\n",
    "    return f\"{author}{year}{title_excerpt}\"\n",
    "\n",
    "\n",
    "STOP_WORDS = [\n",
    "    \"i\",\n",
    "    \"me\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"we\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"you\",\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"yourself\",\n",
    "    \"yourselves\",\n",
    "    \"he\",\n",
    "    \"him\",\n",
    "    \"his\",\n",
    "    \"himself\",\n",
    "    \"she\",\n",
    "    \"her\",\n",
    "    \"hers\",\n",
    "    \"herself\",\n",
    "    \"it\",\n",
    "    \"its\",\n",
    "    \"itself\",\n",
    "    \"they\",\n",
    "    \"them\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"themselves\",\n",
    "    \"what\",\n",
    "    \"which\",\n",
    "    \"who\",\n",
    "    \"whom\",\n",
    "    \"this\",\n",
    "    \"that\",\n",
    "    \"these\",\n",
    "    \"those\",\n",
    "    \"am\",\n",
    "    \"is\",\n",
    "    \"are\",\n",
    "    \"was\",\n",
    "    \"were\",\n",
    "    \"be\",\n",
    "    \"been\",\n",
    "    \"being\",\n",
    "    \"have\",\n",
    "    \"has\",\n",
    "    \"had\",\n",
    "    \"having\",\n",
    "    \"do\",\n",
    "    \"does\",\n",
    "    \"did\",\n",
    "    \"doing\",\n",
    "    \"a\",\n",
    "    \"an\",\n",
    "    \"the\",\n",
    "    \"and\",\n",
    "    \"but\",\n",
    "    \"if\",\n",
    "    \"or\",\n",
    "    \"because\",\n",
    "    \"as\",\n",
    "    \"until\",\n",
    "    \"while\",\n",
    "    \"of\",\n",
    "    \"at\",\n",
    "    \"by\",\n",
    "    \"for\",\n",
    "    \"with\",\n",
    "    \"about\",\n",
    "    \"against\",\n",
    "    \"between\",\n",
    "    \"into\",\n",
    "    \"through\",\n",
    "    \"during\",\n",
    "    \"before\",\n",
    "    \"after\",\n",
    "    \"above\",\n",
    "    \"below\",\n",
    "    \"to\",\n",
    "    \"from\",\n",
    "    \"up\",\n",
    "    \"down\",\n",
    "    \"in\",\n",
    "    \"out\",\n",
    "    \"on\",\n",
    "    \"off\",\n",
    "    \"over\",\n",
    "    \"under\",\n",
    "    \"again\",\n",
    "    \"further\",\n",
    "    \"then\",\n",
    "    \"once\",\n",
    "    \"here\",\n",
    "    \"there\",\n",
    "    \"when\",\n",
    "    \"where\",\n",
    "    \"why\",\n",
    "    \"how\",\n",
    "    \"all\",\n",
    "    \"any\",\n",
    "    \"both\",\n",
    "    \"each\",\n",
    "    \"few\",\n",
    "    \"more\",\n",
    "    \"most\",\n",
    "    \"other\",\n",
    "    \"some\",\n",
    "    \"such\",\n",
    "    \"no\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"only\",\n",
    "    \"own\",\n",
    "    \"same\",\n",
    "    \"so\",\n",
    "    \"than\",\n",
    "    \"too\",\n",
    "    \"very\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"can\",\n",
    "    \"will\",\n",
    "    \"just\",\n",
    "    \"don\",\n",
    "    \"should\",\n",
    "    \"now\",\n",
    "]\n",
    "\"\"\"The list of stop words taken from NLTK.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9c9e7a1-44cd-428b-97aa-7f30bb54e6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import BytesIO, TextIOWrapper\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd169d79-a5a9-4356-9f33-c458a0ce9a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/tobiaochsner/Documents/Bayesian Total-Evidence Dating Reveals the Recent Crown Radiation of Penguins-phylodata/yule-n10-52.log', \"rb\") as fh:\n",
    "    file = BytesIO(fh.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1fe96212-123a-4d85-ac99-4bd65468d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = TextIOWrapper(file)\n",
    "tsv_file = csv.DictReader(wrapper, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ffc72f51-5ecb-43d7-9410-36fd94447745",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = list(tsv_file)\n",
    "num_rows = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a55964e7-ff4c-489d-93a2-eb620e100d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_lines = lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "784ceaa8-66d6-4188-9e6a-c2b6ce33bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_file = BytesIO()\n",
    "preview_wrapper = TextIOWrapper(preview_file, encoding='utf-8', newline='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac5ff1f9-5eac-4625-91e8-593f1eedccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_writer = csv.DictWriter(preview_wrapper, fieldnames=preview_lines[0].keys(), delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43e7256c-f017-4493-ab6f-cdddf2995cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview_writer.writeheader()\n",
    "preview_writer.writerows(preview_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7f130fb-f349-44da-bdf3-b5b3e89b5c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x10c508860>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_wrapper.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c377c96e-2c17-4140-8e62-cd9b46a81c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1327"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_file.getbuffer().nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b6798863-687c-48ad-b351-f814c9ab2acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phylodata.utils.bytesio_utils import get_nexus_from_bytesio, get_xml_from_bytesio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "079b0d49-fb41-45cd-a0a6-1886535cd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/tobiaochsner/Documents/Bayesian Total-Evidence Dating Reveals the Recent Crown Radiation of Penguins-phylodata/yule-n10-9.trees', \"rb\") as fh:\n",
    "    file = BytesIO(fh.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bad9b455-7d9e-4634-8d51-0f2b21f49820",
   "metadata": {},
   "outputs": [],
   "source": [
    "nexus = get_nexus_from_bytesio(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f3b768cf-18f5-4fd1-bc2a-9aeaf4b7ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_trees_commands = nexus.TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5dc31044-1ab0-4999-9a95-3a84b3f9525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from commonnexus.blocks.trees import Tree\n",
    "\n",
    "preview_trees_commands = []\n",
    "num_previewed_trees = 0\n",
    "\n",
    "for command in full_trees_commands:\n",
    "    # test if it is a TREE command\n",
    "    try:\n",
    "        Tree(command)\n",
    "    except:\n",
    "        # this is not a tree command\n",
    "        # we keep it\n",
    "        preview_trees_commands.append(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c54a425a-2664-4c32-a5c8-4baadb820031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Token(text='\\n', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='Begin', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='trees', type=<TokenType.WORD: 1>),\n",
       "  Token(text=';', type=<TokenType.PUNCTUATION: 4>)),\n",
       " (Token(text='\\n\\t', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='Translate', type=<TokenType.WORD: 1>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='1', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='0', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='2', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='1', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='3', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='2', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='4', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='3', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='5', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='4', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='6', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='5', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='7', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='6', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='8', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='7', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t   ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='9', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='8', type=<TokenType.WORD: 1>),\n",
       "  Token(text=',', type=<TokenType.PUNCTUATION: 4>),\n",
       "  Token(text='\\n\\t\\t  ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='10', type=<TokenType.WORD: 1>),\n",
       "  Token(text=' ', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='9', type=<TokenType.WORD: 1>),\n",
       "  Token(text='\\n', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text=';', type=<TokenType.PUNCTUATION: 4>)),\n",
       " (Token(text='\\n', type=<TokenType.WHITESPACE: 5>),\n",
       "  Token(text='End', type=<TokenType.WORD: 1>),\n",
       "  Token(text=';', type=<TokenType.PUNCTUATION: 4>))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preview_trees_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46705d1-bc8d-4593-98a3-a4d137169c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
