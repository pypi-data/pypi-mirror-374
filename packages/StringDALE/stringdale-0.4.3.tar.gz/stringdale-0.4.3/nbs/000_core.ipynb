{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from itertools import count,product\n",
    "from joblib import Memory\n",
    "import stringdale\n",
    "from typing import Dict, List, Iterator, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git paths and env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def get_git_root():\n",
    "        return Path(stringdale.__file__).parent.parent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_git_root() == Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_env(path=None):\n",
    "    if path is None:\n",
    "        path = get_git_root() / '.env.dev'\n",
    "    else:\n",
    "        path = Path(path)\n",
    "    return load_dotenv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_env()\n",
    "# assert os.getenv('OPENAI_API_KEY') is not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.cache'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('DISKCACHE','.cache')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Create a cache directory in the user's home directory\n",
    "cache_location = os.environ.get('DISKCACHE','.cache')\n",
    "cache_dir = get_git_root() / cache_location\n",
    "cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Initialize the Memory object for caching\n",
    "disk_cache = Memory(cache_dir, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _count_multiplicity(sets):\n",
    "    \"\"\"Count how many times each unique item appears across multiple sets.\n",
    "    \"\"\"\n",
    "    # Combine all sets and count frequencies\n",
    "    counter = Counter()\n",
    "    for s in sets:\n",
    "        counter.update(s)\n",
    "    return counter\n",
    "\n",
    "def _duplicates(sets):\n",
    "    \"\"\"Check if any item appears in more than one set.\n",
    "    \"\"\"\n",
    "    counts = _count_multiplicity(sets)\n",
    "    return [item for item,count in counts.items() if count > 1]\n",
    "\n",
    "def merge_list_dicts(dict1, dict2):\n",
    "    \"\"\"Merge two dictionaries with list values by concatenating their lists.\n",
    "    \n",
    "    Args:\n",
    "        dict1: defaultdict(list) or dict with list values\n",
    "        dict2: dict with list values\n",
    "    \n",
    "    Returns:\n",
    "        defaultdict(list) with concatenated lists\n",
    "    \"\"\"\n",
    "    result = defaultdict(list, dict1)\n",
    "    for k, v in dict2.items():\n",
    "        result[k].extend(v)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial dicts\n",
    "d1 = defaultdict(list, {'a': [1, 2], 'b': [3]})\n",
    "d2 = {'a': [4], 'c': [5, 6]}\n",
    "\n",
    "# Merge them\n",
    "merged = merge_list_dicts(d1, d2)\n",
    "assert merged == defaultdict(list, {'a': [1, 2, 4], 'b': [3], 'c': [5, 6]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def new_combinations(\n",
    "    new_params: Dict[str, List],\n",
    "    old_params: Dict[str, List]\n",
    ") -> Iterator[Dict[str,Any]]:\n",
    "    \"\"\"\n",
    "    Generate cartesian product of parameters by first choosing new/old for each parameter.\n",
    "    Skip combinations that only use old parameters.\n",
    "    \n",
    "    Args:\n",
    "        new_params: Dictionary of new parameter values to try\n",
    "        old_params: Dictionary of previously tried parameter values\n",
    "    \"\"\"\n",
    "    # Get all parameter names\n",
    "    param_names = list(set(new_params) | set(old_params))\n",
    "    \n",
    "    # Generate all possible new/old choices for each parameter (True=new, False=old)\n",
    "    for choices in product([True, False], repeat=len(param_names)):\n",
    "        # Skip if all choices are old\n",
    "        if not any(choices):\n",
    "            continue\n",
    "        \n",
    "        # For each parameter, use either new or old values based on choice\n",
    "        param_values = []\n",
    "        for param_name, use_new in zip(param_names, choices):\n",
    "            if use_new:\n",
    "                values = new_params.get(param_name, [])\n",
    "            else:\n",
    "                values = old_params.get(param_name, [])\n",
    "            param_values.append(values)\n",
    "        \n",
    "        # Generate combinations for this new/old choice pattern\n",
    "        yield dict(zip(param_names, param_values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic functionality\n",
    "new = {'a': [1, 2], 'b': ['x']}\n",
    "old = {'a': [2, 3], 'b': ['x', 'y']}\n",
    "\n",
    "results = list(new_combinations(new, old))\n",
    "serialized_results = { json.dumps(result,sort_keys=True) for result in results }\n",
    "assert serialized_results == {'{\"a\": [1, 2], \"b\": [\"x\", \"y\"]}',\n",
    " '{\"a\": [1, 2], \"b\": [\"x\"]}',\n",
    " '{\"a\": [2, 3], \"b\": [\"x\"]}'} , results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Should include combinations with at least one new value\n",
    "# assert {'a': 1, 'b': 'x'} in results\n",
    "# assert {'a': 1, 'b': 'y'} in results\n",
    "# assert {'a': 2, 'b': 'x'} in results  # 2 is in both new and old\n",
    "\n",
    "# # Should not include combinations with only old values\n",
    "# assert {'a': 3, 'b': 'y'} not in results\n",
    "\n",
    "# # Test 2: Empty new params\n",
    "# new = {}\n",
    "# old = {'a': [1, 2], 'b': ['x']}\n",
    "# assert list(product_with_memory(new, old)) == []\n",
    "\n",
    "# # Test 3: Empty old params\n",
    "# new = {'a': [1, 2], 'b': ['x']}\n",
    "# old = {}\n",
    "# results = list(product_with_memory(new, old))\n",
    "# assert len(results) == 2  # All combinations of new params\n",
    "\n",
    "# # Test 4: Disjoint parameters\n",
    "# new = {'a': [1, 2]}\n",
    "# old = {'b': ['x', 'y']}\n",
    "# results = list(product_with_memory(new, old))\n",
    "# assert len(results) == 4  # 2 new 'a' values Ã— 2 old 'b' values\n",
    "# assert {'a': 1, 'b': 'x'} in results\n",
    "# assert {'a': 1, 'b': 'y'} in results\n",
    "# assert {'a': 2, 'b': 'x'} in results\n",
    "# assert {'a': 2, 'b': 'y'} in results\n",
    "\n",
    "# # Test 5: Overlapping values\n",
    "# new = {'a': [1, 2, 3]}\n",
    "# old = {'a': [3, 4, 5]}\n",
    "# results = list(product_with_memory(new, old))\n",
    "# assert {'a': 1} in results\n",
    "# assert {'a': 2} in results\n",
    "# assert {'a': 3} in results  # 3 is in both new and old\n",
    "# assert {'a': 4} not in results\n",
    "# assert {'a': 5} not in results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def dict_cartesian_product(input_dict, keys):\n",
    "    \"\"\"\n",
    "    Generate cartesian products of dictionary values for specified keys while preserving other keys.\n",
    "    \n",
    "    Args:\n",
    "        input_dict (dict): Input dictionary with list values\n",
    "        keys (list): List of keys to generate products from\n",
    "        \n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries containing all possible combinations\n",
    "    \"\"\"\n",
    "    # Get the lists of values for the specified keys\n",
    "    value_lists = [input_dict[key] for key in keys]\n",
    "    \n",
    "    # Generate cartesian product of the values\n",
    "    products = product(*value_lists)\n",
    "    \n",
    "    # Create dictionaries for each product combination\n",
    "    result = []\n",
    "    for values in products:\n",
    "        # Start with a copy of the original dict\n",
    "        new_dict = input_dict.copy()\n",
    "        # Update only the specified keys with their new values\n",
    "        for key, value in zip(keys, values):\n",
    "            new_dict[key] = value\n",
    "        result.append(new_dict)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "data = {\n",
    "    'color': ['red', 'blue'],\n",
    "    'size': ['S', 'M'],\n",
    "    'style': ['casual', 'formal']\n",
    "}\n",
    "\n",
    "keys_to_combine = ['color', 'size']\n",
    "result = dict_cartesian_product(data, keys_to_combine)\n",
    "assert result == [{'color': 'red', 'size': 'S', 'style': ['casual', 'formal']},\n",
    " {'color': 'red', 'size': 'M', 'style': ['casual', 'formal']},\n",
    " {'color': 'blue', 'size': 'S', 'style': ['casual', 'formal']},\n",
    " {'color': 'blue', 'size': 'M', 'style': ['casual', 'formal']}],result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[defaultdict(list, {'x': 0}),\n",
       " defaultdict(list, {'x': 1}),\n",
       " defaultdict(list, {'x': 2})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = defaultdict(list)\n",
    "d['x'] = [0, 1, 2]\n",
    "dict_cartesian_product(d,'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NamedLambda():\n",
    "    def __init__(self,name,func):\n",
    "        self.name = name\n",
    "        self.func = func\n",
    "\n",
    "    def __call__(self,*args,**kwargs):\n",
    "        return self.func(*args,**kwargs)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Union,Awaitable,Callable,Any\n",
    "import inspect\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def maybe_await(func_or_coro: Any, args, kwargs) -> Any:\n",
    "    \"\"\"\n",
    "    Prefer __acall__ if it exists, otherwise try normal async/sync calling patterns\n",
    "    \"\"\"\n",
    "    if hasattr(func_or_coro, '__call__'):\n",
    "        if inspect.iscoroutinefunction(func_or_coro.__call__):\n",
    "            return await func_or_coro.__call__(*args, **kwargs)\n",
    "        else:\n",
    "            return func_or_coro.__call__(*args, **kwargs)\n",
    "    elif inspect.iscoroutinefunction(func_or_coro):\n",
    "        return await func_or_coro(*args, **kwargs)\n",
    "    elif inspect.iscoroutine(func_or_coro):\n",
    "        return await func_or_coro\n",
    "    elif callable(func_or_coro):\n",
    "        return func_or_coro(*args, **kwargs)\n",
    "    else:\n",
    "        return func_or_coro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def maybe_await(func_or_coro: Any, args, kwargs) -> Any:\n",
    "    \"\"\"\n",
    "    Prefer __acall__ if it exists, otherwise try normal async/sync calling patterns\n",
    "    \"\"\"\n",
    "    if inspect.iscoroutinefunction(func_or_coro):\n",
    "        # print('iscoroutinefunction')\n",
    "        coro = func_or_coro(*args, **kwargs)  # Get the coroutine object\n",
    "        res = await coro  # Await it before returning\n",
    "        return res\n",
    "    elif hasattr(func_or_coro, '__call__'):\n",
    "        if inspect.iscoroutinefunction(func_or_coro.__call__):\n",
    "            return await func_or_coro.__call__(*args, **kwargs)\n",
    "        else:\n",
    "            return func_or_coro.__call__(*args, **kwargs)\n",
    "    elif inspect.iscoroutine(func_or_coro):\n",
    "        return await func_or_coro  # Already a coroutine, just await it\n",
    "    elif callable(func_or_coro):\n",
    "        return func_or_coro(*args, **kwargs)\n",
    "    else:\n",
    "        return func_or_coro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F():\n",
    "    def __call__(self,x):\n",
    "        print('acall')\n",
    "        return x+1\n",
    "\n",
    "class AsyncF():\n",
    "    async def __call__(self,x):\n",
    "        print('call')\n",
    "        return x+1\n",
    "\n",
    "async def plus1async(x,**kwargs):\n",
    "    await asyncio.sleep(0.1)\n",
    "    return x+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert inspect.iscoroutinefunction(plus1async)\n",
    "co = await maybe_await(plus1async,[1],{})\n",
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acall\n",
      "2\n",
      "call\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(await maybe_await(F(),[1],{}))\n",
    "print(await maybe_await(AsyncF(),[1],{}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import pickle\n",
    "from typing import Dict, Any, Tuple\n",
    "import inspect\n",
    "from joblib.memory import MemorizedFunc\n",
    "import json\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def get_input_output_from_cache(func) -> Dict[Tuple, Any]:\n",
    "    \"\"\"Extracts input/output pairs from a joblib Memory cache\n",
    "    \n",
    "    Args:\n",
    "        func: The cached function (must be decorated with joblib.Memory.cache)\n",
    "        \n",
    "    Returns:\n",
    "        Dict[Tuple, Any]: Dictionary mapping input tuples (args, kwargs) to outputs\n",
    "    \"\"\"\n",
    "    # Check if function is a MemorizedFunc type\n",
    "    if not isinstance(func, MemorizedFunc):\n",
    "        raise ValueError(f\"Function {func.__name__} doesn't appear to be cached with joblib.Memory\")\n",
    "    \n",
    "    # Get all cache directories for this function\n",
    "    backend_path = Path(func.store_backend.location)\n",
    "    func_name = func.func.__name__\n",
    "    func_cache_dirs = list(backend_path.glob(f\"*/{func_name}\"))\n",
    "    \n",
    "    if not func_cache_dirs:\n",
    "        raise ValueError(f\"No cache found for function {func_name} in {func.store_backend.location}\")\n",
    "    \n",
    "    cache_dict = {}\n",
    "    for func_cache_dir in func_cache_dirs:\n",
    "        for output_file in func_cache_dir.glob('**/output.pkl'):\n",
    "            # Get output from output.pkl\n",
    "            with open(output_file, 'rb') as f:\n",
    "                output = pickle.load(f)\n",
    "            \n",
    "            # Get inputs from metadata.json\n",
    "            metadata_file = output_file.parent / 'metadata.json'\n",
    "            if metadata_file.exists():\n",
    "                metadata = json.loads(metadata_file.read_text())\n",
    "                input_args = metadata.get('input_args', {})\n",
    "                # Create a hashable key from the input args\n",
    "                cache_key = frozenset(input_args.items())\n",
    "                cache_dict[cache_key] = output\n",
    "        \n",
    "    return cache_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def mock_from_dict(func, cache_dict: Dict[Tuple, Any], call_on_missing: bool = True):\n",
    "    \"\"\"Creates a mock function that uses cached results from a dictionary,\n",
    "    optionally calling the original function for missing inputs\n",
    "    \n",
    "    Args:\n",
    "        func: The original function to mock\n",
    "        cache_dict: Dictionary mapping input tuples (args, kwargs) to outputs\n",
    "        call_on_missing: If True, calls original function when input not in cache\n",
    "        \n",
    "    Returns:\n",
    "        callable: A function that returns cached results or calls the original\n",
    "    \"\"\"\n",
    "    # Get the original function's signature\n",
    "    sig = inspect.signature(func.func if isinstance(func, MemorizedFunc) else func)\n",
    "    \n",
    "    def mocked_func(*args, **kwargs):\n",
    "        # Convert args to kwargs using the function signature\n",
    "        bound_args = sig.bind(*args, **kwargs)\n",
    "        all_kwargs = bound_args.arguments\n",
    "        \n",
    "        # Create cache key from kwargs\n",
    "        cache_key = frozenset(all_kwargs.items())\n",
    "        \n",
    "        if cache_key in cache_dict:\n",
    "            return cache_dict[cache_key]\n",
    "        elif call_on_missing:\n",
    "            return func(**all_kwargs)\n",
    "        else:\n",
    "            raise KeyError(f\"No cached result for inputs: {all_kwargs}\")\n",
    "    \n",
    "    return mocked_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "from joblib import Memory\n",
    "from unittest.mock import patch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{frozenset({('y', '2'), ('x', '1')}): 3, frozenset({('y', '4'), ('x', '3')}): 7}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the cache\n",
    "memory = Memory(Path.home() / '.cache', verbose=0)\n",
    "# memory.clear()\n",
    "# Define and cache the function\n",
    "@memory.cache\n",
    "def expensive_api_call(x, y):\n",
    "    # Simulate expensive API call\n",
    "    print(f'calling expensive_api_call({x}, {y})')  # To show when real function is called\n",
    "    return x + y\n",
    "\n",
    "expensive_api_call(1, 2)\n",
    "expensive_api_call(3, 4)\n",
    "\n",
    "cache_dict = get_input_output_from_cache(expensive_api_call)\n",
    "print(cache_dict)\n",
    "\n",
    "# Use the mock in a context manager\n",
    "mock_func = mock_from_dict(expensive_api_call, cache_dict)\n",
    "\n",
    "with patch('__main__.expensive_api_call',mock_func) as mock_api:\n",
    "    # Configure mock to use our cached results\n",
    "    \n",
    "    # These calls will use cached results without hitting the real API\n",
    "    assert mock_func(1, 2) == 3\n",
    "    assert mock_func(3, 4) == 7\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "\n",
    "# Create a handler that flushes after each write\n",
    "class FlushingStreamHandler(logging.StreamHandler):\n",
    "    def emit(self, record):\n",
    "        super().emit(record)\n",
    "        self.flush()\n",
    "\n",
    "@contextmanager\n",
    "def checkLogs(level: int=logging.DEBUG, name :str='__main__', toFile: str|Path=None,format=\"%(message)s\"):\n",
    "    \"\"\"context manager for temporarily changing logging levels. used for debugging purposes\n",
    "\n",
    "    Args:\n",
    "        level (logging.Level: optional): logging level to change the logger to. Defaults to logging.DEBUG.\n",
    "        name (str: optional): module name to raise logging level for. Defaults to root logger\n",
    "        toFile (Path: optional): File to output logs to. Defaults to None\n",
    "        \n",
    "\n",
    "    Yields:\n",
    "        [logging.Logger]: the logger object that we raised the level of\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    current_level = logger.getEffectiveLevel()\n",
    "    format = \"%(name)s - %(levelname)s - %(message)s\"\n",
    "    logger.setLevel(level)\n",
    "    if len(logger.handlers) == 0:\n",
    "        pass\n",
    "        sh = FlushingStreamHandler()\n",
    "        sh.setFormatter(logging.Formatter(format))\n",
    "        logger.addHandler(sh)\n",
    "    if toFile != None:\n",
    "        fh = logging.FileHandler(toFile)\n",
    "        fh.setFormatter(logging.Formatter(format))\n",
    "        logger.addHandler(fh)\n",
    "    try:\n",
    "        yield logger\n",
    "    finally:\n",
    "        logger.setLevel(current_level)\n",
    "        if toFile != None:\n",
    "            logger.removeHandler(fh)\n",
    "        if len(logger.handlers) == 1:\n",
    "            logger.handlers= []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from jinja2 import Template, Environment, PackageLoader, meta\n",
    "\n",
    "def jinja_undeclared_vars(template):\n",
    "    \"\"\"Computes all undeclared vars in a jinja template\n",
    "\n",
    "    Args:\n",
    "        template (Path or str): Path to file of template or string with the template content\n",
    "\n",
    "    Returns:\n",
    "        set: set of all undeclared vars\n",
    "    \"\"\"\n",
    "    if isinstance(template, Path):\n",
    "        template = template.read_text()\n",
    "    env = Environment()\n",
    "    parsed_content = env.parse(template)\n",
    "    return meta.find_undeclared_variables(parsed_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def jinja_render(template, params: dict, silent=True, to_file: Path = None):\n",
    "    \"\"\"renders a jinja template\n",
    "\n",
    "    Args:\n",
    "        template (Path or str): Path to file of template or string with the template content\n",
    "        params (Dict): parameter dictionary with the variables to render into the template\n",
    "        silent (Bool, Optional): Whether to print the rendered template to screen, defaults to False\n",
    "        to_file (Path, Optional): If a path is supplied, prints the template to the file of said path\n",
    "\n",
    "    Returns:\n",
    "        set: set of all undeclared vars\n",
    "    \"\"\"\n",
    "    if isinstance(template, Path):\n",
    "        template = template.read_text()\n",
    "    instance_str = Template(template).render(**params)\n",
    "\n",
    "    if not silent:\n",
    "        print(instance_str)\n",
    "\n",
    "    if to_file:\n",
    "        to_file.write_text(instance_str)\n",
    "        return None\n",
    "    else:\n",
    "        return instance_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Any,Dict\n",
    "from copy import deepcopy\n",
    "from textwrap import dedent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def _clean_whitespace(content: str) -> str:\n",
    "    \"\"\"Clean whitespace in content by removing empty lines at start/end and dedenting.\n",
    "    \n",
    "    Args:\n",
    "        content: String content to clean\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned content string\n",
    "    \"\"\"\n",
    "    if not isinstance(content, str):\n",
    "        return content\n",
    "    return dedent(content).strip()\n",
    "\n",
    "def json_undeclared_vars(data: Any) -> set:\n",
    "    \"\"\"Recursively traverse a JSON-like object and collect all undeclared Jinja variables.\n",
    "    \n",
    "    Args:\n",
    "        data: JSON-like object to analyze\n",
    "        \n",
    "    Returns:\n",
    "        set: Set of all undeclared variables found in the structure\n",
    "        \n",
    "    Examples:\n",
    "        >>> data = {\n",
    "        ...     \"user_{{name}}\": {\n",
    "        ...         \"age\": \"{{age}} years old\",\n",
    "        ...         \"greeting\": \"Hello {{name}}!\"\n",
    "        ...     }\n",
    "        ... }\n",
    "        >>> json_undeclared_vars(data)\n",
    "        {'name', 'age'}\n",
    "    \"\"\"\n",
    "    undeclared = set()\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        # Check keys and values\n",
    "        for k, v in data.items():\n",
    "            if isinstance(k, str):\n",
    "                undeclared.update(jinja_undeclared_vars(k))\n",
    "            undeclared.update(json_undeclared_vars(v))\n",
    "    elif isinstance(data, list):\n",
    "        # Check list elements\n",
    "        for item in data:\n",
    "            undeclared.update(json_undeclared_vars(item))\n",
    "    elif isinstance(data, str):\n",
    "        # Check string for variables\n",
    "        undeclared.update(jinja_undeclared_vars(data))\n",
    "    \n",
    "    return undeclared\n",
    "\n",
    "def json_render(data: Any, context: Dict[str, Any],clean_whitespace: bool = True) -> Any:\n",
    "    \"\"\"Recursively traverse a JSON-like object and render all strings using Jinja.\n",
    "    \n",
    "    Args:\n",
    "        data: JSON-like object to render\n",
    "        context: Dictionary of variables to use for rendering\n",
    "        \n",
    "    Returns:\n",
    "        Rendered copy of the input data structure\n",
    "        \n",
    "    Examples:\n",
    "        >>> context = {\"name\": \"Alice\", \"age\": 30}\n",
    "        >>> data = {\n",
    "        ...     \"user_{{name}}\": {\n",
    "        ...         \"age\": \"{{age}} years old\",\n",
    "        ...         \"greeting\": \"Hello {{name}}!\"\n",
    "        ...     }\n",
    "        ... }\n",
    "        >>> json_render(data, context)\n",
    "        {\n",
    "            \"user_Alice\": {\n",
    "                \"age\": \"30 years old\",\n",
    "                \"greeting\": \"Hello Alice!\"\n",
    "            }\n",
    "        }\n",
    "    \"\"\"\n",
    "    # Deep copy to avoid modifying original\n",
    "    data = deepcopy(data)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        # Create new dict with rendered keys and recursively rendered values\n",
    "        return {\n",
    "            jinja_render(str(k), context): json_render(v, context,clean_whitespace) \n",
    "            for k, v in data.items()\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively render list elements\n",
    "        return [json_render(item, context,clean_whitespace) for item in data]\n",
    "    elif isinstance(data, str):\n",
    "        # Render string values\n",
    "        if clean_whitespace:\n",
    "            return _clean_whitespace(jinja_render(data, context))\n",
    "        else:\n",
    "            return jinja_render(data, context)\n",
    "    else:\n",
    "        # Return non-string values unchanged\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "        \"{{user_name}}\": {\n",
    "                 \"age\": [\"{{age}} years old\"],\n",
    "                 \"greeting\": \"\"\"\n",
    "                 Hello\n",
    "                 {{name}}!\n",
    "                 \"\"\",\n",
    "             }\n",
    "        }\n",
    "\n",
    "assert json_undeclared_vars(data) == {'user_name', 'age', 'name'}\n",
    "context = {\"user_name\": \"John\", \"age\": 30, \"name\": \"Alice\"}\n",
    "rendered = json_render(data, context)\n",
    "assert rendered == {'John': {'age': ['30 years old'], 'greeting': 'Hello\\nAlice!'}},rendered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better exception messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from functools import wraps\n",
    "import inspect\n",
    "from typing import Callable, TypeVar, ParamSpec\n",
    "from stringdale.core import jinja_render\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "P = ParamSpec(\"P\")\n",
    "R = TypeVar(\"R\")\n",
    "\n",
    "def wrap_exception(template: str, verbose: bool = False) -> Callable[[Callable[P, R]], Callable[P, R]]:\n",
    "    \"\"\"Decorator that adds context to exceptions using a template string.\n",
    "    \n",
    "    Args:\n",
    "        template: A jinja template string that can reference function kwargs\n",
    "        verbose: If True, includes all local variables in error message\n",
    "    \n",
    "    Example:\n",
    "        @helpful_context(\"Failed to process {{filename}}\")\n",
    "        def process_file(filename: str):\n",
    "            ...\n",
    "    \"\"\"\n",
    "    def decorator(func: Callable[P, R]) -> Callable[P, R]:\n",
    "        required_vars = jinja_undeclared_vars(template)\n",
    "        if 'self' in required_vars:\n",
    "            raise ValueError(\"self is not allowed in template strings\")\n",
    "        @wraps(func)\n",
    "        def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                # Get bound arguments\n",
    "                sig = inspect.signature(func)\n",
    "                bound_args = sig.bind(*args, **kwargs)\n",
    "                bound_args.apply_defaults()\n",
    "                \n",
    "                # Get local variables at time of error\n",
    "                frame = inspect.trace()[-2][0]\n",
    "                local_vars = {k:v for k,v in frame.f_locals.items() \n",
    "                            if not k.startswith('__')}\n",
    "                \n",
    "                # Combine args and locals for template rendering\n",
    "                template_vars = {**bound_args.arguments, **local_vars}\n",
    "                template_vars = {k:repr(v) for k,v in template_vars.items()}\n",
    "\n",
    "                # cant render self since it clashes with the render class of jinja\n",
    "                if 'self' in template_vars:\n",
    "                    template_vars.pop('self')\n",
    "                                \n",
    "                # Render template with combined context\n",
    "                context_msg = jinja_render(template, template_vars)\n",
    "                \n",
    "                # Build error message\n",
    "                error_parts = [\n",
    "                    f\"{context_msg}\",\n",
    "                ]\n",
    "                \n",
    "                # Add locals if verbose\n",
    "                if verbose:\n",
    "                    error_parts.extend([\n",
    "                        f\"\\nLocal variables in {func.__name__}() at time of error:\",\n",
    "                        *[f\"  {k} = {repr(v)}\" for k,v in local_vars.items()]\n",
    "                    ])\n",
    "                \n",
    "                # Add original error at the end\n",
    "                error_parts.append(f\"{str(e)}\")\n",
    "                \n",
    "                raise type(e)(\"\\n\".join(error_parts)) from e\n",
    "                \n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'\\n'.join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read config file 'missing.json'. Local x value was 42\n",
      "\n",
      "Local variables in read_config() at time of error:\n",
      "  filepath = 'missing.json'\n",
      "  encoding = 'utf-8'\n",
      "  x = 42\n",
      "  data = {'test': 123}\n",
      "[Errno 2] No such file or directory: 'missing.json'\n"
     ]
    }
   ],
   "source": [
    "@wrap_exception(\"Failed to read config file {{filepath}}. Local x value was {{x}}\", verbose=True)\n",
    "def read_config(filepath: str, encoding: str = 'utf-8') -> dict:\n",
    "    x = 42  # example local variable\n",
    "    data = {'test': 123}  # another local\n",
    "    with open(filepath, encoding=encoding) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# When called with invalid parameters:\n",
    "try:\n",
    "    read_config('missing.json')\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    # Output will be something like:\n",
    "    # Failed to read config file missing.json. Local x value was 42\n",
    "    # Original error: [Errno 2] No such file or directory: 'missing.json'\n",
    "    # \n",
    "    # Local variables at time of error:\n",
    "    #   filepath = 'missing.json'\n",
    "    #   encoding = 'utf-8'\n",
    "    #   x = 42\n",
    "    #   data = {'test': 123}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def is_valid_object(obj, model):\n",
    "    try:\n",
    "        model.model_validate(obj)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Foo(BaseModel):\n",
    "    a: int\n",
    "    b: str\n",
    "\n",
    "\n",
    "assert is_valid_object({\"a\": 10, \"b\": \"adf\"}, Foo)\n",
    "assert not is_valid_object({\"a\": \"hehe\", \"b\": \"adf\"}, Foo)\n",
    "assert is_valid_object({\"a\": 10, \"b\": \"adf\", \"c\": 3}, Foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from enum import Enum\n",
    "from pydantic import Field\n",
    "from typing import Union\n",
    "\n",
    "#| export\n",
    "def get_missing(model, keys=None):\n",
    "    \"\"\"\n",
    "    Recursively check a model for Missing or None values and return a list of keys with Missing/None values.\n",
    "    \n",
    "    Args:\n",
    "        model: A pydantic model instance or dict\n",
    "        keys: Optional list of specific keys to check. If None, checks all keys.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of keys (as strings) that have Missing or None values\n",
    "    \"\"\"\n",
    "    missing_keys = []\n",
    "    \n",
    "    # Convert BaseModel to dict if needed\n",
    "    if isinstance(model, BaseModel):\n",
    "        model = model.model_dump()\n",
    "    \n",
    "    # If keys is provided, only check those specific keys\n",
    "    items_to_check = model.items()\n",
    "    if keys is not None:\n",
    "        items_to_check = [(k, model.get(k)) for k in keys if k in model]\n",
    "\n",
    "    for key, value in items_to_check:\n",
    "        # Check for Missing enum or None\n",
    "        if value is None:\n",
    "            missing_keys.append(key)\n",
    "        # Recursively check nested structures\n",
    "        elif isinstance(value, (dict, BaseModel)):\n",
    "            nested_keys = None if keys is None else [\n",
    "                k.split('.', 1)[1] for k in (keys or [])\n",
    "                if k.startswith(f\"{key}.\") and '.' in k\n",
    "            ]\n",
    "            if nested_keys != []:  # Only recurse if we have nested keys to check or keys is None\n",
    "                nested_missing = get_missing(value, nested_keys)\n",
    "                missing_keys.extend(f\"{key}.{k}\" for k in nested_missing)\n",
    "                \n",
    "    return missing_keys\n",
    "\n",
    "def has_missing(model, keys=None):\n",
    "    return len(get_missing(model, keys)) > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from sqlmodel import Field,SQLModel\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UserData(SQLModel):\n",
    "    name: Optional[str] = Field(None, description='The name of the user')\n",
    "    age: Optional[int] = Field(None, description='The age of the user')\n",
    "    email: Optional[str] = Field(None, description='The email of the user')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test empty dict\n",
    "assert has_missing(UserData()) == True\n",
    "\n",
    "# Test basic dict with Missing value\n",
    "test_dict = {\"name\": None, \"age\": 25}\n",
    "assert get_missing(test_dict) == [\"name\"]\n",
    "\n",
    "# Test dict with no Missing values\n",
    "test_dict = {\"name\": \"John\", \"age\": 25}\n",
    "assert get_missing(test_dict) == []\n",
    "\n",
    "# Test nested dict with Missing values\n",
    "test_nested = {\n",
    "    \"user\": {\n",
    "        \"name\": \"John\",\n",
    "        \"details\": {\n",
    "            \"email\": None,\n",
    "            \"phone\": \"123-456-7890\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "assert get_missing(test_nested) == [\"user.details.email\"]\n",
    "\n",
    "# Test with Pydantic model\n",
    "test_model = UserData(name=\"John\", age=None, email=\"john@example.com\")\n",
    "assert get_missing(test_model) == [\"age\"]\n",
    "\n",
    "# Test with all Missing values\n",
    "test_all_missing = UserData(name=None, age=None, email=None)\n",
    "assert sorted(get_missing(test_all_missing)) == sorted([\"name\", \"age\", \"email\"])\n",
    "\n",
    "test_partial = UserData(name=None, age=25, email=None)\n",
    "assert get_missing(test_partial, keys=[\"name\", \"age\"]) == [\"name\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
