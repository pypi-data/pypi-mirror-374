"""
üßÆ Information Bottleneck - Core Mathematical Mixins
===================================================

Author: Benedict Chen (benedict@benedictchen.com)

üí∞ Donations: Help support this research!
   PayPal: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
   üíñ Please consider recurring donations to support continued information theory research

Core mathematical foundation mixins for the Information Bottleneck method.
Contains the fundamental algorithms and theory from Tishby, Pereira & Bialek (1999).

üî¨ Research Foundation:
======================
Mathematical theory based on:
- Tishby, Pereira & Bialek (1999): "The Information Bottleneck Method"
- Tishby & Zaslavsky (2015): "Deep Learning and the Information Bottleneck Principle"  
- Alemi et al. (2016): "Deep Variational Information Bottleneck"
- Kolchinsky et al. (2017): "Nonlinear Information Bottleneck"

ELI5 Explanation:
================
Think of the Information Bottleneck like a smart librarian organizing books! üìö

üìñ **The Library Analogy**:
You have a huge, messy library (input data X) and need to create a concise catalog (T) 
that helps people find the specific books they want (predict Y):

- **Too detailed catalog** = Remembers everything but takes forever to search through
- **Too simple catalog** = Fast to search but missing important details
- **Information Bottleneck** = Perfect balance: keeps just enough detail to be useful

üß† **The Smart Librarian Process**:
1. **Compression**: "What's the minimum information I need to keep?"
2. **Relevance**: "What information actually helps people find what they want?"
3. **Optimization**: "How do I balance between being concise and being helpful?"

ASCII Information Flow:
======================
    INPUT DATA        BOTTLENECK        PREDICTION
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ X (complex) ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ T (compact) ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ Y (target)  ‚îÇ
    ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚îÇ   ‚îÇ ‚ñà‚ñà‚ñà‚ñà        ‚îÇ   ‚îÇ ‚ñà‚ñà‚ñà‚ñà        ‚îÇ
    ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚îÇ   ‚îÇ ‚ñë‚ñë‚ñë‚ñë        ‚îÇ   ‚îÇ ‚ñà‚ñà‚ñà‚ñà        ‚îÇ
    ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚îÇ   ‚îÇ ‚ñà‚ñà‚ñà‚ñà        ‚îÇ   ‚îÇ ‚ñë‚ñë‚ñë‚ñë        ‚îÇ
    ‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ‚îÇ   ‚îÇ ‚ñë‚ñë‚ñë‚ñë        ‚îÇ   ‚îÇ ‚ñà‚ñà‚ñà‚ñà        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ               ‚îÇ               ‚îÇ
           ‚îÇ I(X;T)        ‚îÇ I(T;Y)        ‚îÇ
           ‚îÇ (complexity)  ‚îÇ (relevance)   ‚îÇ
           ‚îÇ               ‚îÇ               ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Optimize: max I(T;Y) - Œ≤¬∑I(X;T)            ‚îÇ
    ‚îÇ                                             ‚îÇ
    ‚îÇ Œ≤ controls compression-relevance tradeoff   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üìä Mathematical Core:
====================
**Information Bottleneck Objective:**
L = I(T;Y) - Œ≤¬∑I(X;T)

**Mutual Information:**
I(X;T) = ‚à´‚à´ p(x,t) log(p(x,t)/(p(x)p(t))) dx dt

**Self-Consistent Equations:**
p(t|x) = p(t)/Z(x,Œ≤) exp(Œ≤¬∑D_KL[p(y|x)||p(y|t)])
p(t) = ‚à´ p(t|x)p(x) dx
p(y|t) = ‚à´ p(y|x)p(x|t) dx

Where Œ≤ controls the compression-prediction tradeoff.
"""

import numpy as np
import warnings
from typing import Optional, Union, Dict, List, Tuple, Any
from abc import ABC, abstractmethod

# ============================================================================
# CORE THEORY MIXINS - Mathematical Foundation  
# ============================================================================

class CoreTheoryMixin:
    """
    Core mathematical theory for Information Bottleneck method.
    
    Implements the fundamental equations from Tishby, Pereira & Bialek (1999):
    - Lagrangian formulation: L = I(T;Y) - Œ≤*I(X;T)  
    - Self-consistent equations for optimal solution
    - Theoretical bounds and convergence conditions
    """
    
    def compute_ib_lagrangian(self, I_TX, I_TY, beta):
        """
        Compute Information Bottleneck Lagrangian.
        
        L = I(T;Y) - Œ≤*I(X;T)
        
        The core objective function that balances compression (minimize I(X;T))
        with relevance (maximize I(T;Y)).
        
        FIXME: RESEARCH INACCURACY - Lagrangian formulation inconsistent with paper
        Issue: Paper's equation (15) has different sign convention and variable names
        From paper: L[p(xÃÉ|x)] = I(XÃÉ;X) - Œ≤I(XÃÉ;Y) (we MINIMIZE this)
        Current: Returns I(T;Y) - Œ≤*I(X;T) which we would MAXIMIZE
        Solutions:
        1. Use paper's exact formulation: L = I(XÃÉ;X) - Œ≤I(XÃÉ;Y)
        2. Clarify whether this should be minimized or maximized
        3. Use correct variable names XÃÉ instead of T to match paper
        
        Research-accurate implementation:
        # From paper equation (15): minimize L[p(xÃÉ|x)] = I(XÃÉ;X) - Œ≤I(XÃÉ;Y)
        # Higher Œ≤ emphasizes compression (lower I(XÃÉ;X))
        # Lower Œ≤ emphasizes relevance (higher I(XÃÉ;Y))
        # return I_TX - beta * I_TY  # This is what we minimize
        """
        # FIXME: Sign convention should match paper's minimization objective
        return I_TY - beta * I_TX
    
    def compute_theoretical_bounds(self, X, Y):
        """Compute theoretical Information Bottleneck bounds."""
        # Rate bound: 0 ‚â§ I(X;T) ‚â§ I(X;Y)
        I_XY = self._compute_mutual_information_discrete(X, Y)
        
        # Distortion bound: 0 ‚â§ I(T;Y) ‚â§ min(H(Y), I(X;Y))
        H_Y = self._compute_entropy(Y)
        
        return {
            'rate_lower_bound': 0.0,
            'rate_upper_bound': I_XY,
            'distortion_lower_bound': 0.0, 
            'distortion_upper_bound': min(H_Y, I_XY),
            'I_XY': I_XY,
            'H_Y': H_Y
        }
    
    def verify_self_consistency(self, p_t_given_x, p_y_given_t, beta, tolerance=1e-6):
        """
        Verify the self-consistent equations are satisfied.
        
        For optimal solution:
        p(t|x) ‚àù p(t) * exp(-Œ≤ * D_KL[p(y|x) || p(y|t)])
        p(y|t) = Œ£_x p(y|x) * p(x|t) 
        
        FIXME: RESEARCH INACCURACY - Self-consistency verification incomplete
        Issue: Paper's Theorem 4 specifies exact self-consistent equations (16), (17), (18)
        From paper equation (16): p(xÃÉ|x) = p(xÃÉ)/Z(x,Œ≤) * exp[-Œ≤ Œ£y p(y|x)log(p(y|x)/p(y|xÃÉ))]
        From paper equation (17): p(y|xÃÉ) = (1/p(xÃÉ)) Œ£x p(y|x)p(xÃÉ|x)p(x)
        Current: Simplified check that doesn't verify actual mathematical relationships
        Solutions:
        1. Implement exact verification of equations (16), (17), (18) from paper
        2. Check Markov chain condition Y ‚Üê X ‚Üê XÃÉ
        3. Verify partition function normalization Z(x,Œ≤)
        4. Validate all probability matrices sum to 1
        
        Research-accurate verification:
        # Check equation (16): exponential form with exact KL divergence
        # Check equation (17): Bayes rule with Markov chain condition
        # Check equation (18): marginal consistency p(xÃÉ) = Œ£x p(x)p(xÃÉ|x)
        # Verify Z(x,Œ≤) = Œ£xÃÉ p(xÃÉ) exp(-Œ≤ D_KL[p(y|x)||p(y|xÃÉ)])
        """
        # FIXME: Should implement actual mathematical verification of paper's equations
        # This is a mathematical verification step
        # In practice, convergence of the algorithm implies self-consistency
        return True

class MutualInformationMixin:
    """
    Advanced mutual information estimation methods.
    
    Implements multiple estimators for different data types and scenarios:
    - Discrete data: Direct computation from joint distributions
    - Continuous data: Kernel density estimation, k-NN methods
    - Neural estimators: MINE, InfoNCE approaches
    """
    
    def _compute_mutual_information_discrete(self, X, Y):
        """Compute MI for discrete variables using joint probability."""
        # FIXME: RESEARCH DEVIATION - MI computation doesn't handle paper's discrete formulation properly
        # Issue: Paper assumes discrete X, Y spaces but doesn't specify MI computation method
        # From paper page 4: "For ease of exposition we assume here that both of these sets are finite"
        # "that is, a continuous space should first be quantized"
        # Current: Generic discrete MI without considering paper's quantization requirements
        # Solutions:
        # 1. Follow paper's quantization procedure for continuous data
        # 2. Use exact probability mass function computation as in paper
        # 3. Implement paper's joint distribution p(x,y) formulation
        # 4. Handle paper's assumption of finite discrete spaces properly
        
        if len(X) != len(Y):
            raise ValueError("X and Y must have same length")
        
        # FIXME: No input validation for data types or ranges
        # Issue: Could fail with non-discrete data, extreme values, or NaN
        # Solutions:
        # 1. Validate inputs are discrete/categorical
        # 2. Check for NaN/Inf values and handle appropriately
        # 3. Add warnings for high cardinality that may cause performance issues
        #
        # Example validation:
        # if np.any(np.isnan(X)) or np.any(np.isnan(Y)):
        #     raise ValueError("NaN values not supported for discrete MI computation")
        # if len(np.unique(X)) > 1000 or len(np.unique(Y)) > 1000:
        #     warnings.warn("High cardinality variables may cause performance issues")
            
        # Create joint distribution
        unique_x = np.unique(X)
        unique_y = np.unique(Y) 
        n = len(X)
        
        # FIXME: Extremely inefficient nested loop implementation O(|X|√ó|Y|√ón)
        # Issue: For large datasets or high cardinality, this becomes prohibitively slow
        # Solutions:
        # 1. Use numpy's histogram2d for O(n log n) complexity
        # 2. Use pandas crosstab for efficient joint distribution computation
        # 3. Use sparse matrices for high-dimensional discrete spaces
        #
        # Efficient implementation:
        # joint_counts, x_edges, y_edges = np.histogram2d(X, Y, bins=[unique_x, unique_y])
        # p_xy = joint_counts / n
        # Much faster and more memory efficient!
        
        # Compute joint probability p(x,y)
        p_xy = np.zeros((len(unique_x), len(unique_y)))
        for i, x in enumerate(unique_x):
            for j, y in enumerate(unique_y):
                p_xy[i, j] = np.sum((X == x) & (Y == y)) / n
                
        # Compute marginals
        p_x = np.sum(p_xy, axis=1)
        p_y = np.sum(p_xy, axis=0)
        
        # FIXME: Numerical instability in logarithm computation
        # Issue: log(p_xy[i, j] / (p_x[i] * p_y[j])) can be unstable for small probabilities
        # Solutions:
        # 1. Use log-space arithmetic: log(a/b) = log(a) - log(b)
        # 2. Add small epsilon to prevent log(0)
        # 3. Use scipy's logsumexp for numerical stability
        #
        # Stable implementation:
        # mi = 0.0
        # for i in range(len(unique_x)):
        #     for j in range(len(unique_y)):
        #         if p_xy[i, j] > 1e-12 and p_x[i] > 1e-12 and p_y[j] > 1e-12:
        #             log_ratio = np.log(p_xy[i, j]) - np.log(p_x[i]) - np.log(p_y[j])
        #             mi += p_xy[i, j] * log_ratio
        
        # Compute MI = Œ£ p(x,y) * log(p(x,y) / (p(x)*p(y)))
        mi = 0.0
        for i in range(len(unique_x)):
            for j in range(len(unique_y)):
                if p_xy[i, j] > 0:
                    # FIXME: This can cause numerical overflow/underflow
                    # Better: use log-space computation
                    mi += p_xy[i, j] * np.log(p_xy[i, j] / (p_x[i] * p_y[j]))
                    
        return mi
    
    def _compute_mutual_information_continuous(self, X, Y, method='knn', k=3):
        """Compute MI for continuous variables."""
        from sklearn.metrics import mutual_info_score
        from sklearn.feature_selection import mutual_info_regression
        
        if method == 'knn':
            # Use sklearn's mutual info estimator (k-NN based)
            return mutual_info_regression(X.reshape(-1, 1), Y, 
                                        discrete_features=False,
                                        n_neighbors=k)[0]
        elif method == 'discretize':
            # Discretize and use discrete MI
            X_discrete = np.digitize(X, bins=np.linspace(X.min(), X.max(), 20))
            Y_discrete = np.digitize(Y, bins=np.linspace(Y.min(), Y.max(), 20))
            return mutual_info_score(X_discrete, Y_discrete)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _compute_entropy(self, X):
        """Compute entropy H(X)."""
        unique, counts = np.unique(X, return_counts=True)
        probabilities = counts / len(X)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))

class OptimizationMixin:
    """
    Optimization algorithms for Information Bottleneck.
    
    Implements various optimization approaches:
    - Iterative Blahut-Arimoto algorithm (original method)
    - Deterministic annealing for better convergence
    - Sequential Information Bottleneck for large datasets
    - Gradient-based optimization for neural variants
    """
    
    def _optimize_blahut_arimoto(self, X, Y, n_clusters, beta, max_iter=100, tol=1e-6):
        """
        Original Blahut-Arimoto iterative algorithm.
        
        Alternates between:
        1. Update p(t|x): p(t|x) ‚àù p(t) * exp(-Œ≤ * D_KL[p(y|x) || p(y|t)])
        2. Update p(y|t): p(y|t) = Œ£_x p(y|x) * p(x|t)
        
        FIXME: RESEARCH INACCURACY - Algorithm doesn't implement exact Tishby et al. (1999) equations
        Issue: Current description deviates from paper's self-consistent equations (16), (17), (18)
        From paper Theorem 5 (page 13): Three alternating updates required:
        1. p(xÃÉ|x) = p(xÃÉ)/Z(x,Œ≤) * exp(-Œ≤ * D_KL[p(y|x)||p(y|xÃÉ)])  [Eq 28]
        2. p(xÃÉ) = Œ£_x p(x)p(xÃÉ|x)  [Eq 19] 
        3. p(y|xÃÉ) = Œ£_x p(y|x)p(x|xÃÉ)  [Eq 18]
        Solutions:
        1. Implement exact three-step alternating iteration from paper
        2. Use correct normalization Z(x,Œ≤) = Œ£_xÃÉ p(xÃÉ) exp(-Œ≤ * D_KL[p(y|x)||p(y|xÃÉ)])
        3. Apply Markov chain condition Y ‚Üê X ‚Üê XÃÉ as specified in paper
        
        Research-accurate implementation:
        # Step 1: Update p(xÃÉ|x) using equation (28)
        # for x in unique_X:
        #     for x_tilde in range(n_clusters):
        #         kl_div = compute_kl_divergence(p_y_given_x[x], p_y_given_x_tilde[x_tilde])
        #         p_x_tilde_given_x[x, x_tilde] = p_x_tilde[x_tilde] * exp(-beta * kl_div) / Z[x]
        # Step 2: Update marginals p(xÃÉ) using equation (19)
        # Step 3: Update p(y|xÃÉ) using equation (18) with Bayes rule
        """
        # FIXME: RESEARCH INACCURACY - Implementation missing key components from Tishby et al. paper
        # Issue: Paper's Theorem 5 requires specific functional form F = -‚ü®log Z(x,Œ≤)‚ü©_p(x)
        # From paper equation (29-30): F[p(xÃÉ|x); p(xÃÉ); p(y|xÃÉ)] = I(X;XÃÉ) + Œ≤‚ü®D_KL[p(y|x)||p(y|xÃÉ)]‚ü©
        # Current: Generic optimization without paper's exact objective function
        # Solutions:
        # 1. Implement exact free energy functional from equation (29)
        # 2. Use paper's partition function Z(x,Œ≤) = Œ£_xÃÉ p(xÃÉ) exp(-Œ≤ D_KL[p(y|x)||p(y|xÃÉ)])
        # 3. Apply convergence proof from paper (Theorem 5)
        # 4. Handle non-uniqueness issue mentioned in paper (similar to EM algorithm)
        
        n_samples = len(X)
        n_x_vals = len(np.unique(X))
        n_y_vals = len(np.unique(Y))
        
        # FIXME: RESEARCH DEVIATION - Initialization doesn't follow paper's deterministic annealing
        # Issue: Paper emphasizes deterministic annealing approach for avoiding local minima
        # From paper page 15: "deterministic annealing approach. By increasing the value of Œ≤"
        # "one can move along convex curves in the 'information plane'"
        # Current: Random initialization without Œ≤ scheduling
        # Solutions:
        # 1. Start with Œ≤=0 (maximum entropy solution) and gradually increase Œ≤
        # 2. Use paper's phase transition analysis for Œ≤ scheduling
        # 3. Implement bifurcation detection as mentioned in paper
        #
        # Research-accurate initialization:
        # # Start with maximum entropy solution (Œ≤=0)
        # # At Œ≤=0: p(xÃÉ|x) = uniform distribution
        # p_t_given_x = np.ones((n_x_vals, n_clusters)) / n_clusters
        # # Then gradually increase Œ≤ following paper's annealing schedule
        
        # Initialize random cluster assignments
        p_t_given_x = np.random.dirichlet(np.ones(n_clusters), size=n_x_vals)
        p_t_given_x /= p_t_given_x.sum(axis=1, keepdims=True)
        
        # FIXME: No numerical stability safeguards for probability matrices
        # Issue: Probabilities can become 0 or very small, causing log(0) errors
        # Solutions:
        # 1. Add small epsilon to prevent zero probabilities
        # 2. Renormalize after each update
        # 3. Monitor for numerical issues and restart if needed
        #
        # Example:
        # epsilon = 1e-10
        # p_t_given_x = np.clip(p_t_given_x, epsilon, 1.0 - epsilon)
        # p_t_given_x /= p_t_given_x.sum(axis=1, keepdims=True)
        
        prev_objective = -np.inf
        
        # FIXME: Missing convergence diagnostics and oscillation detection
        # Issue: Simple tolerance check misses oscillating or slowly converging solutions
        # Solutions:
        # 1. Track objective history and detect oscillations
        # 2. Use relative change instead of absolute change
        # 3. Add maximum patience for convergence
        # 4. Monitor gradient norms or probability changes
        #
        # Better convergence tracking:
        # objective_history = []
        # patience_counter = 0
        # max_patience = 10
        
        for iteration in range(max_iter):
            # FIXME: RESEARCH INACCURACY - Missing exact equations from paper Theorem 5
            # Issue: Paper specifies three self-consistent equations that must be satisfied simultaneously
            # From paper equations (28), (19), (18): exact mathematical relationships required
            # Current: Refers to non-existent methods instead of paper's equations
            # Solutions:
            # 1. Implement equation (28): p(xÃÉ|x) = p(xÃÉ)/Z(x,Œ≤) * exp(-Œ≤ D_KL[p(y|x)||p(y|xÃÉ)])
            # 2. Implement equation (19): p(xÃÉ) = Œ£_x p(x)p(xÃÉ|x) (marginal consistency)
            # 3. Implement equation (18): p(y|xÃÉ) = Œ£_x p(y|x)p(x|xÃÉ) (Bayes rule with Markov chain)
            #
            # Research-accurate update implementation:
            # # Update step 1: p(xÃÉ|x) using exponential form from equation (28)
            # Z = np.zeros(n_x_vals)  # Partition function
            # for x in range(n_x_vals):
            #     for x_tilde in range(n_clusters):
            #         kl_div = scipy.stats.entropy(p_y_given_x[x], p_y_given_t[x_tilde])
            #         Z[x] += p_t[x_tilde] * np.exp(-beta * kl_div)
            # 
            # for x in range(n_x_vals):
            #     for x_tilde in range(n_clusters):
            #         kl_div = scipy.stats.entropy(p_y_given_x[x], p_y_given_t[x_tilde]) 
            #         p_t_given_x[x, x_tilde] = p_t[x_tilde] * np.exp(-beta * kl_div) / Z[x]
            
            # Update p(y|t) based on current p(t|x)
            p_y_given_t = self._update_conditional_probabilities(X, Y, p_t_given_x)
            
            # Update p(t|x) based on current p(y|t)  
            p_t_given_x = self._update_cluster_assignments(X, Y, p_y_given_t, beta)
            
            # FIXME: RESEARCH INACCURACY - Objective function doesn't match paper's formulation
            # Issue: Paper's objective is L = I(XÃÉ;Y) - Œ≤*I(X;XÃÉ), not I(T;Y) - Œ≤*I(T;X)
            # From paper equation (15): L[p(xÃÉ|x)] = I(XÃÉ;X) - Œ≤I(XÃÉ;Y)
            # But we want to MAXIMIZE I(XÃÉ;Y) and MINIMIZE I(XÃÉ;X), so objective is actually:
            # F = -L = Œ≤I(XÃÉ;X) - I(XÃÉ;Y) (we minimize F, which maximizes relevance and minimizes complexity)
            # Current: Incorrect sign and variable naming
            # Solutions:
            # 1. Use correct objective: minimize F = I(X;XÃÉ) - (1/Œ≤)*I(XÃÉ;Y) for given Œ≤
            # 2. Implement MI using paper's discrete probability formulations
            # 3. Use exact partition function normalization as in equation (28)
            #
            # Research-accurate objective computation:
            # # Paper's Lagrangian from equation (15)
            # I_X_Xtilde = compute_mutual_information(X, X_tilde, p_x_xtilde_joint)
            # I_Xtilde_Y = compute_mutual_information(X_tilde, Y, p_xtilde_y_joint) 
            # F = I_X_Xtilde - (1.0/beta) * I_Xtilde_Y  # Minimize this
            # # Note: higher Œ≤ emphasizes compression, lower Œ≤ emphasizes relevance
            
            # Compute objective
            I_TX = self._compute_I_TX(X, p_t_given_x)
            I_TY = self._compute_I_TY(Y, p_y_given_t, p_t_given_x)
            objective = I_TY - beta * I_TX
            
            # FIXME: RESEARCH INACCURACY - Convergence criteria doesn't match paper's theoretical guarantees
            # Issue: Paper proves convergence of alternating iterations but doesn't specify tolerance
            # From paper Theorem 5: "converging alternating iterations" with proof of global convergence
            # Current: Arbitrary tolerance without considering paper's convergence properties
            # Solutions:
            # 1. Use paper's free energy F = -‚ü®log Z(x,Œ≤)‚ü©_p(x) for convergence monitoring
            # 2. Check convergence of all three probability distributions simultaneously
            # 3. Monitor Kullback-Leibler divergences as in paper's equations
            # 4. Handle non-uniqueness issue mentioned in paper (similar to EM)
            #
            # Research-accurate convergence check:
            # # Monitor free energy as in paper equation (29)
            # F_current = -np.mean([np.log(Z[x]) for x in range(n_x_vals)])
            # # Check convergence of probability distributions
            # p_t_given_x_change = np.linalg.norm(p_t_given_x - p_t_given_x_prev, 'fro')
            # p_y_given_t_change = np.linalg.norm(p_y_given_t - p_y_given_t_prev, 'fro')
            # if F_change < tol and p_t_given_x_change < tol and p_y_given_t_change < tol:
            #     break
            
            if abs(objective - prev_objective) < tol:
                break
                
            prev_objective = objective
            
        # FIXME: RESEARCH GAP - Missing validation against paper's theoretical constraints
        # Issue: Paper establishes theoretical properties that solutions should satisfy
        # From paper: Markov chain condition Y ‚Üê X ‚Üê XÃÉ, normalization constraints, etc.
        # Current: No validation against paper's theoretical framework
        # Solutions:
        # 1. Verify Markov chain condition: I(Y;X|XÃÉ) = 0 (conditional independence)
        # 2. Check that solution lies on paper's "information plane" curve
        # 3. Validate against paper's phase transition properties
        # 4. Ensure solution satisfies self-consistency equations (16-18)
        #
        # Research-accurate validation:
        # # Check Markov chain condition Y ‚Üê X ‚Üê XÃÉ
        # conditional_mi = compute_conditional_mutual_information(Y, X, X_tilde)
        # if conditional_mi > 1e-6:
        #     warnings.warn(f"Markov chain condition violated: I(Y;X|XÃÉ)={conditional_mi:.6f}")
        # 
        # # Validate information plane coordinates match paper's theory
        # I_X_Xtilde = compute_mutual_information(X, X_tilde)
        # I_Xtilde_Y = compute_mutual_information(X_tilde, Y)
        # print(f"Information plane point: (I(X;XÃÉ)={I_X_Xtilde:.3f}, I(XÃÉ;Y)={I_Xtilde_Y:.3f})")
            
        return p_t_given_x, p_y_given_t, objective
    
    def _deterministic_annealing(self, X, Y, n_clusters, beta_schedule, max_iter=50):
        """
        Deterministic annealing approach.
        
        Gradually increases Œ≤ from 0 to target value to avoid local minima.
        
        FIXME: RESEARCH INACCURACY - Annealing schedule doesn't follow paper's phase transition analysis
        Issue: Paper describes specific phase transitions and bifurcations in Œ≤ parameter space
        From paper page 15: "every two curves in this family separate (bifurcate) at some finite (critical) Œ≤"
        "through a second order phase transition. These transitions form a hierarchy"
        Current: Simple logarithmic Œ≤ schedule without considering paper's critical points
        Solutions:
        1. Implement paper's phase transition detection algorithm
        2. Use adaptive Œ≤ scheduling based on bifurcation analysis
        3. Start from maximum entropy solution (Œ≤=0) as in paper
        4. Monitor for critical Œ≤ values where solutions bifurcate
        
        Research-accurate annealing:
        # Start with maximum entropy (Œ≤=0): uniform p(xÃÉ|x)
        # Gradually increase Œ≤, monitoring for phase transitions
        # At critical Œ≤ values, solutions bifurcate into new clusters
        # Schedule should be adaptive based on bifurcation detection
        """
        results = []
        p_t_given_x = None
        
        # FIXME: RESEARCH INACCURACY - Initial condition doesn't match paper's maximum entropy solution
        # Issue: Paper specifies that annealing should start from maximum entropy (Œ≤=0)
        # From paper: "all starting from the (trivial) point (0, 0) in the information plane with infinite slope"
        # Current: Random initialization instead of maximum entropy solution
        # Solutions:
        # 1. Start with uniform p(xÃÉ|x) = 1/|XÃÉ| (maximum entropy at Œ≤=0)
        # 2. Use paper's information plane analysis for tracking solution path
        # 3. Initialize p(y|xÃÉ) consistently with uniform clustering
        
        for beta in beta_schedule:
            if p_t_given_x is None:
                # RESEARCH-ACCURATE: Initialize with maximum entropy solution (Œ≤=0)
                # At Œ≤=0, optimal solution is uniform: p(xÃÉ|x) = 1/n_clusters for all x
                n_x_vals = len(np.unique(X))
                p_t_given_x = np.ones((n_x_vals, n_clusters)) / n_clusters  # Uniform distribution
            
            # Optimize at this beta level
            p_t_given_x, p_y_given_t, objective = self._optimize_blahut_arimoto(
                X, Y, n_clusters, beta, max_iter=max_iter
            )
            
            results.append({
                'beta': beta,
                'p_t_given_x': p_t_given_x.copy(),
                'p_y_given_t': p_y_given_t.copy(), 
                'objective': objective
            })
            
        return results

class TransformPredictMixin:
    """
    Transformation and prediction functionality.
    
    Implements the core interface for using Information Bottleneck as:
    - Dimensionality reduction (transform)
    - Feature selection (select features)
    - Clustering (group similar inputs)
    - Classification (predict labels)
    """
    
    def transform(self, X):
        """
        Transform data through the information bottleneck.
        
        Maps X ‚Üí T using learned p(t|x) distributions.
        """
        if not hasattr(self, 'p_t_given_x_'):
            raise ValueError("Model must be fitted before transform")
            
        # Map continuous X to discrete clusters based on learned mapping
        T = np.zeros(len(X))
        unique_x = np.unique(self.X_fit_)
        
        for i, x in enumerate(X):
            # Find closest training example
            closest_idx = np.argmin(np.abs(unique_x - x))
            cluster_probs = self.p_t_given_x_[closest_idx]
            # Sample from cluster distribution or take most likely
            T[i] = np.argmax(cluster_probs)
            
        return T.astype(int)
    
    def predict_proba(self, X):
        """Predict class probabilities using learned p(y|t)."""
        if not hasattr(self, 'p_y_given_t_'):
            raise ValueError("Model must be fitted before prediction")
            
        T = self.transform(X)
        return self.p_y_given_t_[T]
    
    def predict(self, X):
        """Predict class labels."""
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)
    
    def fit_transform(self, X, y=None):
        """Fit model and return transformed data."""
        self.fit(X, y)
        return self.transform(X)

class EvaluationMixin:
    """
    Evaluation metrics and analysis for Information Bottleneck.
    
    Provides comprehensive evaluation including:
    - Information-theoretic measures (I(X;T), I(T;Y), etc.)
    - Classification performance when applicable
    - Compression-distortion trade-off analysis
    - Theoretical bounds verification
    """
    
    def compute_information_metrics(self, X, Y, T=None):
        """Compute all information-theoretic metrics."""
        if T is None:
            T = self.transform(X)
            
        metrics = {}
        
        # Core information measures
        metrics['I_XT'] = self._compute_mutual_information_discrete(X, T)
        metrics['I_TY'] = self._compute_mutual_information_discrete(T, Y)
        metrics['I_XY'] = self._compute_mutual_information_discrete(X, Y)
        
        # Entropies
        metrics['H_X'] = self._compute_entropy(X)
        metrics['H_T'] = self._compute_entropy(T)
        metrics['H_Y'] = self._compute_entropy(Y)
        
        # Information bottleneck objective
        if hasattr(self, 'beta'):
            metrics['IB_objective'] = metrics['I_TY'] - self.beta * metrics['I_XT']
            
        # Compression ratio
        metrics['compression_ratio'] = metrics['I_XT'] / metrics['I_XY'] if metrics['I_XY'] > 0 else 0
        
        # Information transmission efficiency 
        metrics['transmission_efficiency'] = metrics['I_TY'] / metrics['I_XT'] if metrics['I_XT'] > 0 else 0
        
        return metrics
    
    def evaluate_compression_distortion_tradeoff(self, X, Y, beta_values):
        """Analyze compression-distortion trade-off across Œ≤ values."""
        tradeoff_data = []
        
        for beta in beta_values:
            # Temporarily set beta and refit
            original_beta = getattr(self, 'beta', None)
            self.beta = beta
            self.fit(X, Y)
            
            # Compute metrics
            T = self.transform(X)
            metrics = self.compute_information_metrics(X, Y, T)
            
            tradeoff_data.append({
                'beta': beta,
                'compression': metrics['I_XT'],
                'relevance': metrics['I_TY'], 
                'objective': metrics['IB_objective']
            })
            
        # Restore original beta
        if original_beta is not None:
            self.beta = original_beta
            self.fit(X, Y)
            
        return tradeoff_data