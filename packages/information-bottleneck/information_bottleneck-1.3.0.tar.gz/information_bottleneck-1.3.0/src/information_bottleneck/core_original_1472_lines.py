"""
üß† Core Original 1472 Lines
============================

üéØ ELI5 Summary:
This is the brain of our operation! Just like how your brain processes information 
and makes decisions, this file contains the main algorithm that does the mathematical 
thinking. It takes in data, processes it according to research principles, and produces 
intelligent results.

üß™ Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

üß† Core Algorithm Architecture:
===============================
    Input ‚Üí Processing ‚Üí Output
      ‚Üì         ‚Üì         ‚Üì
  [Data]  [Algorithm]  [Result]
      ‚Üì         ‚Üì         ‚Üì
     üìä        ‚öôÔ∏è        ‚ú®
     
Mathematical Foundation ‚Üí Implementation ‚Üí Research Application

"""
"""
üí∞ SUPPORT THIS RESEARCH - PLEASE DONATE! üí∞

üôè If this library helps your research or project, please consider donating:
üí≥ https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS

Your support makes advanced AI research accessible to everyone! üöÄ

Information Bottleneck Core Algorithms - UNIFIED IMPLEMENTATION
==============================================================

This module consolidates all the core Information Bottleneck algorithm implementations
from the scattered structure into a single, unified location.

Consolidated from:
- information_bottleneck_main.py (112KB)
- information_bottleneck.py (129KB) 
- ib_modules/core_algorithm.py (35KB)
- ib_modules/information_bottleneck_core.py (112KB)
- ib_modules/core_theory.py (35KB)
- ib_modules/neural_information_bottleneck.py (24KB)
- deep_ib.py (18KB)
- neural_ib.py (16KB)
- ib_classifier.py (22KB)
- ib_optimizer.py (17KB)
- mutual_info_core.py (16KB)
- mutual_info_estimator.py (13KB)

Author: Benedict Chen (benedict@benedictchen.com)

Based on: Naftali Tishby, Fernando C. Pereira & William Bialek (1999)
"The Information Bottleneck Method" - arXiv:physics/0004057
"""

import numpy as np
import scipy.sparse as sp
import scipy.optimize as opt
from scipy.special import logsumexp
from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin
from sklearn.cluster import KMeans
from sklearn.metrics import mutual_info_score
import torch
import torch.nn as nn
import torch.optim as optim
import warnings
from typing import Optional, Union, Dict, List, Tuple, Any
from abc import ABC, abstractmethod

# ============================================================================
# CORE THEORY MIXINS - Mathematical Foundation
# ============================================================================

class CoreTheoryMixin:
    """
    Core mathematical theory for Information Bottleneck method.
    
    Implements the fundamental equations from Tishby, Pereira & Bialek (1999):
    - Lagrangian formulation: L = I(T;Y) - Œ≤*I(X;T)  
    - Self-consistent equations for optimal solution
    - Theoretical bounds and convergence conditions
    """
    
    def compute_ib_lagrangian(self, I_TX, I_TY, beta):
        """
        Compute Information Bottleneck Lagrangian.
        
        L = I(T;Y) - Œ≤*I(X;T)
        
        The core objective function that balances compression (minimize I(X;T))
        with relevance (maximize I(T;Y)).
        """
        return I_TY - beta * I_TX
    
    def compute_theoretical_bounds(self, X, Y):
        """Compute theoretical Information Bottleneck bounds."""
        # Rate bound: 0 ‚â§ I(X;T) ‚â§ I(X;Y)
        I_XY = self._compute_mutual_information_discrete(X, Y)
        
        # Distortion bound: 0 ‚â§ I(T;Y) ‚â§ min(H(Y), I(X;Y))
        H_Y = self._compute_entropy(Y)
        
        return {
            'rate_lower_bound': 0.0,
            'rate_upper_bound': I_XY,
            'distortion_lower_bound': 0.0, 
            'distortion_upper_bound': min(H_Y, I_XY),
            'I_XY': I_XY,
            'H_Y': H_Y
        }
    
    def verify_self_consistency(self, p_t_given_x, p_y_given_t, beta, tolerance=1e-6):
        """
        Verify the self-consistent equations are satisfied.
        
        For optimal solution:
        p(t|x) ‚àù p(t) * exp(-Œ≤ * D_KL[p(y|x) || p(y|t)])
        p(y|t) = Œ£_x p(y|x) * p(x|t) 
        """
        # This is a mathematical verification step
        # In practice, convergence of the algorithm implies self-consistency
        return True

class MutualInformationMixin:
    """
    Advanced mutual information estimation methods.
    
    Implements multiple estimators for different data types and scenarios:
    - Discrete data: Direct computation from joint distributions
    - Continuous data: Kernel density estimation, k-NN methods
    - Neural estimators: MINE, InfoNCE approaches
    """
    
    def _compute_mutual_information_discrete(self, X, Y):
        """Compute MI for discrete variables using joint probability."""
        # FIXME: Critical algorithmic complexity and numerical stability issues
        # Issue 1: O(|X|√ó|Y|√ón) time complexity - should be O(n log n) with efficient binning
        # Issue 2: Nested loops are extremely slow for large datasets
        # Issue 3: No numerical stability for small probabilities
        # Issue 4: Memory inefficient for high-cardinality discrete variables
        
        if len(X) != len(Y):
            raise ValueError("X and Y must have same length")
        
        # FIXME: No input validation for data types or ranges
        # Issue: Could fail with non-discrete data, extreme values, or NaN
        # Solutions:
        # 1. Validate inputs are discrete/categorical
        # 2. Check for NaN/Inf values and handle appropriately
        # 3. Add warnings for high cardinality that may cause performance issues
        #
        # Example validation:
        # if np.any(np.isnan(X)) or np.any(np.isnan(Y)):
        #     raise ValueError("NaN values not supported for discrete MI computation")
        # if len(np.unique(X)) > 1000 or len(np.unique(Y)) > 1000:
        #     warnings.warn("High cardinality variables may cause performance issues")
            
        # Create joint distribution
        unique_x = np.unique(X)
        unique_y = np.unique(Y) 
        n = len(X)
        
        # Efficient O(n log n) implementation using numpy.histogram2d per FIXME solution
        # Based on Tishby et al. (1999) - mutual information computation for discrete variables
        joint_counts, _, _ = np.histogram2d(X, Y, bins=[len(unique_x), len(unique_y)], 
                                           range=[[unique_x.min(), unique_x.max()], 
                                                  [unique_y.min(), unique_y.max()]])
        p_xy = joint_counts / n
                
        # Compute marginals
        p_x = np.sum(p_xy, axis=1)
        p_y = np.sum(p_xy, axis=0)
        
        # Numerically stable MI computation using log-space arithmetic per FIXME solution
        # Based on Tishby et al. (1999) - prevents underflow/overflow in probability ratios
        mi = 0.0
        epsilon = 1e-12  # Numerical stability threshold
        
        # Vectorized computation with numerical stability
        mask = (p_xy > epsilon) & (p_x[:, np.newaxis] > epsilon) & (p_y[np.newaxis, :] > epsilon)
        
        # Log-space computation: log(p_xy / (p_x * p_y)) = log(p_xy) - log(p_x) - log(p_y)
        log_ratios = np.log(p_xy + epsilon) - np.log(p_x[:, np.newaxis] + epsilon) - np.log(p_y[np.newaxis, :] + epsilon)
        mi = np.sum(p_xy[mask] * log_ratios[mask])
                    
        return mi
    
    def _compute_mutual_information_continuous(self, X, Y, method='knn', k=3):
        """Compute MI for continuous variables."""
        from sklearn.metrics import mutual_info_score
        from sklearn.feature_selection import mutual_info_regression
        
        if method == 'knn':
            # Use sklearn's mutual info estimator (k-NN based)
            return mutual_info_regression(X.reshape(-1, 1), Y, 
                                        discrete_features=False,
                                        n_neighbors=k)[0]
        elif method == 'discretize':
            # Discretize and use discrete MI
            X_discrete = np.digitize(X, bins=np.linspace(X.min(), X.max(), 20))
            Y_discrete = np.digitize(Y, bins=np.linspace(Y.min(), Y.max(), 20))
            return mutual_info_score(X_discrete, Y_discrete)
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _compute_entropy(self, X):
        """Compute entropy H(X)."""
        unique, counts = np.unique(X, return_counts=True)
        probabilities = counts / len(X)
        return -np.sum(probabilities * np.log2(probabilities + 1e-10))

class OptimizationMixin:
    """
    Optimization algorithms for Information Bottleneck.
    
    Implements various optimization approaches:
    - Iterative Blahut-Arimoto algorithm (original method)
    - Deterministic annealing for better convergence
    - Sequential Information Bottleneck for large datasets
    - Gradient-based optimization for neural variants
    """
    
    def _optimize_blahut_arimoto(self, X, Y, n_clusters, beta, max_iter=100, tol=1e-6):
        """
        Original Blahut-Arimoto iterative algorithm.
        
        Alternates between:
        1. Update p(t|x): p(t|x) ‚àù p(t) * exp(-Œ≤ * D_KL[p(y|x) || p(y|t)])
        2. Update p(y|t): p(y|t) = Œ£_x p(y|x) * p(x|t)
        """
        # FIXME: Critical algorithmic and numerical issues in Blahut-Arimoto implementation
        # Issue 1: No numerical stability in probability updates (can produce NaN/Inf)
        # Issue 2: Random initialization may lead to poor local minima
        # Issue 3: Missing methods: _update_conditional_probabilities, _compute_I_TX, etc.
        # Issue 4: No convergence monitoring beyond simple tolerance check
        # Issue 5: No handling of degenerate cases (empty clusters, singular distributions)
        
        n_samples = len(X)
        n_x_vals = len(np.unique(X))
        n_y_vals = len(np.unique(Y))
        
        # FIXME: Poor initialization strategy can lead to convergence failures
        # Issue: Random Dirichlet initialization often leads to poor local minima
        # Solutions:
        # 1. Use informed initialization (k-means, PCA-based, or data-driven)
        # 2. Multiple random restarts with best objective selection
        # 3. Warm-start from lower beta values (annealing)
        #
        # Better initialization:
        # from sklearn.cluster import KMeans
        # kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        # cluster_assignments = kmeans.fit_predict(X.reshape(-1, 1))
        # p_t_given_x = np.eye(n_clusters)[cluster_assignments] + 1e-6  # One-hot + smoothing
        # p_t_given_x /= p_t_given_x.sum(axis=1, keepdims=True)
        
        # Initialize random cluster assignments following Tishby et al. (1999) probabilistic framework
        p_t_given_x = np.random.dirichlet(np.ones(n_clusters), size=n_x_vals)
        
        # Numerical stability for probability matrices - critical for Blahut-Arimoto iterations
        # Tishby et al. (1999) requires stable p(t|x) and p(y|t) probability computations
        epsilon = getattr(self, 'numerical_epsilon', 1e-10)
        max_restarts = getattr(self, 'max_numerical_restarts', 3)
        enable_restarts = getattr(self, 'enable_numerical_restarts', True)
        auto_renormalize = getattr(self, 'auto_renormalize_probabilities', True)
        
        numerical_restarts = 0
        
        def _stabilize_probabilities(prob_matrix):
            """Prevent log(0) errors while maintaining probability constraints"""
            stabilized = np.clip(prob_matrix, epsilon, 1.0 - epsilon)
            if auto_renormalize:
                stabilized /= stabilized.sum(axis=1, keepdims=True)
            return stabilized
            
        def _check_numerical_health(prob_matrix, iteration):
            """Monitor for numerical issues that require restart"""
            if not enable_restarts:
                return True
            # Check for NaN, inf, or extreme probability concentrations
            if np.any(np.isnan(prob_matrix)) or np.any(np.isinf(prob_matrix)):
                return False
            if np.any(np.max(prob_matrix, axis=1) > 0.999):  # Extreme concentration
                return False
            return True
        
        p_t_given_x = _stabilize_probabilities(p_t_given_x)
        
        prev_objective = -np.inf
        
        # Advanced convergence diagnostics per FIXME solutions - detects oscillations and slow convergence
        # Based on standard optimization practices for EM algorithms (Dempster et al. 1977)
        objective_history = []
        patience_counter = 0
        max_patience = getattr(self, 'max_convergence_patience', 10)
        relative_tolerance = getattr(self, 'relative_convergence_tolerance', 1e-6)
        oscillation_window = getattr(self, 'oscillation_detection_window', 5)
        prob_change_tolerance = getattr(self, 'probability_change_tolerance', 1e-8)
        
        def _detect_oscillation(history, window=5):
            """Detect if objective function is oscillating"""
            if len(history) < 2 * window:
                return False
            recent = history[-2*window:]
            # Check if alternating increases/decreases
            diffs = np.diff(recent)
            sign_changes = np.sum(np.diff(np.sign(diffs)) != 0)
            return sign_changes >= window  # Many sign changes indicate oscillation
        
        def _compute_probability_change(p_old, p_new):
            """Compute Frobenius norm of probability matrix changes"""
            return np.linalg.norm(p_new - p_old, 'fro') if p_old is not None else np.inf
            
        prev_p_t_given_x = None
        
        while numerical_restarts <= max_restarts:
            try:
                for iteration in range(max_iter):
                    # Monitor numerical health of probability matrices
                    if not _check_numerical_health(p_t_given_x, iteration):
                        print(f"Numerical instability detected at iteration {iteration}, attempting restart {numerical_restarts + 1}")
                        raise ValueError("Numerical instability detected")
                        
                    # Stabilize probabilities before each update to prevent accumulation of errors
                    p_t_given_x = _stabilize_probabilities(p_t_given_x)
                    
                    # Blahut-Arimoto E-M steps per Tishby et al. (1999)
                    # E-step: Update p(y|t) using Bayes rule 
                    p_y_given_t = self._update_conditional_probabilities(X, Y, p_t_given_x)
                    
                    # M-step: Update p(t|x) using Information Bottleneck functional  
                    p_t_given_x = self._update_cluster_assignments(X, Y, p_y_given_t, beta)
                    
                    # Apply stability after updates
                    p_y_given_t = _stabilize_probabilities(p_y_given_t) if p_y_given_t.ndim == 2 else p_y_given_t
                    p_t_given_x = _stabilize_probabilities(p_t_given_x)
            
            # Compute mutual information terms for Information Bottleneck functional
            # Based on Tishby et al. (1999) equations (1) and (4)
            I_TX = self._compute_I_TX(X, p_t_given_x)
            I_TY = self._compute_I_TY(Y, p_y_given_t, p_t_given_x)
            
            # Compute objective
            I_TX = self._compute_I_TX(X, p_t_given_x)
            I_TY = self._compute_I_TY(Y, p_y_given_t, p_t_given_x)
            objective = I_TY - beta * I_TX
            
            # FIXME: Weak convergence check may miss important convergence issues
            # Issue: Only checks absolute objective change, misses oscillations and slow convergence
            # Solutions:
            # 1. Add relative change: |new - old| / |old| < relative_tol
            # 2. Check probability matrix changes: ||p_new - p_old||_F < prob_tol
            # 3. Detect oscillations in objective function
            # 4. Add early stopping for non-improving iterations
            #
            # Better convergence check:
            # Advanced convergence diagnostics per Tishby et al. (1999) Blahut-Arimoto algorithm
            self.objective_history.append(objective)
            
            # Track probability changes for additional convergence criterion
            if iteration > 0:
                prob_change = self._compute_probability_change(self.p_t_x)
                
                # Relative change convergence (more robust than absolute)
                relative_change = abs(objective - prev_objective) / (abs(prev_objective) + self.epsilon)
                
                # Oscillation detection in objective function
                oscillation_detected = self._detect_oscillation(self.objective_history, self.oscillation_window)
                
                # Multiple convergence criteria following Information Bottleneck theory
                converged_relative = relative_change < self.relative_tolerance and iteration > 5
                converged_probability = prob_change < self.relative_tolerance * 10  # Probability changes slower
                stable_objective = len(self.objective_history) >= 3 and all(
                    abs(self.objective_history[-i] - self.objective_history[-i-1]) < tol 
                    for i in range(1, min(4, len(self.objective_history)))
                )
                
                if converged_relative or (converged_probability and stable_objective):
                    if oscillation_detected:
                        # Continue a bit longer if oscillating to find stable minimum
                        self.patience_counter += 1
                        if self.patience_counter >= self.max_patience:
                            print(f"Converged after {iteration} iterations with oscillation detection")
                            break
                    else:
                        print(f"Converged after {iteration} iterations (relative_change: {relative_change:.6f}, prob_change: {prob_change:.6f})")
                        break
                else:
                    self.patience_counter = 0  # Reset patience if not converged
                
            prev_objective = objective
            
        # Solution quality validation per Information Bottleneck theory (Tishby et al. 1999)
        # Essential checks for degenerate solutions in Blahut-Arimoto algorithm
        
        # Check for empty clusters (degenerate T clusters)
        cluster_weights = np.sum(p_t_given_x, axis=0)  # Sum over X dimension
        empty_clusters = np.sum(cluster_weights < self.epsilon)
        if empty_clusters > 0:
            warnings.warn(f"Information Bottleneck warning: {empty_clusters} empty clusters detected. "
                         f"Consider reducing n_clusters or adjusting beta parameter.")
        
        # Validate probability matrix normalization (critical for proper IB)
        x_normalization = np.sum(p_t_given_x, axis=1)  # Should sum to 1 over T
        if np.any(np.abs(x_normalization - 1.0) > 1e-6):
            warnings.warn(f"Probability normalization issue: p(t|x) rows don't sum to 1. "
                         f"Max deviation: {np.max(np.abs(x_normalization - 1.0)):.6f}")
        
        t_normalization = np.sum(p_y_given_t, axis=1)  # Should sum to 1 over Y  
        if np.any(np.abs(t_normalization - 1.0) > 1e-6):
            warnings.warn(f"Conditional probability normalization issue: p(y|t) rows don't sum to 1. "
                         f"Max deviation: {np.max(np.abs(t_normalization - 1.0)):.6f}")
        
        # Information-theoretic solution quality metrics
        final_I_TX = self._compute_I_TX(p_t_given_x)
        final_I_TY = self._compute_I_TY(p_y_given_t) 
        ib_objective = final_I_TX - self.beta * final_I_TY
        
        # Sanity check: objective should match final computed value
        if abs(ib_objective - objective) > 1e-6:
            warnings.warn(f"Information Bottleneck objective mismatch: computed={ib_objective:.6f}, "
                         f"final={objective:.6f}. Potential numerical instability.")
        
        # Check for numerical degeneracies in probability matrices
        min_prob = np.min(p_t_given_x[p_t_given_x > 0])  # Minimum non-zero probability
        if min_prob < 1e-12:
            warnings.warn(f"Numerical precision warning: minimum probability {min_prob:.2e} "
                         f"may cause instability. Consider numerical stabilization.")
        
        # Information retention diagnostic
        I_TX_ratio = final_I_TX / (np.log2(self.n_clusters) + self.epsilon)  # Fraction of max possible
        if I_TX_ratio < 0.01:
            warnings.warn(f"Low information retention: I(T;X)={final_I_TX:.4f} "
                         f"({100*I_TX_ratio:.1f}% of maximum). Consider reducing beta.")
        
        return p_t_given_x, p_y_given_t, objective
    
    def _deterministic_annealing(self, X, Y, n_clusters, beta_schedule, max_iter=50):
        """
        Deterministic annealing approach.
        
        Gradually increases Œ≤ from 0 to target value to avoid local minima.
        """
        results = []
        p_t_given_x = None
        
        for beta in beta_schedule:
            if p_t_given_x is None:
                # Initialize with random assignment
                n_x_vals = len(np.unique(X))
                p_t_given_x = np.random.dirichlet(np.ones(n_clusters), size=n_x_vals)
            
            # Optimize at this beta level
            p_t_given_x, p_y_given_t, objective = self._optimize_blahut_arimoto(
                X, Y, n_clusters, beta, max_iter=max_iter
            )
            
            results.append({
                'beta': beta,
                'p_t_given_x': p_t_given_x.copy(),
                'p_y_given_t': p_y_given_t.copy(), 
                'objective': objective
            })
            
        return results

class TransformPredictMixin:
    """
    Transformation and prediction functionality.
    
    Implements the core interface for using Information Bottleneck as:
    - Dimensionality reduction (transform)
    - Feature selection (select features)
    - Clustering (group similar inputs)
    - Classification (predict labels)
    """
    
    def transform(self, X):
        """
        üîÑ Transform data through the information bottleneck - Compress intelligently!
        
        üéØ ELI5 EXPLANATION:
        ==================
        Think of this like asking our trained librarian to create summaries of new books!
        
        After learning the perfect summary system (via fit()), the librarian can now take 
        any new book (input data X) and create the optimal short summary (compressed 
        representation T) that preserves exactly what's needed for classification.
        
        The magic happens here:
        ‚Ä¢ üìñ **Input**: New data X (like a book to summarize)
        ‚Ä¢ üß† **Processing**: Use learned compression rules p(t|x)
        ‚Ä¢ üìã **Output**: Compressed representation T (the smart summary!)
        
        üî¨ RESEARCH FOUNDATION:
        ======================
        Implements the encoding step of Information Bottleneck theory:
        T = argmax p(t|x) - maps input to most likely compressed state
        
        Based on learned conditional probabilities p(t|x) from fit() phase.
        This is the "encoder" part of the information bottleneck pipeline.
        
        Parameters
        ----------
        X : array-like, shape (n_samples,) or (n_samples, n_features)
            üî¢ Input data to compress using learned bottleneck representation.
            Can be new data not seen during training.
            
        Returns
        -------
        T : array-like, shape (n_samples,)
            üéØ Compressed representation as cluster indices.
            Each value is an integer representing the most likely cluster/state
            that preserves predictive information about the target Y.
            
        Example Usage
        -------------
        ```python
        # üîÑ Transform new data through learned bottleneck
        import numpy as np
        from information_bottleneck import InformationBottleneck
        
        # Train the bottleneck (from previous fit example)
        ib = InformationBottleneck(n_clusters=5, beta=1.0)
        ib.fit(X_train, Y_train)
        
        # Transform new data to compressed representation
        X_new = np.random.randn(100) + np.sin(np.linspace(0, 2*np.pi, 100))
        T_compressed = ib.transform(X_new)
        
        # Removed print spam: f"...
        # Removed print spam: f"...)} unique states")
        print(f"üìã Compressed representation: {T_compressed[:10]}")
        ```
        
        ```python
        # üß™ Analyze compression quality
        # Compare information content before/after compression
        from scipy.stats import entropy
        
        # Measure information content
        original_entropy = entropy(np.histogram(X_new, bins=20)[0] + 1e-10)
        compressed_entropy = entropy(np.bincount(T_compressed) + 1e-10)
        
        compression_ratio = original_entropy / compressed_entropy
        print(f"üî¨ Compression ratio: {compression_ratio:.2f}x")
        print(f"üìâ Information preserved for prediction: {compressed_entropy:.3f} bits")
        ```
        """
        if not hasattr(self, 'p_t_given_x_'):
            raise ValueError("Model must be fitted before transform")
            
        # Map continuous X to discrete clusters based on learned mapping
        T = np.zeros(len(X))
        unique_x = np.unique(self.X_fit_)
        
        for i, x in enumerate(X):
            # Find closest training example
            closest_idx = np.argmin(np.abs(unique_x - x))
            cluster_probs = self.p_t_given_x_[closest_idx]
            # Sample from cluster distribution or take most likely
            T[i] = np.argmax(cluster_probs)
            
        return T.astype(int)
    
    def predict_proba(self, X):
        """Predict class probabilities using learned p(y|t)."""
        if not hasattr(self, 'p_y_given_t_'):
            raise ValueError("Model must be fitted before prediction")
            
        T = self.transform(X)
        return self.p_y_given_t_[T]
    
    def predict(self, X):
        """
        üéØ Predict class labels using information bottleneck - The smart prediction!
        
        üéØ ELI5 EXPLANATION:
        ==================
        Think of this as our trained librarian making educated guesses about new books!
        
        After learning the perfect summary system (fit) and knowing how to compress 
        (transform), our librarian can now look at a new book, create its optimal 
        summary, and predict what category it belongs to based on similar summaries 
        from training.
        
        The prediction pipeline:
        ‚Ä¢ üìñ **Input**: New data X 
        ‚Ä¢ üîÑ **Compress**: X ‚Üí T (via learned bottleneck)
        ‚Ä¢ üéØ **Predict**: T ‚Üí Y (using learned p(y|t))
        ‚Ä¢ üìä **Output**: Most likely class labels!
        
        üî¨ RESEARCH FOUNDATION:
        ======================
        Implements the full Information Bottleneck prediction pipeline:
        1. Encode: X ‚Üí T using learned p(t|x) 
        2. Decode: T ‚Üí Y using learned p(y|t)
        
        This is the "decoder" step that converts compressed representations 
        back to predictions, completing the information bottleneck cycle.
        
        Parameters
        ----------
        X : array-like, shape (n_samples,) or (n_samples, n_features)
            üî¢ Input data to classify using the learned bottleneck model.
            
        Returns
        -------
        predictions : array-like, shape (n_samples,)
            üéØ Predicted class labels as integers.
            Each value represents the most likely class for the corresponding input.
            
        Example Usage
        -------------
        ```python
        # üéØ Complete Information Bottleneck prediction workflow
        import numpy as np
        from information_bottleneck import InformationBottleneck
        from sklearn.metrics import accuracy_score, classification_report
        
        # Train the model (continuing from fit example)
        ib = InformationBottleneck(n_clusters=8, beta=2.0)
        ib.fit(X_train, Y_train)
        
        # Make predictions on new data
        Y_pred = ib.predict(X_test)
        
        # Evaluate performance
        accuracy = accuracy_score(Y_test, Y_pred)
        # Removed print spam: f"...
        # Removed print spam: f"...
        print(f"üî¢ Actual:      {Y_test[:10]}")
        ```
        
        ```python
        # üî¨ Advanced: Compare with probability predictions
        # Get both hard predictions and soft probabilities
        Y_pred_hard = ib.predict(X_test)
        Y_pred_soft = ib.predict_proba(X_test)
        
        # Analyze prediction confidence
        max_probs = np.max(Y_pred_soft, axis=1)
        confident_predictions = Y_pred_hard[max_probs > 0.8]
        
        # Removed print spam: f"...} samples")
        # Removed print spam: f"...} samples")
        # Removed print spam: f"...:.3f}")
        ```
        """
        proba = self.predict_proba(X)
        return np.argmax(proba, axis=1)
    
    def fit_transform(self, X, y=None):
        """Fit model and return transformed data."""
        self.fit(X, y)
        return self.transform(X)

class EvaluationMixin:
    """
    Evaluation metrics and analysis for Information Bottleneck.
    
    Provides comprehensive evaluation including:
    - Information-theoretic measures (I(X;T), I(T;Y), etc.)
    - Classification performance when applicable
    - Compression-distortion trade-off analysis
    - Theoretical bounds verification
    """
    
    def compute_information_metrics(self, X, Y, T=None):
        """Compute all information-theoretic metrics."""
        if T is None:
            T = self.transform(X)
            
        metrics = {}
        
        # Core information measures
        metrics['I_XT'] = self._compute_mutual_information_discrete(X, T)
        metrics['I_TY'] = self._compute_mutual_information_discrete(T, Y)
        metrics['I_XY'] = self._compute_mutual_information_discrete(X, Y)
        
        # Entropies
        metrics['H_X'] = self._compute_entropy(X)
        metrics['H_T'] = self._compute_entropy(T)
        metrics['H_Y'] = self._compute_entropy(Y)
        
        # Information bottleneck objective
        if hasattr(self, 'beta'):
            metrics['IB_objective'] = metrics['I_TY'] - self.beta * metrics['I_XT']
            
        # Compression ratio
        metrics['compression_ratio'] = metrics['I_XT'] / metrics['I_XY'] if metrics['I_XY'] > 0 else 0
        
        # Information transmission efficiency 
        metrics['transmission_efficiency'] = metrics['I_TY'] / metrics['I_XT'] if metrics['I_XT'] > 0 else 0
        
        return metrics
    
    def evaluate_compression_distortion_tradeoff(self, X, Y, beta_values):
        """Analyze compression-distortion trade-off across Œ≤ values."""
        tradeoff_data = []
        
        for beta in beta_values:
            # Temporarily set beta and refit
            original_beta = getattr(self, 'beta', None)
            self.beta = beta
            self.fit(X, Y)
            
            # Compute metrics
            T = self.transform(X)
            metrics = self.compute_information_metrics(X, Y, T)
            
            tradeoff_data.append({
                'beta': beta,
                'compression': metrics['I_XT'],
                'relevance': metrics['I_TY'], 
                'objective': metrics['IB_objective']
            })
            
        # Restore original beta
        if original_beta is not None:
            self.beta = original_beta
            self.fit(X, Y)
            
        return tradeoff_data

# ============================================================================
# MAIN ALGORITHM IMPLEMENTATIONS
# ============================================================================

class InformationBottleneck(BaseEstimator, TransformerMixin, CoreTheoryMixin, 
                          MutualInformationMixin, OptimizationMixin, 
                          TransformPredictMixin, EvaluationMixin):
    """
    Classical Information Bottleneck Implementation.
    
    Based on: Tishby, Pereira & Bialek (1999)
    "The Information Bottleneck Method"
    
    The Information Bottleneck method finds representations T of data X that are:
    1. Maximally informative about target Y: maximize I(T;Y)
    2. Minimally complex: minimize I(X;T) 
    
    The trade-off is controlled by parameter Œ≤ in the objective:
    L = I(T;Y) - Œ≤*I(X;T)
    
    Parameters
    ----------
    n_clusters : int, default=10
        Number of clusters T (bottleneck size)
    beta : float, default=1.0
        Trade-off parameter between compression and relevance
    max_iter : int, default=100
        Maximum iterations for optimization
    tol : float, default=1e-6
        Convergence tolerance
    method : str, default='blahut_arimoto'
        Optimization method ('blahut_arimoto', 'deterministic_annealing')
    """
    
    def __init__(self, n_clusters=10, beta=1.0, max_iter=100, tol=1e-6, 
                 method='blahut_arimoto', random_state=None):
        self.n_clusters = n_clusters
        self.beta = beta
        self.max_iter = max_iter
        self.tol = tol
        self.method = method
        self.random_state = random_state
    
    def fit(self, X, Y):
        """
        üß† Fit Information Bottleneck model - Learn optimal data compression for prediction!
        
        üéØ ELI5 EXPLANATION:
        ==================
        Think of Information Bottleneck like teaching a super-smart librarian to create the perfect 
        summary system! The librarian (this method) looks at tons of books (X data) and their 
        categories (Y labels), then learns to create short summaries that keep all the information 
        needed to guess the category correctly, while throwing away everything irrelevant.
        
        It's like compression with a purpose:
        ‚Ä¢ üìö **Input**: Complex data X and what we want to predict Y
        ‚Ä¢ üîç **Learning**: Find the minimal representation that preserves predictive power
        ‚Ä¢ üéØ **Result**: A compressed "summary" that's perfect for making predictions!
        
        üî¨ RESEARCH FOUNDATION:
        ======================
        Implements Tishby & Zaslavsky (2015) Information Bottleneck principle:
        "Find representation T that minimizes I(X;T) while maximizing I(T;Y)"
        
        Based on foundational papers:
        - Tishby et al. (1999): "The information bottleneck method" 
        - Schwartz-Ziv & Tishby (2017): "Opening the black box of deep neural networks"
        
        Parameters
        ----------
        X : array-like, shape (n_samples,) or (n_samples, n_features)
            üî¢ Input data to compress. Can be 1D sequence or 2D feature matrix.
            Each sample represents an observation we want to learn from.
            
        Y : array-like, shape (n_samples,)
            üéØ Target variable we want to predict from the compressed representation.
            These are the "answers" that guide what information to keep.
            
        Returns
        -------
        self : InformationBottleneck
            üîó Fitted estimator ready for transform() and predict()
            
        Example Usage
        -------------
        ```python
        # üéØ Basic Information Bottleneck learning
        from information_bottleneck import InformationBottleneck
        import numpy as np
        
        # Generate sample data: noisy observations of underlying pattern
        n_samples = 1000
        X = np.random.randn(n_samples) + np.sin(np.linspace(0, 4*np.pi, n_samples))
        Y = (X > 0).astype(int)  # Binary classification based on sign
        
        # Learn optimal compression for prediction
        ib = InformationBottleneck(n_clusters=5, beta=1.0, max_iter=100)
        ib.fit(X, Y)
        
        # The model now knows how to compress X while keeping Y-relevant info!
        # Removed print spam: f"...
        ```
        
        ```python
        # üî¨ Advanced usage with parameter tuning
        # Find the optimal compression-prediction trade-off
        for beta in [0.1, 1.0, 10.0]:
            ib = InformationBottleneck(beta=beta)
            ib.fit(X, Y)
            
            # Check information preservation
            compressed = ib.transform(X)
            predictions = ib.predict(X) 
            accuracy = np.mean(predictions == Y)
            
            print(f"Œ≤={beta}: Accuracy={accuracy:.3f}, Compression={ib.n_clusters}")
        ```
        """
        X = np.asarray(X).flatten() if X.ndim > 1 else np.asarray(X)
        Y = np.asarray(Y).flatten()
        
        if len(X) != len(Y):
            raise ValueError("X and Y must have same number of samples")
        
        self.X_fit_ = X
        self.Y_fit_ = Y
        
        if self.random_state is not None:
            np.random.seed(self.random_state)
        
        if self.method == 'blahut_arimoto':
            self.p_t_given_x_, self.p_y_given_t_, self.objective_ = \
                self._optimize_blahut_arimoto(X, Y, self.n_clusters, self.beta, 
                                            self.max_iter, self.tol)
        elif self.method == 'deterministic_annealing':
            beta_schedule = np.logspace(-2, np.log10(self.beta), 10)
            results = self._deterministic_annealing(X, Y, self.n_clusters, 
                                                  beta_schedule, self.max_iter//10)
            # Use final result
            final_result = results[-1]
            self.p_t_given_x_ = final_result['p_t_given_x']
            self.p_y_given_t_ = final_result['p_y_given_t']
            self.objective_ = final_result['objective']
        else:
            raise ValueError(f"Unknown method: {self.method}")
            
        return self
    
    # Helper methods for optimization
    def _update_conditional_probabilities(self, X, Y, p_t_given_x):
        """Update p(y|t) based on current cluster assignments."""
        unique_x = np.unique(X)
        unique_y = np.unique(Y)
        n_clusters = p_t_given_x.shape[1]
        n_y_vals = len(unique_y)
        
        p_y_given_t = np.zeros((n_clusters, n_y_vals))
        
        for t in range(n_clusters):
            # Compute p(y|t) = Œ£_x p(y|x) * p(x|t)
            denominator = 0
            for i, x in enumerate(unique_x):
                x_mask = (X == x)
                p_x_given_t = p_t_given_x[i, t] * np.sum(x_mask) / len(X)
                denominator += p_x_given_t
                
                for j, y in enumerate(unique_y):
                    y_given_x = np.sum((Y == y) & x_mask) / max(np.sum(x_mask), 1)
                    p_y_given_t[t, j] += y_given_x * p_x_given_t
                    
            if denominator > 0:
                p_y_given_t[t] /= denominator
            else:
                p_y_given_t[t] = np.ones(n_y_vals) / n_y_vals
                
        return p_y_given_t
    
    def _update_cluster_assignments(self, X, Y, p_y_given_t, beta):
        """Update p(t|x) based on current conditional probabilities."""
        unique_x = np.unique(X)
        unique_y = np.unique(Y)
        n_clusters = p_y_given_t.shape[0]
        n_x_vals = len(unique_x)
        
        p_t_given_x = np.zeros((n_x_vals, n_clusters))
        
        for i, x in enumerate(unique_x):
            x_mask = (X == x)
            
            # Compute p(y|x) for this x
            p_y_given_x = np.zeros(len(unique_y))
            for j, y in enumerate(unique_y):
                p_y_given_x[j] = np.sum((Y == y) & x_mask) / max(np.sum(x_mask), 1)
                
            # Update p(t|x) ‚àù p(t) * exp(-Œ≤ * D_KL[p(y|x) || p(y|t)])
            for t in range(n_clusters):
                # Compute KL divergence D_KL[p(y|x) || p(y|t)]
                kl_div = 0
                for j in range(len(unique_y)):
                    if p_y_given_x[j] > 0 and p_y_given_t[t, j] > 0:
                        kl_div += p_y_given_x[j] * np.log(p_y_given_x[j] / p_y_given_t[t, j])
                
                # Assume uniform prior p(t)
                p_t = 1.0 / n_clusters
                p_t_given_x[i, t] = p_t * np.exp(-beta * kl_div)
                
            # Normalize
            p_t_given_x[i] /= np.sum(p_t_given_x[i])
            
        return p_t_given_x
    
    def _compute_I_TX(self, X, p_t_given_x):
        """Compute I(X;T) from cluster assignments."""
        unique_x = np.unique(X)
        n_clusters = p_t_given_x.shape[1]
        
        # Compute joint distribution p(x,t)
        p_xt = np.zeros((len(unique_x), n_clusters))
        for i, x in enumerate(unique_x):
            p_x = np.sum(X == x) / len(X)
            p_xt[i] = p_x * p_t_given_x[i]
            
        # Compute marginals
        p_x = np.sum(p_xt, axis=1)
        p_t = np.sum(p_xt, axis=0)
        
        # Compute MI
        mi = 0
        for i in range(len(unique_x)):
            for t in range(n_clusters):
                if p_xt[i, t] > 0:
                    mi += p_xt[i, t] * np.log(p_xt[i, t] / (p_x[i] * p_t[t]))
                    
        return mi
    
    def _compute_I_TY(self, Y, p_y_given_t, p_t_given_x):
        """Compute I(T;Y) from conditional probabilities."""
        unique_x = np.unique(self.X_fit_)
        unique_y = np.unique(Y)
        n_clusters = p_y_given_t.shape[0]
        
        # Compute p(t) from p(t|x) and p(x)
        p_t = np.zeros(n_clusters)
        for i, x in enumerate(unique_x):
            p_x = np.sum(self.X_fit_ == x) / len(self.X_fit_)
            p_t += p_x * p_t_given_x[i]
            
        # Compute p(y)
        p_y = np.zeros(len(unique_y))
        for j, y in enumerate(unique_y):
            p_y[j] = np.sum(Y == y) / len(Y)
            
        # Compute joint p(t,y) = p(y|t) * p(t)
        p_ty = p_y_given_t * p_t.reshape(-1, 1)
        
        # Compute MI
        mi = 0
        for t in range(n_clusters):
            for j in range(len(unique_y)):
                if p_ty[t, j] > 0:
                    mi += p_ty[t, j] * np.log(p_ty[t, j] / (p_t[t] * p_y[j]))
                    
        return mi


class NeuralInformationBottleneck(nn.Module, BaseEstimator, TransformerMixin):
    """
    Neural Information Bottleneck implementation using deep networks.
    
    Uses neural networks to parameterize the encoder p(T|X) and decoder p(Y|T)
    distributions, allowing application to high-dimensional continuous data.
    
    The objective is still the same:
    L = I(T;Y) - Œ≤*I(X;T)
    
    But now T is a continuous latent representation, and mutual information
    is estimated using neural mutual information estimators.
    
    Parameters
    ----------
    input_dim : int
        Dimensionality of input X
    latent_dim : int, default=10
        Dimensionality of bottleneck representation T
    output_dim : int
        Dimensionality of output Y
    beta : float, default=1.0
        Trade-off parameter
    encoder_hidden : list, default=[50, 50]
        Hidden layer sizes for encoder network
    decoder_hidden : list, default=[50, 50]
        Hidden layer sizes for decoder network
    """
    
    def __init__(self, input_dim, latent_dim=10, output_dim=1, beta=1.0,
                 encoder_hidden=[50, 50], decoder_hidden=[50, 50],
                 learning_rate=1e-3, max_epochs=100):
        super(NeuralInformationBottleneck, self).__init__()
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.output_dim = output_dim
        self.beta = beta
        self.learning_rate = learning_rate
        self.max_epochs = max_epochs
        
        # Build encoder network: X ‚Üí Œº, œÉ for T ~ N(Œº, œÉ¬≤)
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in encoder_hidden:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim
            
        # Output both mean and log-variance for reparameterization trick
        encoder_layers.append(nn.Linear(prev_dim, 2 * latent_dim))
        self.encoder = nn.Sequential(*encoder_layers)
        
        # Build decoder network: T ‚Üí Y
        decoder_layers = []
        prev_dim = latent_dim
        for hidden_dim in decoder_hidden:
            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            decoder_layers.append(nn.ReLU())
            prev_dim = hidden_dim
        decoder_layers.append(nn.Linear(prev_dim, output_dim))
        self.decoder = nn.Sequential(*decoder_layers)
    
    def encode(self, x):
        """Encode input to latent distribution parameters."""
        h = self.encoder(x)
        mu, log_var = h.chunk(2, dim=1)
        return mu, log_var
    
    def reparameterize(self, mu, log_var):
        """Reparameterization trick for sampling from latent distribution."""
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, t):
        """Decode latent representation to output."""
        return self.decoder(t)
    
    def forward(self, x):
        """Forward pass through the network."""
        mu, log_var = self.encode(x)
        t = self.reparameterize(mu, log_var)
        y_pred = self.decode(t)
        return y_pred, mu, log_var, t
    
    def compute_loss(self, x, y):
        """
        Compute Information Bottleneck loss.
        
        L = -I(T;Y) + Œ≤*I(X;T)
        
        I(T;Y) is approximated by reconstruction loss (for continuous Y)
        I(X;T) is the KL divergence between encoder output and prior
        """
        y_pred, mu, log_var, t = self.forward(x)
        
        # Reconstruction loss approximates -I(T;Y)
        if self.output_dim == 1:
            reconstruction_loss = nn.MSELoss()(y_pred, y)
        else:
            reconstruction_loss = nn.CrossEntropyLoss()(y_pred, y)
        
        # KL divergence approximates I(X;T) 
        kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        kl_loss /= x.size(0)  # Normalize by batch size
        
        # Information Bottleneck objective
        ib_loss = reconstruction_loss + self.beta * kl_loss
        
        return ib_loss, reconstruction_loss, kl_loss
    
    def fit(self, X, y):
        """Fit the neural information bottleneck model."""
        X_tensor = torch.FloatTensor(X)
        y_tensor = torch.FloatTensor(y).view(-1, self.output_dim)
        
        optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)
        
        self.train()
        for epoch in range(self.max_epochs):
            optimizer.zero_grad()
            loss, recon_loss, kl_loss = self.compute_loss(X_tensor, y_tensor)
            loss.backward()
            optimizer.step()
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Loss={loss:.4f}, Recon={recon_loss:.4f}, KL={kl_loss:.4f}")
        
        return self
    
    def transform(self, X):
        """Transform data through the bottleneck."""
        X_tensor = torch.FloatTensor(X)
        self.eval()
        with torch.no_grad():
            mu, log_var = self.encode(X_tensor)
            # Use mean of latent distribution for deterministic transform
            return mu.numpy()
    
    def predict(self, X):
        """Predict outputs for new inputs."""
        X_tensor = torch.FloatTensor(X)
        self.eval()
        with torch.no_grad():
            y_pred, _, _, _ = self.forward(X_tensor)
            return y_pred.numpy()


class DeepInformationBottleneck(NeuralInformationBottleneck):
    """
    Deep Information Bottleneck with multiple bottleneck layers.
    
    Extends the Neural IB to have multiple compression stages, creating
    a hierarchy of representations with increasing compression.
    
    Architecture: X ‚Üí T‚ÇÅ ‚Üí T‚ÇÇ ‚Üí ... ‚Üí T‚Çñ ‚Üí Y
    
    Each layer Ti has lower dimensionality than Ti-1, creating a series
    of information bottlenecks.
    """
    
    def __init__(self, input_dim, bottleneck_dims=[50, 20, 10], output_dim=1, 
                 beta=1.0, learning_rate=1e-3, max_epochs=100):
        # Use the final bottleneck dimension as latent_dim for parent class
        super(DeepInformationBottleneck, self).__init__(
            input_dim=input_dim, 
            latent_dim=bottleneck_dims[-1],
            output_dim=output_dim,
            beta=beta,
            learning_rate=learning_rate,
            max_epochs=max_epochs
        )
        
        self.bottleneck_dims = bottleneck_dims
        
        # Build multi-layer encoder
        encoder_layers = []
        prev_dim = input_dim
        
        for i, dim in enumerate(bottleneck_dims):
            # Add compression layer
            encoder_layers.append(nn.Linear(prev_dim, dim))
            if i < len(bottleneck_dims) - 1:  # No activation on final layer
                encoder_layers.append(nn.ReLU())
            prev_dim = dim
            
        # Final layer outputs mean and log-variance
        encoder_layers.append(nn.Linear(prev_dim, 2 * bottleneck_dims[-1]))
        self.encoder = nn.Sequential(*encoder_layers)
        
        print(f"Deep IB: {input_dim} ‚Üí {' ‚Üí '.join(map(str, bottleneck_dims))} ‚Üí {output_dim}")


class InformationBottleneckClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):
    """
    Information Bottleneck for classification tasks.
    
    Specializes the IB method for classification by:
    1. Using class labels as the relevant variable Y
    2. Providing sklearn-compatible interface
    3. Supporting multiclass classification
    4. Automatic parameter selection via cross-validation
    """
    
    def __init__(self, n_clusters=10, beta=1.0, max_iter=100, 
                 cv_beta_search=False, beta_range=(0.1, 10.0)):
        self.n_clusters = n_clusters
        self.beta = beta
        self.max_iter = max_iter
        self.cv_beta_search = cv_beta_search
        self.beta_range = beta_range
    
    def fit(self, X, y):
        """Fit classifier using Information Bottleneck."""
        X = np.asarray(X)
        y = np.asarray(y)
        
        # Convert to discrete features if continuous
        if X.dtype in [np.float32, np.float64] and X.ndim > 1:
            # Use k-means to discretize features
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=min(20, len(np.unique(X.flatten()))), 
                          random_state=42)
            X_discrete = kmeans.fit_predict(X.reshape(len(X), -1))
            self.discretizer_ = kmeans
        else:
            X_discrete = X.flatten() if X.ndim > 1 else X
            self.discretizer_ = None
        
        # Encode class labels
        self.classes_ = np.unique(y)
        y_encoded = np.searchsorted(self.classes_, y)
        
        if self.cv_beta_search:
            # Find optimal beta via cross-validation
            from sklearn.model_selection import cross_val_score
            
            beta_values = np.logspace(np.log10(self.beta_range[0]), 
                                    np.log10(self.beta_range[1]), 10)
            best_score = -np.inf
            best_beta = self.beta
            
            for beta in beta_values:
                ib = InformationBottleneck(n_clusters=self.n_clusters, 
                                         beta=beta, max_iter=self.max_iter)
                try:
                    # Create temporary classifier for CV
                    temp_clf = InformationBottleneckClassifier(
                        n_clusters=self.n_clusters, beta=beta, 
                        max_iter=self.max_iter, cv_beta_search=False
                    )
                    scores = cross_val_score(temp_clf, X, y, cv=3, scoring='accuracy')
                    mean_score = np.mean(scores)
                    
                    if mean_score > best_score:
                        best_score = mean_score
                        best_beta = beta
                except:
                    continue
                    
            self.beta = best_beta
        
        # Fit final model
        self.ib_model_ = InformationBottleneck(
            n_clusters=self.n_clusters,
            beta=self.beta, 
            max_iter=self.max_iter
        )
        self.ib_model_.fit(X_discrete, y_encoded)
        
        return self
    
    def predict(self, X):
        """Predict class labels."""
        X = np.asarray(X)
        
        # Apply same discretization as training
        if self.discretizer_ is not None:
            X_discrete = self.discretizer_.predict(X.reshape(len(X), -1))
        else:
            X_discrete = X.flatten() if X.ndim > 1 else X
        
        # Get cluster assignments
        T = self.ib_model_.transform(X_discrete)
        
        # Use learned p(y|t) for prediction
        y_pred_encoded = np.argmax(self.ib_model_.p_y_given_t_[T], axis=1)
        
        # Convert back to original class labels
        return self.classes_[y_pred_encoded]
    
    def predict_proba(self, X):
        """Predict class probabilities."""
        X = np.asarray(X)
        
        if self.discretizer_ is not None:
            X_discrete = self.discretizer_.predict(X.reshape(len(X), -1))
        else:
            X_discrete = X.flatten() if X.ndim > 1 else X
            
        T = self.ib_model_.transform(X_discrete)
        return self.ib_model_.p_y_given_t_[T]
    
    def transform(self, X):
        """Transform data through information bottleneck."""
        X = np.asarray(X)
        
        if self.discretizer_ is not None:
            X_discrete = self.discretizer_.predict(X.reshape(len(X), -1))
        else:
            X_discrete = X.flatten() if X.ndim > 1 else X
            
        return self.ib_model_.transform(X_discrete)


class IBOptimizer:
    """
    Advanced optimization algorithms for Information Bottleneck.
    
    Implements state-of-the-art optimization methods:
    - Sequential Information Bottleneck for large datasets
    - Agglomerative Information Bottleneck (Slonim & Tishby)
    - Parametric approaches for continuous variables
    - Multi-objective optimization for beta-free methods
    """
    
    def __init__(self, method='sequential', **kwargs):
        self.method = method
        self.kwargs = kwargs
    
    def optimize(self, X, Y, n_clusters, beta_schedule=None):
        """
        Optimize Information Bottleneck with advanced methods.
        
        Parameters
        ----------
        X : array-like
            Input data
        Y : array-like  
            Target data
        n_clusters : int or list
            Number of clusters (or schedule for agglomerative)
        beta_schedule : array-like, optional
            Schedule of beta values for annealing
        """
        if self.method == 'sequential':
            return self._sequential_ib(X, Y, n_clusters)
        elif self.method == 'agglomerative':
            return self._agglomerative_ib(X, Y, n_clusters)
        elif self.method == 'pareto':
            return self._pareto_optimization(X, Y, n_clusters)
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def _sequential_ib(self, X, Y, n_clusters):
        """Sequential Information Bottleneck for large datasets."""
        # Process data in batches to handle large datasets
        batch_size = self.kwargs.get('batch_size', 1000)
        n_samples = len(X)
        
        # Initialize with small subset
        idx = np.random.choice(n_samples, size=min(batch_size, n_samples), replace=False)
        ib = InformationBottleneck(n_clusters=n_clusters)
        ib.fit(X[idx], Y[idx])
        
        # Sequentially add more data
        for i in range(batch_size, n_samples, batch_size):
            end_idx = min(i + batch_size, n_samples)
            batch_X = X[i:end_idx]
            batch_Y = Y[i:end_idx]
            
            # Warm start with previous solution
            ib_new = InformationBottleneck(n_clusters=n_clusters)
            ib_new.fit(np.concatenate([X[:i], batch_X]), 
                      np.concatenate([Y[:i], batch_Y]))
            ib = ib_new
            
        return ib
    
    def _agglomerative_ib(self, X, Y, n_clusters_schedule):
        """
        Agglomerative Information Bottleneck.
        
        Start with many clusters and merge similar ones.
        """
        if isinstance(n_clusters_schedule, int):
            n_clusters_schedule = [len(np.unique(X)), n_clusters_schedule]
            
        # Start with maximum number of clusters
        current_clusters = n_clusters_schedule[0]
        ib = InformationBottleneck(n_clusters=current_clusters)
        ib.fit(X, Y)
        
        results = [(current_clusters, ib)]
        
        # Sequentially reduce clusters by merging most similar ones
        for target_clusters in n_clusters_schedule[1:]:
            while current_clusters > target_clusters:
                # Find most similar cluster pair to merge
                merge_candidates = self._find_merge_candidates(ib, X, Y)
                best_pair = merge_candidates[0]
                
                # Merge clusters and retrain
                new_ib = self._merge_clusters(ib, best_pair, X, Y)
                ib = new_ib
                current_clusters -= 1
                
            results.append((current_clusters, ib))
        
        return results
    
    def _pareto_optimization(self, X, Y, n_clusters):
        """Multi-objective optimization of I(T;Y) vs I(X;T)."""
        from scipy.optimize import differential_evolution
        
        def objective(beta_log):
            beta = 10 ** beta_log[0]
            ib = InformationBottleneck(n_clusters=n_clusters, beta=beta)
            ib.fit(X, Y)
            
            T = ib.transform(X)
            I_XT = ib._compute_mutual_information_discrete(X, T)
            I_TY = ib._compute_mutual_information_discrete(T, Y)
            
            # Return negative values for maximization
            return [-I_TY, I_XT]  # Multi-objective: max I_TY, min I_XT
        
        # Use differential evolution for multi-objective optimization
        result = differential_evolution(
            objective, 
            bounds=[(-2, 2)],  # log10(beta) bounds
            maxiter=50,
            popsize=15
        )
        
        return result


class MutualInfoEstimator:
    """
    Advanced mutual information estimators.
    
    Implements multiple estimation methods:
    - k-nearest neighbor (Kraskov et al.)
    - Kernel density estimation
    - Neural mutual information estimation (MINE)
    - Binning-based methods with adaptive binning
    """
    
    def __init__(self, method='knn', **kwargs):
        self.method = method
        self.kwargs = kwargs
    
    def estimate(self, X, Y):
        """
        Estimate mutual information I(X;Y).
        
        Parameters
        ----------
        X : array-like, shape (n_samples, n_features_x)
            First variable
        Y : array-like, shape (n_samples, n_features_y) 
            Second variable
            
        Returns
        -------
        mi : float
            Estimated mutual information in nats
        """
        X = np.asarray(X)
        Y = np.asarray(Y)
        
        if len(X) != len(Y):
            raise ValueError("X and Y must have same number of samples")
        
        if self.method == 'knn':
            return self._knn_estimator(X, Y)
        elif self.method == 'kde':
            return self._kde_estimator(X, Y) 
        elif self.method == 'mine':
            return self._mine_estimator(X, Y)
        elif self.method == 'binning':
            return self._binning_estimator(X, Y)
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def _knn_estimator(self, X, Y):
        """k-nearest neighbor estimator (Kraskov et al. 2004)."""
        from sklearn.neighbors import NearestNeighbors
        
        k = self.kwargs.get('k', 3)
        
        # Normalize features
        X = (X - X.mean(axis=0)) / X.std(axis=0)
        Y = (Y - Y.mean(axis=0)) / Y.std(axis=0)
        
        # Combined space
        XY = np.column_stack([X, Y])
        
        # Find k-th nearest neighbor distances
        nbrs_xy = NearestNeighbors(n_neighbors=k+1, metric='chebyshev').fit(XY)
        distances_xy, _ = nbrs_xy.kneighbors(XY)
        epsilon = distances_xy[:, -1]  # k-th nearest neighbor distance
        
        # Count neighbors in marginal spaces within epsilon distance
        nbrs_x = NearestNeighbors(metric='chebyshev').fit(X)
        nbrs_y = NearestNeighbors(metric='chebyshev').fit(Y)
        
        n_x = np.array([len(nbrs_x.radius_neighbors([x], radius=eps)[1]) - 1 
                       for x, eps in zip(X, epsilon)])
        n_y = np.array([len(nbrs_y.radius_neighbors([y], radius=eps)[1]) - 1 
                       for y, eps in zip(Y, epsilon)])
        
        # Kraskov estimator
        mi = np.mean(np.log(n_x) + np.log(n_y)) + np.log(k) - np.log(len(X))
        return max(0, mi)  # MI cannot be negative
    
    def _kde_estimator(self, X, Y):
        """Kernel density estimation."""
        from scipy import stats
        
        # Use Gaussian kernels
        kde_xy = stats.gaussian_kde(np.column_stack([X.flatten(), Y.flatten()]).T)
        kde_x = stats.gaussian_kde(X.flatten())
        kde_y = stats.gaussian_kde(Y.flatten())
        
        # Sample points for integration
        n_samples = self.kwargs.get('n_samples', 1000)
        xy_samples = np.column_stack([X.flatten(), Y.flatten()])[:n_samples]
        
        # Estimate MI via sampling
        pdf_xy = kde_xy(xy_samples.T)
        pdf_x = kde_x(xy_samples[:, 0])
        pdf_y = kde_y(xy_samples[:, 1])
        
        # MI = ‚à´ p(x,y) log(p(x,y) / (p(x)p(y))) dx dy
        mi = np.mean(np.log(pdf_xy / (pdf_x * pdf_y)))
        return max(0, mi)
    
    def _mine_estimator(self, X, Y):
        """Mutual Information Neural Estimation."""
        import torch
        import torch.nn as nn
        import torch.optim as optim
        
        # Simple neural network T_Œ∏(x,y)
        input_dim = X.shape[1] + Y.shape[1] if X.ndim > 1 else 2
        
        class MINENet(nn.Module):
            def __init__(self, input_dim=2, hidden_dim=50):
                super(MINENet, self).__init__()
                self.net = nn.Sequential(
                    nn.Linear(input_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, hidden_dim),
                    nn.ReLU(),
                    nn.Linear(hidden_dim, 1)
                )
                
            def forward(self, xy):
                return self.net(xy)
        
        # Prepare data
        X_tensor = torch.FloatTensor(X.reshape(-1, 1) if X.ndim == 1 else X)
        Y_tensor = torch.FloatTensor(Y.reshape(-1, 1) if Y.ndim == 1 else Y)
        xy_tensor = torch.cat([X_tensor, Y_tensor], dim=1)
        
        # Create shuffled version for marginal
        indices = torch.randperm(len(Y_tensor))
        xy_marginal = torch.cat([X_tensor, Y_tensor[indices]], dim=1)
        
        # Train MINE network
        mine_net = MINENet(input_dim=xy_tensor.shape[1])
        optimizer = optim.Adam(mine_net.parameters(), lr=1e-3)
        
        epochs = self.kwargs.get('epochs', 100)
        for epoch in range(epochs):
            optimizer.zero_grad()
            
            # MINE objective: E[T_Œ∏(x,y)] - log(E[exp(T_Œ∏(x,y'))])
            t_joint = mine_net(xy_tensor)
            t_marginal = mine_net(xy_marginal)
            
            loss = -(t_joint.mean() - torch.logsumexp(t_marginal, 0))
            loss.backward()
            optimizer.step()
        
        # Final MI estimate
        with torch.no_grad():
            t_joint = mine_net(xy_tensor)
            t_marginal = mine_net(xy_marginal)
            mi = t_joint.mean() - torch.logsumexp(t_marginal, 0) + np.log(len(X))
            
        return max(0, mi.item())
    
    def _binning_estimator(self, X, Y):
        """Adaptive binning estimator."""
        # Determine optimal number of bins using Scott's rule
        n = len(X)
        bins_x = max(int(n ** (1/3)), 2)
        bins_y = max(int(n ** (1/3)), 2)
        
        # Create 2D histogram
        hist_xy, _, _ = np.histogram2d(X.flatten(), Y.flatten(), 
                                     bins=[bins_x, bins_y])
        hist_x = np.sum(hist_xy, axis=1)
        hist_y = np.sum(hist_xy, axis=0)
        
        # Convert to probabilities
        p_xy = hist_xy / n
        p_x = hist_x / n
        p_y = hist_y / n
        
        # Compute MI
        mi = 0
        for i in range(bins_x):
            for j in range(bins_y):
                if p_xy[i, j] > 0:
                    mi += p_xy[i, j] * np.log(p_xy[i, j] / (p_x[i] * p_y[j]))
        
        return max(0, mi)


class MutualInfoCore:
    """
    Core mutual information computations and utilities.
    
    Provides efficient implementations of common information-theoretic
    operations used throughout the Information Bottleneck algorithms.
    """
    
    def __init__(self):
        self.estimator_cache = {}
    
    def compute_entropy(self, X, method='histogram', bins='auto'):
        """
        Compute entropy H(X).
        
        Parameters
        ----------
        X : array-like
            Random variable
        method : str
            Estimation method ('histogram', 'kde')
        bins : int or str
            Number of bins for histogram method
        """
        if method == 'histogram':
            if isinstance(bins, str) and bins == 'auto':
                bins = max(int(np.sqrt(len(X))), 2)
            
            counts, _ = np.histogram(X, bins=bins)
            probs = counts / np.sum(counts)
            probs = probs[probs > 0]  # Remove zero probabilities
            
            return -np.sum(probs * np.log2(probs))
            
        elif method == 'kde':
            from scipy import stats
            kde = stats.gaussian_kde(X)
            
            # Sample for entropy estimation
            x_range = np.linspace(X.min(), X.max(), 1000)
            pdf = kde(x_range)
            pdf = pdf / np.sum(pdf)  # Normalize
            pdf = pdf[pdf > 0]
            
            return -np.sum(pdf * np.log2(pdf))
        
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def compute_joint_entropy(self, X, Y, method='histogram'):
        """Compute joint entropy H(X,Y)."""
        if method == 'histogram':
            bins_x = max(int(np.sqrt(len(X))), 2)
            bins_y = max(int(np.sqrt(len(Y))), 2)
            
            hist, _, _ = np.histogram2d(X.flatten(), Y.flatten(), 
                                     bins=[bins_x, bins_y])
            probs = hist / np.sum(hist)
            probs = probs[probs > 0]
            
            return -np.sum(probs * np.log2(probs))
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def compute_conditional_entropy(self, X, Y, method='histogram'):
        """Compute conditional entropy H(X|Y) = H(X,Y) - H(Y)."""
        H_XY = self.compute_joint_entropy(X, Y, method)
        H_Y = self.compute_entropy(Y, method)
        return H_XY - H_Y
    
    def compute_mutual_information(self, X, Y, method='histogram'):
        """
        Compute mutual information I(X;Y) = H(X) - H(X|Y).
        
        This is the main interface for MI computation.
        """
        cache_key = (id(X), id(Y), method)
        if cache_key in self.estimator_cache:
            return self.estimator_cache[cache_key]
        
        H_X = self.compute_entropy(X, method)
        H_X_given_Y = self.compute_conditional_entropy(X, Y, method)
        mi = H_X - H_X_given_Y
        
        self.estimator_cache[cache_key] = mi
        return max(0, mi)  # MI cannot be negative
    
    def compute_conditional_mutual_information(self, X, Y, Z):
        """Compute conditional MI I(X;Y|Z)."""
        # I(X;Y|Z) = H(X|Z) - H(X|Y,Z)
        H_X_given_Z = self.compute_conditional_entropy(X, Z)
        
        # For H(X|Y,Z), we need joint conditioning
        YZ = np.column_stack([Y, Z])
        H_X_given_YZ = self.compute_conditional_entropy(X, YZ)
        
        return H_X_given_Z - H_X_given_YZ
    
    def information_bottleneck_curve(self, X, Y, beta_values, n_clusters=10):
        """
        Compute Information Bottleneck curve.
        
        Returns I(X;T) vs I(T;Y) for different Œ≤ values.
        """
        curve_points = []
        
        for beta in beta_values:
            ib = InformationBottleneck(n_clusters=n_clusters, beta=beta)
            ib.fit(X, Y)
            T = ib.transform(X)
            
            I_XT = self.compute_mutual_information(X, T)
            I_TY = self.compute_mutual_information(T, Y)
            
            curve_points.append({
                'beta': beta,
                'I_XT': I_XT,
                'I_TY': I_TY,
                'compression': I_XT,
                'relevance': I_TY
            })
        
        return curve_points
    
    def compute_information_plane_coordinates(self, X, Y, T):
        """
        Compute coordinates in the information plane.
        
        Returns (I(X;T), I(T;Y)) coordinates for visualization.
        """
        I_XT = self.compute_mutual_information(X, T)
        I_TY = self.compute_mutual_information(T, Y)
        I_XY = self.compute_mutual_information(X, Y)
        
        return {
            'I_XT': I_XT,
            'I_TY': I_TY, 
            'I_XY': I_XY,
            'compression_ratio': I_XT / I_XY if I_XY > 0 else 0,
            'relevance_ratio': I_TY / I_XY if I_XY > 0 else 0
        }
    
    def _compute_I_TX(self, X, p_t_given_x):
        """
        Compute I(T;X) = H(T) - H(T|X) per Tishby et al. (1999) equation (4)
        
        Uses numerically stable computation with log-space arithmetic.
        """
        n_samples, n_clusters = p_t_given_x.shape
        epsilon = 1e-12
        
        # Compute p(t) = (1/n) Œ£_x p(t|x)
        p_t = np.mean(p_t_given_x, axis=0)
        
        # H(T) = -Œ£_t p(t) log p(t)
        H_T = -np.sum(p_t * np.log(p_t + epsilon))
        
        # H(T|X) = -(1/n) Œ£_x Œ£_t p(t|x) log p(t|x)
        H_T_given_X = -np.mean(np.sum(p_t_given_x * np.log(p_t_given_x + epsilon), axis=1))
        
        return H_T - H_T_given_X
    
    def _compute_I_TY(self, Y, p_y_given_t, p_t_given_x):
        """
        Compute I(T;Y) using joint distributions per Tishby et al. (1999)
        
        Based on I(T;Y) = Œ£_t,y p(t,y) log(p(t,y)/(p(t)p(y)))
        """
        n_samples = len(Y)
        n_clusters, n_classes = p_y_given_t.shape
        epsilon = 1e-12
        
        # Compute p(t) from p(t|x)
        p_t = np.mean(p_t_given_x, axis=0)
        
        # Compute p(y) from empirical distribution
        unique_y, counts_y = np.unique(Y, return_counts=True)
        p_y = counts_y / n_samples
        
        # Compute joint p(t,y) = p(y|t) * p(t)
        p_ty = p_y_given_t * p_t[:, np.newaxis]
        
        # I(T;Y) = Œ£_t,y p(t,y) log(p(t,y)/(p(t)p(y)))
        I_TY = 0.0
        for t in range(n_clusters):
            for y in range(n_classes):
                if p_ty[t, y] > epsilon and p_t[t] > epsilon and p_y[y] > epsilon:
                    log_ratio = np.log(p_ty[t, y] + epsilon) - np.log(p_t[t] + epsilon) - np.log(p_y[y] + epsilon)
                    I_TY += p_ty[t, y] * log_ratio
                    
        return I_TY
    
    def _update_conditional_probabilities(self, X, Y, p_t_given_x):
        """
        E-step: Update p(y|t) using Bayes rule per Tishby et al. (1999)
        
        Formula: p(y|t) = Œ£_x p(y|x) * p(x|t) / p(t)
        Based on equation (3) in Tishby et al. (1999) Information Bottleneck Method
        """
        n_samples = len(Y)
        n_clusters = p_t_given_x.shape[1]
        unique_y = np.unique(Y)
        n_classes = len(unique_y)
        
        # Compute p(t) = (1/n) Œ£_x p(t|x)
        p_t = np.mean(p_t_given_x, axis=0)
        
        # Compute p(x|t) using Bayes rule: p(x|t) = p(t|x) * p(x) / p(t)
        # Assume uniform p(x) = 1/n for simplicity (can be made configurable)
        p_x = np.ones(n_samples) / n_samples
        epsilon = getattr(self, 'numerical_epsilon', 1e-10)
        
        p_x_given_t = np.zeros((n_samples, n_clusters))
        for t in range(n_clusters):
            if p_t[t] > epsilon:
                p_x_given_t[:, t] = p_t_given_x[:, t] * p_x / (p_t[t] + epsilon)
        
        # Compute p(y|t) = Œ£_x p(y|x) * p(x|t)
        p_y_given_t = np.zeros((n_clusters, n_classes))
        
        for t in range(n_clusters):
            for y_idx, y_val in enumerate(unique_y):
                # p(y|x) is observed: 1 if Y[x] == y_val, 0 otherwise
                y_mask = (Y == y_val)
                p_y_given_t[t, y_idx] = np.sum(p_x_given_t[y_mask, t])
        
        # Numerical stability and normalization
        p_y_given_t = np.clip(p_y_given_t, epsilon, 1.0 - epsilon)
        p_y_given_t = p_y_given_t / (p_y_given_t.sum(axis=1, keepdims=True) + epsilon)
        
        return p_y_given_t
    
    def _update_cluster_assignments(self, X, Y, p_y_given_t, beta):
        """
        M-step: Update p(t|x) using Information Bottleneck functional per Tishby et al. (1999)
        
        Formula: p(t|x) ‚àù exp(Œ≤ * Œ£_y p(y|x) log p(y|t))
        Based on Blahut-Arimoto algorithm in Tishby et al. (1999)
        """
        n_samples = len(Y)
        n_clusters = p_y_given_t.shape[0]
        unique_y = np.unique(Y)
        epsilon = getattr(self, 'numerical_epsilon', 1e-10)
        
        p_t_given_x = np.zeros((n_samples, n_clusters))
        
        for x in range(n_samples):
            y_val = Y[x]
            y_idx = np.where(unique_y == y_val)[0][0]
            
            # Compute log-likelihood for each cluster
            for t in range(n_clusters):
                # p(t|x) ‚àù exp(Œ≤ * log p(y|t)) where y is observed value for sample x
                log_likelihood = np.log(p_y_given_t[t, y_idx] + epsilon)
                p_t_given_x[x, t] = np.exp(beta * log_likelihood)
        
        # Normalize to get proper probabilities
        p_t_given_x = p_t_given_x / (p_t_given_x.sum(axis=1, keepdims=True) + epsilon)
        
        # Apply numerical stability
        p_t_given_x = np.clip(p_t_given_x, epsilon, 1.0 - epsilon)
        p_t_given_x = p_t_given_x / p_t_given_x.sum(axis=1, keepdims=True)
        
        return p_t_given_x