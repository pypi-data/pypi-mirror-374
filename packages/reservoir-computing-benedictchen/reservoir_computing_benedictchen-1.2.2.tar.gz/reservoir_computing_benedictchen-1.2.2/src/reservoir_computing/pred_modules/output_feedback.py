"""
ğŸ”„ Output Feedback - Closed-Loop Echo State Network Dynamics
=========================================================

Author: Benedict Chen (benedict@benedictchen.com)

ğŸ’° Donations: Help support this research!
   PayPal: https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=WXQKYYKPHWXHS
   ğŸ’– Please consider recurring donations to support continued ESN research

Output feedback implementation for Echo State Networks. Handles the critical
feedback loop that enables autonomous generation and proper state evolution
in closed-loop configurations.

ğŸ”¬ Research Foundation:
======================
Based on Jaeger, H. (2001) "The Echo State Approach" Section 3.4:
- Equation 1: x(n+1) = f(W_resÂ·x(n) + W_inÂ·u(n+1) + W_fbÂ·y(n))
- Output feedback enables autonomous generation after training
- Critical for sequence generation and closed-loop control tasks
- Stability depends on spectral radius with feedback included

ELI5 Explanation:
================
Think of output feedback like learning to ride a bicycle with your eyes closed!

When you ride a bike normally, you look ahead (input) and adjust your steering.
But with output feedback, it's like the bike "remembers" where it just went 
and uses that memory to help decide where to go next. This lets the Echo State 
Network generate sequences on its own - like writing a song where each note 
helps decide the next note, even when the original music stops!

The tricky part is making sure this "memory loop" doesn't spiral out of control
(like when a microphone gets too close to a speaker and creates feedback noise).

ASCII Feedback Architecture:
============================
    Previous Output    Current Input       Next State
         y(n-1)           u(n)              x(n+1)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Feedback    â”‚  â”‚ External    â”‚  â”‚ Reservoir   â”‚
    â”‚ W_fbÂ·y(n-1) â”‚  â”‚ W_inÂ·u(n)   â”‚  â”‚ State       â”‚
    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                â”‚                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ f(W_resÂ·x(n) +  â”‚
                  â”‚  W_inÂ·u(n) +    â”‚ â”€â”€â”
                  â”‚  W_fbÂ·y(n-1))   â”‚   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                           â”‚            â”‚
                           â–¼            â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                  â”‚ Linear Readout  â”‚   â”‚
                  â”‚ y(n) = W_outÂ·x(n)â”‚â—€â”€â”€â”˜
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â””â”€â”€â”€ Feeds back for next timestep

âš¡ Technical Implementation:
===========================
1. **State Update with Feedback**: x(n+1) = f(W_resÂ·x(n) + W_inÂ·u(n+1) + W_fbÂ·y(n))
2. **Feedback Weight Initialization**: Typically sparse, small magnitude (|W_fb| << |W_res|)
3. **Stability Analysis**: Combined spectral radius Ï(W_res + W_fbÂ·W_out) must be < 1
4. **Autonomous Generation**: Set u(n) = 0 and let y(n-1) drive the dynamics

ğŸ“Š Performance Characteristics:
==============================
â€¢ **Memory Depth**: Feedback extends effective memory beyond reservoir size
â€¢ **Generation Quality**: Depends on feedback scaling and network stability  
â€¢ **Training Complexity**: O(n_reservoirÂ²) for stability analysis
â€¢ **Inference Speed**: Minimal overhead (~5% slower than feedforward)

This module implements the essential "closed loop" that transforms ESNs from 
simple pattern recognizers into autonomous sequence generators.
"""

import numpy as np
from typing import Optional, Tuple, Dict, Any, List, Union, Callable
import warnings
import logging

# Configure logging
logger = logging.getLogger(__name__)


class OutputFeedbackMixin:
    """
    Mixin class to add output feedback capabilities to ESN
    """
    
    def configure_output_feedback(self, 
                                feedback_scaling: float = 1.0,
                                feedback_type: str = 'linear') -> None:
        """Configure output feedback for the ESN"""
        configure_output_feedback(self, feedback_scaling, feedback_type)
    
    def update_state_with_feedback(self, 
                                 current_state: np.ndarray,
                                 input_vec: np.ndarray,
                                 feedback_output: Optional[np.ndarray] = None) -> np.ndarray:
        """Update state with output feedback"""
        return update_state_with_feedback(self, current_state, input_vec, feedback_output)
    
    def set_feedback_weights(self, W_feedback: np.ndarray) -> None:
        """Set output feedback weights"""
        self.W_feedback = W_feedback
        logger.info(f"Output feedback weights set: {W_feedback.shape}")
    
    def initialize_feedback_weights(self, 
                                  scaling: float = 0.1,
                                  sparsity: float = 0.1) -> None:
        """Initialize random feedback weights"""
        if not hasattr(self, 'n_reservoir') or not hasattr(self, 'n_outputs'):
            raise ValueError("ESN must have n_reservoir and n_outputs defined")
        
        # Create sparse random feedback matrix
        W_feedback = np.random.randn(self.n_reservoir, self.n_outputs) * scaling
        
        # Apply sparsity
        if sparsity < 1.0:
            mask = np.random.random((self.n_reservoir, self.n_outputs)) < sparsity
            W_feedback = W_feedback * mask
        
        self.W_feedback = W_feedback
        logger.info(f"Initialized feedback weights: {W_feedback.shape} with sparsity {sparsity}")


def configure_output_feedback(esn_model, 
                            feedback_scaling: float = 1.0,
                            feedback_type: str = 'linear') -> None:
    """
    Configure output feedback mechanism for ESN
    
    Args:
        esn_model: ESN to configure
        feedback_scaling: Global scaling factor for feedback
        feedback_type: Type of feedback ('linear', 'nonlinear', 'delayed')
    """
    esn_model._feedback_scaling = feedback_scaling
    esn_model._feedback_type = feedback_type
    
    # Initialize feedback weights if not present
    if not hasattr(esn_model, 'W_feedback') or esn_model.W_feedback is None:
        if hasattr(esn_model, 'n_reservoir') and hasattr(esn_model, 'n_outputs'):
            esn_model.initialize_feedback_weights()
        else:
            logger.warning("Cannot initialize feedback weights - missing n_reservoir or n_outputs")
    
    logger.info(f"Output feedback configured: type={feedback_type}, scaling={feedback_scaling}")


def update_state_with_feedback(esn_model,
                             current_state: np.ndarray,
                             input_vec: np.ndarray,
                             feedback_output: Optional[np.ndarray] = None) -> np.ndarray:
    """
    Update reservoir state with output feedback
    
    Implements the complete ESN state update equation:
    x(n+1) = f(W_in * u(n+1) + W * x(n) + W_feedback * y(n))
    
    Args:
        esn_model: ESN with state parameters
        current_state: Current reservoir state x(n)
        input_vec: Input vector u(n+1)
        feedback_output: Previous output y(n) for feedback
        
    Returns:
        Updated reservoir state x(n+1)
    """
    # Input contribution: W_in * u(n+1)
    input_contrib = compute_input_contribution(esn_model, input_vec)
    
    # Reservoir recurrence: W * x(n)
    reservoir_contrib = compute_reservoir_contribution(esn_model, current_state)
    
    # Output feedback: W_feedback * y(n)
    feedback_contrib = compute_feedback_contribution(esn_model, feedback_output)
    
    # Combine all contributions
    total_input = input_contrib + reservoir_contrib + feedback_contrib
    
    # Apply noise if configured
    if hasattr(esn_model, 'noise_level') and esn_model.noise_level > 0:
        noise = np.random.normal(0, esn_model.noise_level, total_input.shape)
        total_input += noise
    
    # Apply reservoir activation function
    activation = getattr(esn_model, 'reservoir_activation', np.tanh)
    next_state = activation(total_input)
    
    return next_state


def compute_input_contribution(esn_model, input_vec: np.ndarray) -> np.ndarray:
    """
    Compute input contribution to reservoir state
    
    Args:
        esn_model: ESN with input weights
        input_vec: Input vector
        
    Returns:
        Input contribution W_in * u(n+1)
    """
    if hasattr(esn_model, 'W_in') and esn_model.W_in is not None:
        input_flat = input_vec.flatten()
        return esn_model.W_in @ input_flat
    else:
        # No input weights - return zero contribution
        n_reservoir = getattr(esn_model, 'n_reservoir', len(getattr(esn_model, 'W_reservoir', [100])))
        return np.zeros(n_reservoir)


def compute_reservoir_contribution(esn_model, current_state: np.ndarray) -> np.ndarray:
    """
    Compute reservoir recurrence contribution
    
    Args:
        esn_model: ESN with reservoir weights
        current_state: Current reservoir state
        
    Returns:
        Reservoir contribution W * x(n)
    """
    if hasattr(esn_model, 'W_reservoir') and esn_model.W_reservoir is not None:
        return esn_model.W_reservoir @ current_state
    else:
        # No reservoir weights - simple decay
        return current_state * 0.9


def compute_feedback_contribution(esn_model, feedback_output: Optional[np.ndarray]) -> np.ndarray:
    """
    Compute output feedback contribution
    
    Args:
        esn_model: ESN with feedback weights
        feedback_output: Previous output for feedback
        
    Returns:
        Feedback contribution W_feedback * y(n)
    """
    n_reservoir = getattr(esn_model, 'n_reservoir', 100)
    
    if feedback_output is None:
        return np.zeros(n_reservoir)
    
    if not hasattr(esn_model, 'W_feedback') or esn_model.W_feedback is None:
        return np.zeros(n_reservoir)
    
    # Get feedback scaling
    feedback_scaling = getattr(esn_model, '_feedback_scaling', 1.0)
    
    # Compute feedback contribution
    feedback_flat = feedback_output.flatten()
    feedback_contrib = esn_model.W_feedback @ feedback_flat
    
    # Apply feedback type processing
    feedback_type = getattr(esn_model, '_feedback_type', 'linear')
    
    if feedback_type == 'linear':
        pass  # No additional processing
    elif feedback_type == 'nonlinear':
        # Apply nonlinear transformation to feedback
        feedback_contrib = np.tanh(feedback_contrib)
    elif feedback_type == 'delayed':
        # Implement delay (simplified - would need delay buffer in practice)
        feedback_contrib = feedback_contrib * 0.8
    
    return feedback_contrib * feedback_scaling


def setup_feedback_matrices(esn_model,
                          input_scaling: float = 1.0,
                          spectral_radius: float = 0.95,
                          feedback_scaling: float = 0.1,
                          sparsity: float = 0.1) -> None:
    """
    Setup all feedback-related matrices for ESN
    
    Args:
        esn_model: ESN to configure
        input_scaling: Input weight scaling
        spectral_radius: Target spectral radius for reservoir
        feedback_scaling: Feedback weight scaling
        sparsity: Connection sparsity (0-1)
    """
    if not hasattr(esn_model, 'n_reservoir'):
        raise ValueError("ESN must have n_reservoir defined")
    
    n_reservoir = esn_model.n_reservoir
    n_inputs = getattr(esn_model, 'n_inputs', 1)
    n_outputs = getattr(esn_model, 'n_outputs', 1)
    
    # Input weights W_in
    if not hasattr(esn_model, 'W_in') or esn_model.W_in is None:
        W_in = np.random.uniform(-input_scaling, input_scaling, (n_reservoir, n_inputs))
        esn_model.W_in = W_in
        logger.info(f"Initialized input weights: {W_in.shape}")
    
    # Reservoir weights W
    if not hasattr(esn_model, 'W_reservoir') or esn_model.W_reservoir is None:
        W_reservoir = np.random.randn(n_reservoir, n_reservoir)
        
        # Apply sparsity
        if sparsity < 1.0:
            mask = np.random.random((n_reservoir, n_reservoir)) < sparsity
            W_reservoir = W_reservoir * mask
        
        # Scale to target spectral radius
        eigenvals = np.linalg.eigvals(W_reservoir)
        current_radius = np.max(np.abs(eigenvals))
        if current_radius > 0:
            W_reservoir = W_reservoir * (spectral_radius / current_radius)
        
        esn_model.W_reservoir = W_reservoir
        logger.info(f"Initialized reservoir weights: {W_reservoir.shape}, Ï={spectral_radius}")
    
    # Feedback weights W_feedback
    if not hasattr(esn_model, 'W_feedback') or esn_model.W_feedback is None:
        W_feedback = np.random.uniform(-feedback_scaling, feedback_scaling, (n_reservoir, n_outputs))
        
        # Apply sparsity
        if sparsity < 1.0:
            mask = np.random.random((n_reservoir, n_outputs)) < sparsity
            W_feedback = W_feedback * mask
        
        esn_model.W_feedback = W_feedback
        logger.info(f"Initialized feedback weights: {W_feedback.shape}")


def analyze_feedback_stability(esn_model, max_iterations: int = 1000) -> Dict[str, Any]:
    """
    Analyze the stability of the feedback loop
    
    Args:
        esn_model: ESN with feedback configuration
        max_iterations: Maximum iterations for stability test
        
    Returns:
        Stability analysis results
    """
    if not hasattr(esn_model, 'W_feedback') or esn_model.W_feedback is None:
        return {'stable': True, 'reason': 'no_feedback'}
    
    # Test with small perturbation
    initial_state = np.random.randn(esn_model.n_reservoir) * 0.1
    initial_output = np.random.randn(esn_model.n_outputs) * 0.1
    
    states = [initial_state]
    outputs = [initial_output]
    
    current_state = initial_state.copy()
    current_output = initial_output.copy()
    
    for iteration in range(max_iterations):
        # Update state with feedback
        next_state = update_state_with_feedback(
            esn_model, current_state, np.zeros(esn_model.n_inputs), current_output
        )
        
        # Compute next output (if model is trained)
        if hasattr(esn_model, 'W_out') and esn_model.W_out is not None:
            if hasattr(esn_model, 'predict_single_step'):
                next_output = esn_model.predict_single_step(next_state)
            else:
                # Simple linear readout
                next_output = next_state @ esn_model.W_out
        else:
            next_output = current_output * 0.95  # Decay
        
        states.append(next_state)
        outputs.append(next_output)
        
        # Check for divergence
        state_norm = np.linalg.norm(next_state)
        output_norm = np.linalg.norm(next_output)
        
        if state_norm > 100 or output_norm > 100:
            return {
                'stable': False,
                'reason': 'divergence',
                'iteration': iteration,
                'final_state_norm': state_norm,
                'final_output_norm': output_norm
            }
        
        current_state = next_state
        current_output = next_output
    
    # Compute stability metrics
    final_states = np.array(states[-100:])  # Last 100 states
    state_variance = np.var(final_states, axis=0)
    mean_variance = np.mean(state_variance)
    
    return {
        'stable': mean_variance < 1.0,
        'reason': 'bounded' if mean_variance < 1.0 else 'high_variance',
        'iterations': max_iterations,
        'final_state_variance': mean_variance,
        'final_state_norm': np.linalg.norm(states[-1]),
        'convergence_metric': mean_variance
    }


def optimize_feedback_scaling(esn_model,
                            test_sequence: np.ndarray,
                            scaling_range: Tuple[float, float] = (0.01, 1.0),
                            n_points: int = 20) -> Dict[str, Any]:
    """
    Optimize feedback scaling for stability and performance
    
    Args:
        esn_model: ESN to optimize
        test_sequence: Test sequence for evaluation
        scaling_range: Range of scaling values to test
        n_points: Number of points to test
        
    Returns:
        Optimization results
    """
    scaling_values = np.linspace(scaling_range[0], scaling_range[1], n_points)
    results = []
    
    # Store original feedback weights
    original_feedback = esn_model.W_feedback.copy() if hasattr(esn_model, 'W_feedback') and esn_model.W_feedback is not None else None
    
    if original_feedback is None:
        logger.warning("No feedback weights to optimize")
        return {'status': 'no_feedback'}
    
    for scaling in scaling_values:
        # Scale feedback weights
        esn_model.W_feedback = original_feedback * scaling
        
        # Test stability
        stability = analyze_feedback_stability(esn_model)
        
        # Test performance if possible
        performance = None
        if len(test_sequence) > 0:
            try:
                if hasattr(esn_model, 'predict_sequence'):
                    predictions = esn_model.predict_sequence(test_sequence[:len(test_sequence)//2])
                    targets = test_sequence[len(test_sequence)//2:]
                    min_len = min(len(predictions), len(targets))
                    performance = np.mean((predictions[:min_len] - targets[:min_len])**2)
            except Exception as e:
                logger.warning(f"Performance test failed for scaling {scaling}: {e}")
        
        results.append({
            'scaling': scaling,
            'stable': stability['stable'],
            'stability_metric': stability.get('convergence_metric', float('inf')),
            'performance': performance
        })
    
    # Find best scaling
    stable_results = [r for r in results if r['stable']]
    
    if stable_results:
        if all(r['performance'] is not None for r in stable_results):
            # Choose based on performance
            best = min(stable_results, key=lambda x: x['performance'])
        else:
            # Choose based on stability metric
            best = min(stable_results, key=lambda x: x['stability_metric'])
    else:
        # No stable configurations - choose least unstable
        best = min(results, key=lambda x: x['stability_metric'])
        logger.warning("No stable feedback scaling found - using least unstable")
    
    # Restore best scaling
    esn_model.W_feedback = original_feedback * best['scaling']
    
    return {
        'status': 'success',
        'optimal_scaling': best['scaling'],
        'best_result': best,
        'all_results': results,
        'n_stable': len(stable_results),
        'n_tested': len(results)
    }