"""
ðŸ”§ Basic Configuration Mixin - Core ESN Parameter Settings
=========================================================

Author: Benedict Chen (benedict@benedictchen.com)

This module contains basic configuration methods for Echo State Networks
extracted from the original monolithic configuration_optimization.py file.

Based on: Herbert Jaeger (2001) "The 'Echo State' Approach to Analysing and Training Recurrent Neural Networks"
"""

import numpy as np


class BasicConfigurationMixin:
    """
    ðŸ”§ Basic Configuration Mixin for Echo State Networks
    
    This mixin provides core configuration capabilities for Echo State Networks,
    implementing fundamental settings from Jaeger 2001.
    
    ðŸŒŸ Key Features:
    - Activation function configuration (6 types)
    - Noise strategy configuration (6 types)  
    - State collection method configuration
    - Training solver configuration
    - Output feedback control
    - Leaky integration settings
    - Bias term configuration
    """
    
    def configure_activation_function(self, func_type: str, custom_func=None):
        """
        ðŸŽ¯ Configure Reservoir Activation Function - 6 Powerful Options from Jaeger 2001!
        
        ðŸ”¬ **Research Background**: Jaeger (2001) showed different activation functions 
        dramatically affect Echo State Network performance. This method lets you experiment 
        with all major options to find the perfect fit for your task!
        
        ðŸ“Š **Visual Guide**:
        ```
        ðŸ“ˆ ACTIVATION FUNCTIONS COMPARISON
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Function Type  â”‚   Formula    â”‚   Range         â”‚   Best For       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ ðŸŒŠ tanh         â”‚ tanh(x)      â”‚ [-1, 1]        â”‚ General purpose  â”‚
        â”‚ ðŸ“ˆ sigmoid      â”‚ 1/(1+e^-x)   â”‚ [0, 1]         â”‚ Binary signals   â”‚  
        â”‚ âš¡ relu         â”‚ max(0,x)     â”‚ [0, âˆž]         â”‚ Sparse patterns  â”‚
        â”‚ ðŸ”§ leaky_relu   â”‚ max(0.01x,x) â”‚ (-âˆž, âˆž)       â”‚ Better gradients â”‚
        â”‚ ðŸ“ linear       â”‚ x            â”‚ (-âˆž, âˆž)       â”‚ Linear systems   â”‚
        â”‚ ðŸŽ¨ custom       â”‚ your_func(x) â”‚ user-defined   â”‚ Special tasks    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ```
        
        ðŸŽ® **Usage Examples**:
        ```python
        # ðŸŒŸ EXAMPLE 1: Classic nonlinear time series (recommended)
        esn = EchoStateNetwork(n_reservoir=100)
        esn.configure_activation_function('tanh')  # Smooth, bounded
        
        # ðŸš€ EXAMPLE 2: Sparse pattern recognition 
        esn.configure_activation_function('relu')  # Creates sparse representations
        
        # ðŸ”¥ EXAMPLE 3: Custom activation for special tasks
        def custom_swish(x):
            return x * (1 / (1 + np.exp(-x)))  # Swish activation
        esn.configure_activation_function('custom', custom_func=custom_swish)
        
        # ðŸ’¡ EXAMPLE 4: Binary classification tasks
        esn.configure_activation_function('sigmoid')  # Output range [0,1]
        ```
        
        âš¡ **Performance Tips**:
        - ðŸŒŠ **tanh**: Best general choice, well-tested in literature
        - ðŸ“ˆ **sigmoid**: Use for positive-only outputs  
        - âš¡ **relu**: Great for sparse representations, faster computation
        - ðŸ”§ **leaky_relu**: Fixes "dying ReLU" problem
        - ðŸ“ **linear**: Only for linear dynamics, loses nonlinearity
        - ðŸŽ¨ **custom**: Experiment with modern activations (swish, gelu, etc.)
        
        ðŸ“– **Research Reference**: Jaeger (2001) "The Echo State Approach" - Section 2.1
        
        Args:
            func_type (str): Activation function type - choose from 6 options above
            custom_func (callable, optional): Your custom activation function (only for 'custom' type)
            
        Raises:
            ValueError: If func_type is not one of the 6 valid options
            
        Example:
            >>> esn = EchoStateNetwork(n_reservoir=200)
            >>> esn.configure_activation_function('tanh')  # Classic choice
            âœ“ Activation function set to: tanh
        """
        valid_funcs = ['tanh', 'sigmoid', 'relu', 'leaky_relu', 'linear', 'custom']
        if func_type not in valid_funcs:
            raise ValueError(f"Invalid activation function. Choose from: {valid_funcs}")
        self.activation_function = func_type
        if func_type == 'custom' and custom_func:
            self.custom_activation = custom_func
            if hasattr(self, '_initialize_activation_functions'):
                self._initialize_activation_functions()
        print(f"âœ“ Activation function set to: {func_type}")

    def configure_noise_type(self, noise_type: str, correlation_length: int = 5, training_ratio: float = 1.0):
        """
        ðŸ”Š Configure Reservoir Noise Implementation - 6 Advanced Options from Jaeger 2001!
        
        ðŸ”¬ **Research Background**: Jaeger (2001) demonstrated that strategic noise injection 
        can improve Echo State Property (ESP) and generalization. This method implements all 
        major noise strategies from the research literature!
        
        ðŸ“Š **Noise Types Visual Guide**:
        ```
        ðŸŽšï¸ NOISE IMPLEMENTATION COMPARISON
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Noise Type    â”‚   Where Applied  â”‚   Formula       â”‚   Best For       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ ðŸŽµ additive     â”‚ Reservoir state  â”‚ x + Î¾(0,ÏƒÂ²)    â”‚ General use      â”‚
        â”‚ ðŸŽ¯ input_noise  â”‚ Input signal     â”‚ u + Î¾(0,ÏƒÂ²)    â”‚ Robust learning  â”‚  
        â”‚ âš¡ multiplicativeâ”‚ State scaling    â”‚ x*(1+Î¾(0,ÏƒÂ²))  â”‚ Dynamic systems  â”‚
        â”‚ ðŸŒŠ correlated   â”‚ Spatial pattern  â”‚ spatially-corr  â”‚ Realistic noise  â”‚
        â”‚ ðŸŽ“ train_vs_testâ”‚ Different levels â”‚ Ïƒ_trainâ‰ Ïƒ_test  â”‚ Robustness test  â”‚
        â”‚ ðŸ“Š variance_scaledâ”‚ Adaptive scalingâ”‚ ÏƒÂ² âˆ var(input)â”‚ Signal-adaptive  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ```
        
        ðŸŽ® **Usage Examples**:
        ```python
        # ðŸŒŸ EXAMPLE 1: Input noise for robust learning (Jaeger recommended)
        esn = EchoStateNetwork(n_reservoir=100, noise_level=0.01)
        esn.configure_noise_type('input_noise')  # Noise on inputs only
        
        # ðŸš€ EXAMPLE 2: Spatially correlated noise (more realistic)
        esn.configure_noise_type('correlated', correlation_length=10)
        
        # ðŸ”¥ EXAMPLE 3: Different noise during training vs testing
        esn.configure_noise_type('training_vs_testing', training_ratio=2.0)
        # Training noise = 2.0 * base_noise, testing noise = base_noise
        
        # ðŸ’¡ EXAMPLE 4: Adaptive noise scaling
        esn.configure_noise_type('variance_scaled')  # Noise âˆ input variance
        ```
        
        âš¡ **Performance Guidelines**:
        - ðŸŽ¯ **input_noise**: Recommended by Jaeger, improves robustness
        - ðŸŽµ **additive**: Simple but effective, use small noise_level (0.001-0.01)
        - âš¡ **multiplicative**: Good for dynamic systems, models realistic variations
        - ðŸŒŠ **correlated**: Most realistic, but computationally expensive
        - ðŸŽ“ **training_vs_testing**: Essential for robustness evaluation
        - ðŸ“Š **variance_scaled**: Automatically adapts to signal strength
        
        ðŸ“– **Research Reference**: Jaeger (2001) "The Echo State Approach" - Section 2.3
        
        Args:
            noise_type (str): Noise implementation strategy (6 options above)
            correlation_length (int): Spatial correlation length for 'correlated' noise
            training_ratio (float): Training/testing noise ratio for 'training_vs_testing'
            
        Raises:
            ValueError: If noise_type is not one of the 6 valid options
            
        Example:
            >>> esn = EchoStateNetwork(noise_level=0.01)
            >>> esn.configure_noise_type('input_noise')  # Jaeger's recommendation
            âœ“ Noise type set to: input_noise
        """
        valid_types = ['additive', 'input_noise', 'multiplicative', 'correlated', 'training_vs_testing', 'variance_scaled']
        if noise_type not in valid_types:
            raise ValueError(f"Invalid noise type. Choose from: {valid_types}")
        self.noise_type = noise_type
        self.noise_correlation_length = correlation_length
        self.training_noise_ratio = training_ratio
        print(f"âœ“ Noise type set to: {noise_type}")

    def configure_state_collection_method(self, method: str):
        """
        ðŸ“Š Configure State Collection Strategy - 7 Advanced Methods for Optimal Training
        
        ðŸ”¬ **Research Background**: Different state collection strategies can dramatically
        affect training efficiency and model performance. This method implements advanced
        techniques for optimal reservoir state utilization.
        
        ðŸ“ˆ **State Collection Methods**:
        ```
        ðŸ” STATE COLLECTION COMPARISON
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Method       â”‚   Description    â”‚   Computation   â”‚   Best For      â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ all_states       â”‚ Use every state  â”‚ O(n*T)         â”‚ Small datasets  â”‚
        â”‚ subsampled       â”‚ Every nth state  â”‚ O(n*T/k)       â”‚ Large datasets  â”‚
        â”‚ exponential      â”‚ Exp. weighting   â”‚ O(n*T)         â”‚ Recent focus    â”‚
        â”‚ multi_horizon    â”‚ Multiple delays  â”‚ O(n*T*k)       â”‚ Long memory     â”‚
        â”‚ adaptive_spacing â”‚ Dynamic sampling â”‚ O(n*T)         â”‚ Non-uniform     â”‚
        â”‚ adaptive_washout â”‚ Smart washout    â”‚ O(n*T)         â”‚ Fast convergenceâ”‚
        â”‚ ensemble_washout â”‚ Multi-washout    â”‚ O(n*T*k)       â”‚ Robustness      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ```
        
        Args:
            method (str): State collection method from the 7 options above
            
        Example:
            >>> esn.configure_state_collection_method('adaptive_spacing')
            âœ“ State collection method set to: adaptive_spacing
        """
        valid_methods = ['all_states', 'subsampled', 'exponential', 'multi_horizon', 'adaptive_spacing', 'adaptive_washout', 'ensemble_washout']
        if method not in valid_methods:
            raise ValueError(f"Invalid state collection method. Choose from: {valid_methods}")
        self.state_collection_method = method
        print(f"âœ“ State collection method set to: {method}")

    def configure_training_solver(self, solver: str):
        """
        ðŸ”§ Configure Training Solver - 4 Advanced Optimization Methods
        
        Args:
            solver (str): Training solver - 'ridge', 'pseudo_inverse', 'lsqr', 'elastic_net'
            
        Example:
            >>> esn.configure_training_solver('ridge')
            âœ“ Training solver set to: ridge
        """
        valid_solvers = ['ridge', 'pseudo_inverse', 'lsqr', 'elastic_net']
        if solver not in valid_solvers:
            raise ValueError(f"Invalid training solver. Choose from: {valid_solvers}")
        self.training_solver = solver
        print(f"âœ“ Training solver set to: {solver}")

    def configure_output_feedback(self, mode: str, sparsity: float = 0.1, enable: bool = True):
        """
        ðŸ”„ Configure Output Feedback - 4 Advanced Modes from Jaeger 2001!
        
        ðŸ”¬ **Research Background**: Jaeger (2001) Figure 1 shows output feedback as crucial 
        for recurrent systems. This method implements all feedback strategies from the paper, 
        enabling the full power of Echo State Networks with teacher forcing!
        
        ðŸ“Š **Feedback Modes Visual Guide**:
        ```
        ðŸ”„ OUTPUT FEEDBACK COMPARISON  
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Feedback Mode   â”‚   Connection     â”‚   Computation   â”‚   Best For       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ ðŸŽ¯ direct       â”‚ All â†’ all        â”‚ W_back @ y(t)   â”‚ Full recurrence  â”‚
        â”‚ âš¡ sparse       â”‚ Few â†’ few        â”‚ Sparse W_back   â”‚ Fast computation â”‚  
        â”‚ ðŸ“ scaled_uniformâ”‚ Scaled uniform   â”‚ Î± * y(t)        â”‚ Simple control   â”‚
        â”‚ ðŸ—ï¸ hierarchical â”‚ Layer-wise       â”‚ Hierarchical    â”‚ Complex dynamics â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ```
        
        Args:
            mode (str): Feedback mode ('direct', 'sparse', 'scaled_uniform', 'hierarchical')
            sparsity (float): Connection sparsity for 'sparse' mode
            enable (bool): Whether to enable output feedback
            
        Example:
            >>> esn.configure_output_feedback('sparse', sparsity=0.2)
            âœ“ Output feedback configured: sparse (sparsity=0.2)
        """
        valid_modes = ['direct', 'sparse', 'scaled_uniform', 'hierarchical']
        if mode not in valid_modes:
            raise ValueError(f"Invalid feedback mode. Choose from: {valid_modes}")
        
        self.output_feedback_mode = mode
        self.output_feedback_sparsity = sparsity
        self.output_feedback_enabled = enable
        
        if enable:
            print(f"âœ“ Output feedback configured: {mode} (sparsity={sparsity})")
        else:
            print("âœ“ Output feedback disabled")

    def configure_leaky_integration(self, mode: str, custom_rates=None):
        """
        â° Configure Leaky Integration - 4 Advanced Temporal Memory Modes
        
        Args:
            mode (str): Leaky integration mode ('none', 'uniform', 'adaptive', 'custom')
            custom_rates: Custom leak rates for 'custom' mode
            
        Example:
            >>> esn.configure_leaky_integration('adaptive')
            âœ“ Leaky integration set to: adaptive
        """
        valid_modes = ['none', 'uniform', 'adaptive', 'custom']
        if mode not in valid_modes:
            raise ValueError(f"Invalid leaky integration mode. Choose from: {valid_modes}")
        
        self.leaky_integration_mode = mode
        if mode == 'custom' and custom_rates is not None:
            self.custom_leak_rates = custom_rates
        print(f"âœ“ Leaky integration set to: {mode}")

    def configure_bias_terms(self, bias_type: str, scale: float = 0.1):
        """
        âš–ï¸ Configure Bias Terms - 4 Advanced Bias Strategies
        
        Args:
            bias_type (str): Bias type ('none', 'random', 'learned', 'adaptive')
            scale (float): Bias scaling factor
            
        Example:
            >>> esn.configure_bias_terms('adaptive', scale=0.2)
            âœ“ Bias terms configured: adaptive (scale=0.2)
        """
        valid_types = ['none', 'random', 'learned', 'adaptive']
        if bias_type not in valid_types:
            raise ValueError(f"Invalid bias type. Choose from: {valid_types}")
        
        self.bias_type = bias_type
        self.bias_scale = scale
        print(f"âœ“ Bias terms configured: {bias_type} (scale={scale})")

    def set_training_mode(self, training: bool = True):
        """
        ðŸŽ“ Set Training Mode - Toggle Between Training and Inference
        
        Args:
            training (bool): Whether the network is in training mode
            
        Example:
            >>> esn.set_training_mode(False)  # Switch to inference mode
            âœ“ Training mode: False
        """
        self.training_mode = training
        print(f"âœ“ Training mode: {training}")

    def enable_sparse_computation(self, threshold: float = 1e-6):
        """
        âš¡ Enable Sparse Computation - Optimize Memory and Speed
        
        Args:
            threshold (float): Sparsity threshold for matrix operations
            
        Example:
            >>> esn.enable_sparse_computation(threshold=1e-5)
            âœ“ Sparse computation enabled with threshold: 1e-05
        """
        self.sparse_computation = True
        self.sparse_threshold = threshold
        print(f"âœ“ Sparse computation enabled with threshold: {threshold}")

    def get_configuration_summary(self) -> dict:
        """
        ðŸ“‹ Get Configuration Summary - Complete Overview of All Settings
        
        Returns:
            dict: Complete configuration summary
            
        Example:
            >>> summary = esn.get_configuration_summary()
            >>> print(f"Activation: {summary['activation_function']}")
        """
        summary = {
            'activation_function': getattr(self, 'activation_function', 'tanh'),
            'noise_type': getattr(self, 'noise_type', 'additive'),
            'state_collection_method': getattr(self, 'state_collection_method', 'all_states'),
            'training_solver': getattr(self, 'training_solver', 'ridge'),
            'output_feedback_mode': getattr(self, 'output_feedback_mode', 'none'),
            'output_feedback_enabled': getattr(self, 'output_feedback_enabled', False),
            'leaky_integration_mode': getattr(self, 'leaky_integration_mode', 'none'),
            'bias_type': getattr(self, 'bias_type', 'none'),
            'training_mode': getattr(self, 'training_mode', True),
            'sparse_computation': getattr(self, 'sparse_computation', False)
        }
        return summary