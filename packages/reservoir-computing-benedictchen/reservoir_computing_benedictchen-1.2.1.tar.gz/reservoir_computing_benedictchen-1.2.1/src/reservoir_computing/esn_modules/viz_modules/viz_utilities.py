"""
ğŸ”§ Reservoir Computing - Visualization Utilities Module
======================================================


Author: Benedict Chen (benedict@benedictchen.com)
Based on: Jaeger, H. (2001) "The Echo State Approach to Analysing and Training Recurrent Neural Networks"

ğŸ¯ MODULE PURPOSE:
=================
Utility functions and helper methods for visualization including statistical analysis,
performance assessment, memory capacity estimation, and comprehensive reporting
functions shared across all visualization modules.

ğŸ“Š UTILITY CAPABILITIES:
========================
â€¢ Statistical analysis and reporting functions
â€¢ Performance assessment and classification
â€¢ Memory capacity estimation algorithms
â€¢ Spectral stability and condition number analysis
â€¢ Comprehensive reporting and summary generation
â€¢ Mathematical utility functions for visualization

ğŸ”¬ RESEARCH FOUNDATION:
======================
Based on established evaluation metrics from:
- Jaeger (2001): Original ESN evaluation methodologies
- LukoÅ¡eviÄius & Jaeger (2009): Comprehensive reservoir analysis metrics
- Verstraeten et al. (2007): Memory capacity and evaluation techniques
- Standard statistical and numerical analysis methods

This module provides the foundational utility functions,
split from the 1438-line monolith for shared visualization support.
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from typing import Optional, Tuple, Dict, Any, List, Union
import warnings
from abc import ABC

# Configure professional plotting style
plt.style.use('seaborn-v0_8-whitegrid')

class VizUtilitiesMixin(ABC):
    """
    ğŸ”§ Visualization Utilities Mixin
    
    Provides essential utility functions and helper methods
    for comprehensive visualization and analysis support.
    """

    def _print_reservoir_statistics(self, eigenvals: np.ndarray, degrees: np.ndarray, weights: np.ndarray):
        """
        ğŸ“Š Print Comprehensive Reservoir Statistics
        
        Provides detailed statistical analysis of reservoir properties
        including spectral characteristics and connectivity patterns.
        
        Args:
            eigenvals: Eigenvalues of reservoir matrix
            degrees: Connection degree distribution
            weights: Non-zero weight values
        """
        print("\n" + "="*70)
        print("ğŸ—ï¸  RESERVOIR STRUCTURE ANALYSIS")
        print("="*70)
        
        # Basic structure information
        n_reservoir = len(eigenvals)
        sparsity = 1.0 - (len(weights) / (n_reservoir * n_reservoir))
        
        print(f"ğŸ“ Matrix Dimensions: {n_reservoir}Ã—{n_reservoir}")
        print(f"ğŸ•¸ï¸  Sparsity Level: {sparsity:.1%}")
        print(f"ğŸ”— Total Connections: {len(weights)}")
        print(f"ğŸ’ª Connection Density: {(1-sparsity):.1%}")
        
        # Spectral properties
        spectral_radius = np.max(np.abs(eigenvals))
        print(f"\nğŸŒŒ SPECTRAL ANALYSIS:")
        print(f"   â€¢ Spectral Radius: {spectral_radius:.6f}")
        print(f"   â€¢ Echo State Property: {'âœ“ Satisfied' if spectral_radius < 1.0 else 'âš  Violated'}")
        print(f"   â€¢ Number of Eigenvalues: {len(eigenvals)}")
        print(f"   â€¢ Complex Eigenvalues: {np.sum(np.imag(eigenvals) != 0)}")
        
        # Connection statistics  
        print(f"\nğŸ”— CONNECTION ANALYSIS:")
        print(f"   â€¢ Mean Degree: {degrees.mean():.2f}")
        print(f"   â€¢ Degree Std: {degrees.std():.2f}")
        print(f"   â€¢ Max Degree: {degrees.max()}")
        print(f"   â€¢ Min Degree: {degrees.min()}")
        
        # Weight statistics
        print(f"\nâš–ï¸  WEIGHT ANALYSIS:")
        print(f"   â€¢ Mean Weight: {weights.mean():.4f}")
        print(f"   â€¢ Weight Std: {weights.std():.4f}")
        print(f"   â€¢ Weight Range: [{weights.min():.4f}, {weights.max():.4f}]")
        print(f"   â€¢ Weight Skewness: {stats.skew(weights):.4f}")
        print(f"   â€¢ Weight Kurtosis: {stats.kurtosis(weights):.4f}")
        
        print("="*70)

    def _print_dynamics_statistics(self, states: np.ndarray, inputs: Optional[np.ndarray], 
                                 outputs: Optional[np.ndarray]):
        """
        ğŸ“Š Print Comprehensive Dynamics Statistics
        
        Provides detailed statistical analysis of temporal behavior
        including memory capacity and dynamic range analysis.
        
        Args:
            states: Reservoir state matrix (time_steps Ã— n_reservoir)
            inputs: Input sequence (optional)
            outputs: Output sequence (optional)
        """
        print("\n" + "="*70)
        print("ğŸŒŠ RESERVOIR DYNAMICS ANALYSIS")
        print("="*70)
        
        # Basic dynamics information
        T, N = states.shape
        print(f"â±ï¸  Time Steps: {T}")
        print(f"ğŸ§  Reservoir Size: {N}")
        print(f"ğŸ“Š State Matrix Size: {T}Ã—{N}")
        
        # Activity statistics
        mean_activity = np.mean(states, axis=0)
        std_activity = np.std(states, axis=0)
        max_activity = np.max(np.abs(states))
        
        print(f"\nğŸ¯ ACTIVITY ANALYSIS:")
        print(f"   â€¢ Mean Activity Range: [{mean_activity.min():.4f}, {mean_activity.max():.4f}]")
        print(f"   â€¢ Activity Std Range: [{std_activity.min():.4f}, {std_activity.max():.4f}]")
        print(f"   â€¢ Maximum |Activity|: {max_activity:.4f}")
        print(f"   â€¢ Active Neurons: {np.sum(std_activity > 0.001)}/{N} ({np.sum(std_activity > 0.001)/N:.1%})")
        
        # Temporal characteristics
        print(f"\nâ° TEMPORAL CHARACTERISTICS:")
        print(f"   â€¢ Mean State Norm: {np.mean(np.linalg.norm(states, axis=1)):.4f}")
        print(f"   â€¢ State Norm Std: {np.std(np.linalg.norm(states, axis=1)):.4f}")
        
        # Memory capacity estimation (simplified)
        if T > 10:
            memory_capacity = self._estimate_memory_capacity(states)
            print(f"   â€¢ Estimated Memory Capacity: {memory_capacity:.2f}")
        
        # Input-output relationships
        if inputs is not None:
            print(f"\nğŸ”„ INPUT-OUTPUT RELATIONSHIPS:")
            print(f"   â€¢ Input Dimensions: {inputs.shape[1] if inputs.ndim > 1 else 1}")
            
            # Cross-correlation analysis
            if inputs.ndim > 1:
                input_state_corr = np.mean([np.corrcoef(inputs[:, i], np.mean(states, axis=1))[0,1] 
                                           for i in range(inputs.shape[1]) if not np.isnan(np.corrcoef(inputs[:, i], np.mean(states, axis=1))[0,1])])
            else:
                input_state_corr = np.corrcoef(inputs, np.mean(states, axis=1))[0,1]
            
            if not np.isnan(input_state_corr):
                print(f"   â€¢ Mean Input-State Correlation: {input_state_corr:.4f}")
        
        if outputs is not None:
            print(f"   â€¢ Output Dimensions: {outputs.shape[1] if outputs.ndim > 1 else 1}")
        
        print("="*70)

    def _print_performance_statistics(self, predictions: np.ndarray, targets: np.ndarray, errors: np.ndarray):
        """
        ğŸ“Š Print Comprehensive Performance Statistics
        
        Provides detailed statistical analysis of model performance
        including accuracy metrics and error characteristics.
        
        Args:
            predictions: Model predictions array
            targets: Ground truth target values  
            errors: Prediction errors (predictions - targets)
        """
        print("\n" + "="*70)
        print("ğŸ“Š PERFORMANCE ANALYSIS SUMMARY")
        print("="*70)
        
        # Basic metrics
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        
        print(f"ğŸ¯ ACCURACY METRICS:")
        print(f"   â€¢ RÂ² Score: {r2:.6f}")
        print(f"   â€¢ Mean Squared Error: {mse:.6f}")
        print(f"   â€¢ Root Mean Squared Error: {rmse:.6f}")
        print(f"   â€¢ Mean Absolute Error: {mae:.6f}")
        
        # Error statistics
        print(f"\nğŸ“ˆ ERROR ANALYSIS:")
        print(f"   â€¢ Mean Error: {np.mean(errors):.6f}")
        print(f"   â€¢ Error Standard Deviation: {np.std(errors):.6f}")
        print(f"   â€¢ Maximum Absolute Error: {np.max(np.abs(errors)):.6f}")
        print(f"   â€¢ Error Skewness: {stats.skew(errors.flatten()):.4f}")
        print(f"   â€¢ Error Kurtosis: {stats.kurtosis(errors.flatten()):.4f}")
        
        # Performance assessment
        performance_level = self._assess_performance_level(r2, rmse)
        print(f"\nğŸ† PERFORMANCE LEVEL: {performance_level}")
        
        # Data characteristics
        print(f"\nğŸ“Š DATA CHARACTERISTICS:")
        print(f"   â€¢ Sample Size: {len(predictions)}")
        print(f"   â€¢ Target Range: [{np.min(targets):.4f}, {np.max(targets):.4f}]")
        print(f"   â€¢ Prediction Range: [{np.min(predictions):.4f}, {np.max(predictions):.4f}]")
        
        print("="*70)

    def _print_comparative_summary(self, results: Dict[str, Dict[str, Any]]):
        """
        ğŸ“Š Print Comprehensive Comparative Analysis Summary
        
        Provides detailed comparison across multiple configurations
        including performance rankings and statistical analysis.
        
        Args:
            results: Dictionary with configuration names as keys and metrics as values
        """
        print("\n" + "="*70)
        print("ğŸ† COMPARATIVE ANALYSIS SUMMARY")
        print("="*70)
        
        print(f"ğŸ“Š Configurations Analyzed: {len(results)}")
        
        for config_name, metrics in results.items():
            print(f"\nğŸ”§ {config_name}:")
            for metric_name, metric_value in metrics.items():
                if isinstance(metric_value, (list, np.ndarray)):
                    if len(metric_value) > 0:
                        print(f"   â€¢ {metric_name}: {np.mean(metric_value):.4f} Â± {np.std(metric_value):.4f}")
                elif isinstance(metric_value, (int, float)):
                    print(f"   â€¢ {metric_name}: {metric_value:.4f}")
        
        print("="*70)

    def _print_spectral_statistics(self, eigenvals: np.ndarray, singular_vals: np.ndarray, condition_number: float):
        """
        ğŸ“Š Print Comprehensive Spectral Analysis Statistics
        
        Provides detailed mathematical analysis of spectral properties
        including stability assessment and condition analysis.
        
        Args:
            eigenvals: Eigenvalues of the reservoir matrix
            singular_vals: Singular values from SVD
            condition_number: Matrix condition number
        """
        print("\n" + "="*70)
        print("ğŸŒŒ SPECTRAL ANALYSIS SUMMARY")
        print("="*70)
        
        spectral_radius = np.max(np.abs(eigenvals))
        
        print(f"ğŸ¯ EIGENVALUE ANALYSIS:")
        print(f"   â€¢ Spectral Radius: {spectral_radius:.6f}")
        print(f"   â€¢ Total Eigenvalues: {len(eigenvals)}")
        print(f"   â€¢ Complex Eigenvalues: {np.sum(np.imag(eigenvals) != 0)}")
        print(f"   â€¢ Eigenvalues > 1: {np.sum(np.abs(eigenvals) > 1.0)}")
        print(f"   â€¢ Stability Status: {self._assess_spectral_stability(eigenvals)}")
        
        print(f"\nğŸ“ SINGULAR VALUE ANALYSIS:")
        print(f"   â€¢ Condition Number: {condition_number:.2e}")
        print(f"   â€¢ Numerical Status: {self._assess_condition_number(condition_number)}")
        print(f"   â€¢ Largest Singular Value: {np.max(singular_vals):.6f}")
        print(f"   â€¢ Smallest Singular Value: {np.min(singular_vals):.6f}")
        print(f"   â€¢ Effective Rank (1% threshold): {np.sum(singular_vals > 0.01 * np.max(singular_vals))}")
        
        print("="*70)

    def _assess_performance_level(self, r2: float, rmse: float) -> str:
        """
        ğŸ¯ Assess Performance Level Based on Metrics
        
        Provides qualitative assessment of model performance based on
        standard machine learning evaluation criteria.
        
        Args:
            r2: R-squared coefficient of determination
            rmse: Root mean squared error
            
        Returns:
            str: Performance level description with emoji
        """
        if r2 >= 0.95:
            return "ğŸŒŸ EXCELLENT (RÂ² â‰¥ 0.95)"
        elif r2 >= 0.90:
            return "ğŸ¯ VERY GOOD (RÂ² â‰¥ 0.90)"
        elif r2 >= 0.80:
            return "ğŸ‘ GOOD (RÂ² â‰¥ 0.80)"
        elif r2 >= 0.60:
            return "âš ï¸  FAIR (RÂ² â‰¥ 0.60)"
        else:
            return "âŒ POOR (RÂ² < 0.60)"

    def _assess_spectral_stability(self, eigenvals: np.ndarray) -> str:
        """
        ğŸ” Assess Spectral Stability Based on Eigenvalue Distribution
        
        Evaluates the Echo State Property and stability characteristics
        based on the eigenvalue spectrum of the reservoir matrix.
        
        Args:
            eigenvals: Array of eigenvalues
            
        Returns:
            str: Stability assessment description
        """
        spectral_radius = np.max(np.abs(eigenvals))
        
        if spectral_radius < 1.0:
            return "âœ“ Stable (Echo State Property)"
        elif spectral_radius < 1.1:
            return "âš  Marginally Stable"
        else:
            return "âŒ Unstable"

    def _assess_condition_number(self, condition_number: float) -> str:
        """
        ğŸ” Assess Numerical Condition Based on Condition Number
        
        Evaluates the numerical stability and conditioning of the
        reservoir matrix based on its condition number.
        
        Args:
            condition_number: Matrix condition number (Ïƒ_max / Ïƒ_min)
            
        Returns:
            str: Condition assessment description
        """
        if condition_number < 1e6:
            return "âœ“ Well-Conditioned"
        elif condition_number < 1e12:
            return "âš  Moderately Ill-Conditioned"
        else:
            return "âŒ Severely Ill-Conditioned"

    def _estimate_memory_capacity(self, states: np.ndarray) -> float:
        """
        ğŸ§  Estimate Memory Capacity of Reservoir
        
        Provides simplified estimation of memory capacity based on
        linear memory capacity measure using autocorrelation decay.
        
        Args:
            states: Reservoir state matrix (time_steps Ã— n_reservoir)
            
        Returns:
            float: Estimated memory capacity in time steps
            
        Research Background:
        ===================
        Based on linear memory capacity measure from Jaeger (2001) and
        extended analysis methods from Verstraeten et al. (2007).
        """
        # Simple approximation based on state autocorrelation decay
        mean_state = np.mean(states, axis=1)
        
        # Handle edge cases
        if len(mean_state) < 10:
            return 1.0
            
        try:
            # Compute autocorrelation
            autocorr = np.correlate(mean_state, mean_state, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            
            # Normalize by zero-lag value
            if autocorr[0] != 0:
                autocorr = autocorr / autocorr[0]
            else:
                return 1.0
            
            # Find decay to 1/e
            decay_threshold = 1/np.e
            decay_indices = np.where(autocorr < decay_threshold)[0]
            
            if len(decay_indices) > 0:
                memory_capacity = float(decay_indices[0])
            else:
                memory_capacity = float(len(autocorr) - 1)
                
        except Exception:
            # Fallback to simple estimate
            memory_capacity = min(10.0, len(mean_state) / 4)
            
        return memory_capacity

    def _calculate_statistical_significance(self, group1: np.ndarray, group2: np.ndarray) -> Tuple[float, str]:
        """
        ğŸ“Š Calculate Statistical Significance Between Two Groups
        
        Performs appropriate statistical test to determine if there's a
        significant difference between two performance groups.
        
        Args:
            group1: First group of values
            group2: Second group of values
            
        Returns:
            Tuple[float, str]: (p_value, significance_description)
        """
        try:
            # Perform t-test for independent samples
            t_stat, p_value = stats.ttest_ind(group1, group2)
            
            if p_value < 0.001:
                significance = "Highly Significant (p < 0.001)"
            elif p_value < 0.01:
                significance = "Very Significant (p < 0.01)"
            elif p_value < 0.05:
                significance = "Significant (p < 0.05)"
            elif p_value < 0.1:
                significance = "Marginally Significant (p < 0.1)"
            else:
                significance = "Not Significant (p â‰¥ 0.1)"
                
            return p_value, significance
            
        except Exception:
            return np.nan, "Unable to calculate significance"

    def _normalize_metrics_for_comparison(self, metrics_dict: Dict[str, List[float]]) -> Dict[str, List[float]]:
        """
        ğŸ“ Normalize Metrics for Fair Comparison
        
        Normalizes different metrics to [0, 1] scale for comparative analysis,
        handling both "higher is better" and "lower is better" metrics.
        
        Args:
            metrics_dict: Dictionary of metric names to value lists
            
        Returns:
            Dict[str, List[float]]: Normalized metrics dictionary
        """
        normalized_metrics = {}
        
        # Define metrics where lower is better
        lower_is_better = ['mse', 'rmse', 'mae', 'error', 'loss']
        
        for metric_name, values in metrics_dict.items():
            values = np.array(values)
            
            if len(values) == 0 or np.all(np.isnan(values)):
                normalized_metrics[metric_name] = values
                continue
                
            # Remove NaN values for normalization
            valid_values = values[~np.isnan(values)]
            
            if len(valid_values) <= 1:
                normalized_metrics[metric_name] = np.ones_like(values) * 0.5
                continue
                
            # Normalize based on metric type
            val_min, val_max = np.min(valid_values), np.max(valid_values)
            
            if val_max == val_min:
                normalized_values = np.ones_like(valid_values) * 0.5
            elif any(term in metric_name.lower() for term in lower_is_better):
                # Lower is better - invert normalization
                normalized_values = 1 - (valid_values - val_min) / (val_max - val_min)
            else:
                # Higher is better - standard normalization
                normalized_values = (valid_values - val_min) / (val_max - val_min)
            
            # Reconstruct with NaN preservation
            result = np.full_like(values, np.nan, dtype=float)
            result[~np.isnan(values)] = normalized_values
            normalized_metrics[metric_name] = result.tolist()
            
        return normalized_metrics

# Export the main class
__all__ = ['VizUtilitiesMixin']