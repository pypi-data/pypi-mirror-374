# Dify Dataset SDK API å®Œæ•´å‚è€ƒæ–‡æ¡£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº† Dify Dataset SDK çš„æ‰€æœ‰ 39 ä¸ª API æ–¹æ³•ï¼Œæ¶µç›–æ•°æ®é›†ç®¡ç†ã€æ–‡æ¡£å¤„ç†ã€ç‰‡æ®µç®¡ç†ã€å…ƒæ•°æ®æ“ä½œã€çŸ¥è¯†æ ‡ç­¾å’Œé«˜çº§æ£€ç´¢ç­‰åŠŸèƒ½ï¼Œå¹¶æä¾›å®Œæ•´çš„å‚æ•°è¯´æ˜å’Œé«˜çº§ä½¿ç”¨ç¤ºä¾‹ã€‚

## ç›®å½•

- [Dify Dataset SDK API å®Œæ•´å‚è€ƒæ–‡æ¡£](#dify-dataset-sdk-api-å®Œæ•´å‚è€ƒæ–‡æ¡£)
  - [ç›®å½•](#ç›®å½•)
  - [1. ç®€ä»‹](#1-ç®€ä»‹)
    - [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
    - [æ”¯æŒçš„æ–‡ä»¶æ ¼å¼](#æ”¯æŒçš„æ–‡ä»¶æ ¼å¼)
  - [2. å¿«é€Ÿå¼€å§‹](#2-å¿«é€Ÿå¼€å§‹)
    - [å®‰è£…](#å®‰è£…)
    - [åŸºæœ¬ä½¿ç”¨](#åŸºæœ¬ä½¿ç”¨)
  - [3. API åˆ†ç±»æ¦‚è§ˆ](#3-api-åˆ†ç±»æ¦‚è§ˆ)
  - [4. æ•°æ®é›†ç®¡ç† (5 ä¸ª API)](#4-æ•°æ®é›†ç®¡ç†-5-ä¸ª-api)
    - [4.1 åˆ›å»ºæ•°æ®é›†](#41-åˆ›å»ºæ•°æ®é›†)
    - [4.2 è·å–æ•°æ®é›†åˆ—è¡¨](#42-è·å–æ•°æ®é›†åˆ—è¡¨)
    - [4.3 è·å–æ•°æ®é›†è¯¦æƒ…](#43-è·å–æ•°æ®é›†è¯¦æƒ…)
    - [4.4 æ›´æ–°æ•°æ®é›†](#44-æ›´æ–°æ•°æ®é›†)
    - [4.5 åˆ é™¤æ•°æ®é›†](#45-åˆ é™¤æ•°æ®é›†)
  - [5. æ–‡æ¡£ç®¡ç† (9 ä¸ª API)](#5-æ–‡æ¡£ç®¡ç†-9-ä¸ª-api)
    - [5.1 ä»æ–‡æœ¬åˆ›å»ºæ–‡æ¡£](#51-ä»æ–‡æœ¬åˆ›å»ºæ–‡æ¡£)
    - [5.2 ä»æ–‡ä»¶åˆ›å»ºæ–‡æ¡£](#52-ä»æ–‡ä»¶åˆ›å»ºæ–‡æ¡£)
    - [5.3 è·å–æ–‡æ¡£åˆ—è¡¨](#53-è·å–æ–‡æ¡£åˆ—è¡¨)
    - [5.4 è·å–æ–‡æ¡£è¯¦æƒ…](#54-è·å–æ–‡æ¡£è¯¦æƒ…)
    - [5.5 é€šè¿‡æ–‡æœ¬æ›´æ–°æ–‡æ¡£](#55-é€šè¿‡æ–‡æœ¬æ›´æ–°æ–‡æ¡£)
    - [5.6 è·å–æ–‡æ¡£ç´¢å¼•çŠ¶æ€](#56-è·å–æ–‡æ¡£ç´¢å¼•çŠ¶æ€)
    - [5.7 åˆ é™¤æ–‡æ¡£](#57-åˆ é™¤æ–‡æ¡£)
    - [5.8 é€šè¿‡æ–‡ä»¶æ›´æ–°æ–‡æ¡£](#58-é€šè¿‡æ–‡ä»¶æ›´æ–°æ–‡æ¡£)
    - [5.9 åˆ é™¤æ–‡æ¡£](#59-åˆ é™¤æ–‡æ¡£)
  - [6. æ–‡æ¡£æ‰¹é‡æ“ä½œ (1 ä¸ª API)](#6-æ–‡æ¡£æ‰¹é‡æ“ä½œ-1-ä¸ª-api)
    - [6.1 æ‰¹é‡æ›´æ–°æ–‡æ¡£çŠ¶æ€](#61-æ‰¹é‡æ›´æ–°æ–‡æ¡£çŠ¶æ€)
  - [7. ç‰‡æ®µç®¡ç† (5 ä¸ª API)](#7-ç‰‡æ®µç®¡ç†-5-ä¸ª-api)
    - [7.1 åˆ›å»ºç‰‡æ®µ](#71-åˆ›å»ºç‰‡æ®µ)
    - [7.2 è·å–ç‰‡æ®µåˆ—è¡¨](#72-è·å–ç‰‡æ®µåˆ—è¡¨)
    - [7.3 è·å–ç‰‡æ®µè¯¦æƒ…](#73-è·å–ç‰‡æ®µè¯¦æƒ…)
    - [7.4 æ›´æ–°ç‰‡æ®µ](#74-æ›´æ–°ç‰‡æ®µ)
    - [7.5 åˆ é™¤ç‰‡æ®µ](#75-åˆ é™¤ç‰‡æ®µ)
  - [8. å­ç‰‡æ®µç®¡ç† (4 ä¸ª API)](#8-å­ç‰‡æ®µç®¡ç†-4-ä¸ª-api)
    - [8.1 åˆ›å»ºå­ç‰‡æ®µ](#81-åˆ›å»ºå­ç‰‡æ®µ)
    - [8.2 è·å–å­ç‰‡æ®µåˆ—è¡¨](#82-è·å–å­ç‰‡æ®µåˆ—è¡¨)
    - [8.3 æ›´æ–°å­ç‰‡æ®µ](#83-æ›´æ–°å­ç‰‡æ®µ)
    - [8.4 åˆ é™¤å­ç‰‡æ®µ](#84-åˆ é™¤å­ç‰‡æ®µ)
  - [9. çŸ¥è¯†åº“æ£€ç´¢ (1 ä¸ª API)](#9-çŸ¥è¯†åº“æ£€ç´¢-1-ä¸ª-api)
    - [9.1 æ£€ç´¢çŸ¥è¯†åº“å†…å®¹](#91-æ£€ç´¢çŸ¥è¯†åº“å†…å®¹)
  - [10. æ–‡ä»¶ç®¡ç† (1 ä¸ª API)](#10-æ–‡ä»¶ç®¡ç†-1-ä¸ª-api)
    - [10.1 è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯](#101-è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯)
  - [11. å…ƒæ•°æ®ç®¡ç† (6 ä¸ª API)](#11-å…ƒæ•°æ®ç®¡ç†-6-ä¸ª-api)
    - [11.1 åˆ›å»ºå…ƒæ•°æ®å­—æ®µ](#111-åˆ›å»ºå…ƒæ•°æ®å­—æ®µ)
    - [11.2 è·å–å…ƒæ•°æ®å­—æ®µåˆ—è¡¨](#112-è·å–å…ƒæ•°æ®å­—æ®µåˆ—è¡¨)
    - [11.3 æ›´æ–°å…ƒæ•°æ®å­—æ®µ](#113-æ›´æ–°å…ƒæ•°æ®å­—æ®µ)
    - [11.4 åˆ é™¤å…ƒæ•°æ®å­—æ®µ](#114-åˆ é™¤å…ƒæ•°æ®å­—æ®µ)
    - [11.5 æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®](#115-æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®)
    - [11.6 åˆ‡æ¢å†…ç½®å…ƒæ•°æ®å­—æ®µ](#116-åˆ‡æ¢å†…ç½®å…ƒæ•°æ®å­—æ®µ)
  - [12. çŸ¥è¯†æ ‡ç­¾ç®¡ç† (7 ä¸ª API)](#12-çŸ¥è¯†æ ‡ç­¾ç®¡ç†-7-ä¸ª-api)
    - [12.1 åˆ›å»ºçŸ¥è¯†æ ‡ç­¾](#121-åˆ›å»ºçŸ¥è¯†æ ‡ç­¾)
    - [12.2 ç»‘å®šæ•°æ®é›†åˆ°æ ‡ç­¾](#122-ç»‘å®šæ•°æ®é›†åˆ°æ ‡ç­¾)
    - [12.3 è·å–çŸ¥è¯†æ ‡ç­¾åˆ—è¡¨](#123-è·å–çŸ¥è¯†æ ‡ç­¾åˆ—è¡¨)
    - [12.4 æ›´æ–°çŸ¥è¯†æ ‡ç­¾](#124-æ›´æ–°çŸ¥è¯†æ ‡ç­¾)
    - [12.5 åˆ é™¤çŸ¥è¯†æ ‡ç­¾](#125-åˆ é™¤çŸ¥è¯†æ ‡ç­¾)
    - [12.6 è§£ç»‘æ•°æ®é›†æ ‡ç­¾](#126-è§£ç»‘æ•°æ®é›†æ ‡ç­¾)
    - [12.7 è·å–æ•°æ®é›†æ ‡ç­¾](#127-è·å–æ•°æ®é›†æ ‡ç­¾)
  - [13. åµŒå…¥æ¨¡å‹ç®¡ç† (1 ä¸ª API)](#13-åµŒå…¥æ¨¡å‹ç®¡ç†-1-ä¸ª-api)
    - [13.1 è·å–å¯ç”¨çš„åµŒå…¥æ¨¡å‹åˆ—è¡¨](#131-è·å–å¯ç”¨çš„åµŒå…¥æ¨¡å‹åˆ—è¡¨)
  - [14. é”™è¯¯å¤„ç†](#14-é”™è¯¯å¤„ç†)
    - [é«˜çº§é”™è¯¯å¤„ç†](#é«˜çº§é”™è¯¯å¤„ç†)
  - [15. æ€§èƒ½ä¼˜åŒ–å»ºè®®](#15-æ€§èƒ½ä¼˜åŒ–å»ºè®®)
    - [å¹¶å‘å¤„ç†ç¤ºä¾‹](#å¹¶å‘å¤„ç†ç¤ºä¾‹)
  - [16. æœ€ä½³å®è·µ](#16-æœ€ä½³å®è·µ)
    - [16.1 å®¢æˆ·ç«¯ç®¡ç†](#161-å®¢æˆ·ç«¯ç®¡ç†)
    - [16.2 æ‰¹é‡æ“ä½œä¼˜åŒ–](#162-æ‰¹é‡æ“ä½œä¼˜åŒ–)
    - [16.3 é«˜çº§æ£€ç´¢é…ç½®](#163-é«˜çº§æ£€ç´¢é…ç½®)
    - [16.4 ç›‘æ§å’Œæ—¥å¿—](#164-ç›‘æ§å’Œæ—¥å¿—)
  - [17. é«˜çº§åº”ç”¨åœºæ™¯](#17-é«˜çº§åº”ç”¨åœºæ™¯)
    - [17.1 ä¼ä¸šçŸ¥è¯†åº“æ„å»º](#171-ä¼ä¸šçŸ¥è¯†åº“æ„å»º)
    - [17.2 æ™ºèƒ½é—®ç­”ç³»ç»Ÿ](#172-æ™ºèƒ½é—®ç­”ç³»ç»Ÿ)
    - [17.3 å†…å®¹å®¡æ ¸ä¸è´¨é‡æ§åˆ¶](#173-å†…å®¹å®¡æ ¸ä¸è´¨é‡æ§åˆ¶)
  - [æ€»ç»“](#æ€»ç»“)

## 1. ç®€ä»‹

Dify Dataset SDK æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ Python SDKï¼Œç”¨äºä¸ Dify çŸ¥è¯†åº“ API è¿›è¡Œäº¤äº’ã€‚è¯¥ SDK æä¾›äº†å®Œæ•´çš„çŸ¥è¯†åº“ç®¡ç†èƒ½åŠ›ï¼ŒåŒ…æ‹¬æ•°æ®é›†åˆ›å»ºã€æ–‡æ¡£ä¸Šä¼ å¤„ç†ã€æ™ºèƒ½åˆ†ç‰‡ã€å…ƒæ•°æ®ç®¡ç†ã€æ ‡ç­¾ç»„ç»‡å’Œé«˜çº§æ£€ç´¢ç­‰åŠŸèƒ½ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ“š **å®Œæ•´çš„ API è¦†ç›–**ï¼šæ”¯æŒ Dify çŸ¥è¯†åº“çš„æ‰€æœ‰ 39 ä¸ª API ç«¯ç‚¹
- ğŸ”’ **ç±»å‹å®‰å…¨**ï¼šåŸºäº Pydantic çš„å®Œæ•´ç±»å‹æç¤º
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**ï¼šå…¨é¢çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- âš¡ **é«˜æ€§èƒ½**ï¼šåŸºäº httpx çš„å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
- ğŸ“„ **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒæ–‡æœ¬ã€PDFã€DOCXã€Markdown ç­‰å¤šç§æ–‡æ¡£æ ¼å¼
- ğŸ” **é«˜çº§æ£€ç´¢**ï¼šè¯­ä¹‰æœç´¢ã€å…¨æ–‡æœç´¢ã€æ··åˆæœç´¢
- ğŸ·ï¸ **æ ‡ç­¾ç®¡ç†**ï¼šçŸ¥è¯†åº“æ ‡ç­¾åŒ–ç»„ç»‡
- ğŸ“Š **å…ƒæ•°æ®ç®¡ç†**ï¼šè‡ªå®šä¹‰å…ƒæ•°æ®å­—æ®µå’Œæ–‡æ¡£å…³è”

### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

- æ–‡æœ¬æ–‡ä»¶ï¼š`.txt`, `.md`
- æ–‡æ¡£æ–‡ä»¶ï¼š`.pdf`, `.docx`
- æ•°æ®æ–‡ä»¶ï¼š`.xlsx`, `.csv`
- ç½‘é¡µæ–‡ä»¶ï¼š`.html`

## 2. å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
pip install dify-dataset-sdk
```

### åŸºæœ¬ä½¿ç”¨

```python
from dify_dataset_sdk import DifyDatasetClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = DifyDatasetClient(
    api_key="your-api-key-here",
    base_url="https://api.dify.ai",  # å¯é€‰ï¼Œé»˜è®¤å€¼
    timeout=30.0  # å¯é€‰ï¼Œé»˜è®¤ 30 ç§’
)

# åˆ›å»ºæ•°æ®é›†
dataset = client.create_dataset(
    name="æˆ‘çš„çŸ¥è¯†åº“",
    description="ç”¨äºå­˜å‚¨æŠ€æœ¯æ–‡æ¡£",
    permission="only_me"
)

# åˆ›å»ºæ–‡æ¡£
doc_response = client.create_document_by_text(
    dataset_id=dataset.id,
    name="ç¤ºä¾‹æ–‡æ¡£",
    text="è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æ¡£å†…å®¹ã€‚",
    indexing_technique="high_quality"
)

# å…³é—­å®¢æˆ·ç«¯è¿æ¥
client.close()
```

## 3. API åˆ†ç±»æ¦‚è§ˆ

| åˆ†ç±»         | API æ•°é‡ | ä¸»è¦åŠŸèƒ½                           |
| ------------ | -------- | ---------------------------------- |
| æ•°æ®é›†ç®¡ç†   | 5        | åˆ›å»ºã€åˆ—è¡¨ã€æŸ¥çœ‹ã€æ›´æ–°ã€åˆ é™¤æ•°æ®é›† |
| æ–‡æ¡£ç®¡ç†     | 9        | æ–‡æ¡£åˆ›å»ºã€æ›´æ–°ã€åˆ é™¤ã€çŠ¶æ€æŸ¥è¯¢     |
| æ–‡æ¡£æ‰¹é‡æ“ä½œ | 1        | æ‰¹é‡æ›´æ–°æ–‡æ¡£çŠ¶æ€                   |
| ç‰‡æ®µç®¡ç†     | 5        | æ–‡æ¡£ç‰‡æ®µçš„å¢åˆ æ”¹æŸ¥                 |
| å­ç‰‡æ®µç®¡ç†   | 4        | åˆ†å±‚ç‰‡æ®µçš„ç®¡ç†                     |
| çŸ¥è¯†åº“æ£€ç´¢   | 1        | æ™ºèƒ½æ£€ç´¢å’Œæœç´¢                     |
| æ–‡ä»¶ç®¡ç†     | 1        | ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯æŸ¥è¯¢                   |
| å…ƒæ•°æ®ç®¡ç†   | 6        | è‡ªå®šä¹‰å­—æ®µå’Œæ–‡æ¡£å…ƒæ•°æ®             |
| çŸ¥è¯†æ ‡ç­¾ç®¡ç† | 7        | æ ‡ç­¾åˆ›å»ºã€ç»‘å®šã€è§£ç»‘               |
| åµŒå…¥æ¨¡å‹ç®¡ç† | 1        | å¯ç”¨æ¨¡å‹åˆ—è¡¨æŸ¥è¯¢                   |
| **æ€»è®¡**     | **39**   | **å®Œæ•´çš„çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½**           |

## 4. æ•°æ®é›†ç®¡ç† (5 ä¸ª API)

### 4.1 åˆ›å»ºæ•°æ®é›†

**æ–¹æ³•ç­¾åï¼š**

```python
def create_dataset(
    name: str,                                    # æ•°æ®é›†åç§° (å¿…éœ€)
    description: Optional[str] = None,            # æ•°æ®é›†æè¿°
    indexing_technique: Optional[Literal["high_quality", "economy"]] = None,  # ç´¢å¼•æŠ€æœ¯
    permission: Optional[Literal["only_me", "all_team_members", "partial_members"]] = "only_me",  # æƒé™
    provider: Optional[Literal["vendor", "external"]] = "vendor",             # æä¾›å•†ç±»å‹
    external_knowledge_api_id: Optional[str] = None,      # å¤–éƒ¨çŸ¥è¯†API ID
    external_knowledge_id: Optional[str] = None,          # å¤–éƒ¨çŸ¥è¯†ID
    embedding_model: Optional[str] = None,                # åµŒå…¥æ¨¡å‹åç§°
    embedding_model_provider: Optional[str] = None,       # åµŒå…¥æ¨¡å‹æä¾›å•†
    retrieval_model: Optional[RetrievalModel] = None,     # æ£€ç´¢æ¨¡å‹é…ç½®
    partial_member_list: Optional[List[str]] = None,      # éƒ¨åˆ†æˆå‘˜åˆ—è¡¨
) -> Dataset
```

**å‚æ•°è¯´æ˜ï¼š**

- `name` (str): æ•°æ®é›†åç§° **(å¿…éœ€)**
- `description` (str, å¯é€‰): æ•°æ®é›†æè¿°
- `indexing_technique` (str, å¯é€‰): ç´¢å¼•æŠ€æœ¯ - "high_quality" æˆ– "economy"
- `permission` (str, å¯é€‰): æƒé™çº§åˆ«ï¼Œé»˜è®¤ "only_me"
- `provider` (str, å¯é€‰): æä¾›å•†ç±»å‹ï¼Œ"vendor" æˆ– "external"
- `embedding_model` (str, å¯é€‰): åµŒå…¥æ¨¡å‹åç§°
- `embedding_model_provider` (str, å¯é€‰): åµŒå…¥æ¨¡å‹æä¾›å•†
- `retrieval_model` (RetrievalModel, å¯é€‰): æ£€ç´¢æ¨¡å‹é…ç½®

**åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬åˆ›å»º
dataset = client.create_dataset(
    name="æŠ€æœ¯æ–‡æ¡£åº“",
    description="å­˜å‚¨æ‰€æœ‰æŠ€æœ¯ç›¸å…³æ–‡æ¡£"
)

# é«˜çº§é…ç½®
from dify_dataset_sdk.models import RetrievalModel, RerankingModel

# é…ç½®é‡æ’åºæ¨¡å‹
reranking_model = RerankingModel(
    reranking_provider_name="cohere",
    reranking_model_name="rerank-english-v2.0"
)

# é…ç½®é«˜çº§æ£€ç´¢æ¨¡å‹
retrieval_config = RetrievalModel(
    search_method="hybrid_search",
    reranking_enable=True,
    reranking_mode="reranking_model",
    reranking_model=reranking_model,
    weights=0.3,  # è¯­ä¹‰æœç´¢æƒé‡
    top_k=20,
    score_threshold_enabled=True,
    score_threshold=0.5
)

# åˆ›å»ºä¼ä¸šçº§æ•°æ®é›†
enterprise_dataset = client.create_dataset(
    name="ä¼ä¸šçŸ¥è¯†åº“",
    description="åŒ…å«å…¬å¸æ‰€æœ‰æŠ€æœ¯æ–‡æ¡£å’Œæµç¨‹",
    indexing_technique="high_quality",
    permission="all_team_members",
    embedding_model="text-embedding-ada-002",
    embedding_model_provider="openai",
    retrieval_model=retrieval_config
)
```

### 4.2 è·å–æ•°æ®é›†åˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_datasets(
    keyword: Optional[str] = None,
    tag_ids: Optional[List[str]] = None,
    page: int = 1,
    limit: int = 20,
    include_all: bool = False,
) -> PaginatedResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `keyword` (str, å¯é€‰): æœç´¢å…³é”®è¯
- `tag_ids` (List[str], å¯é€‰): æ ‡ç­¾ ID åˆ—è¡¨ï¼Œç”¨äºè¿‡æ»¤
- `page` (int): é¡µç ï¼Œé»˜è®¤ 1
- `limit` (int): æ¯é¡µæ•°é‡ï¼Œé»˜è®¤ 20
- `include_all` (bool): æ˜¯å¦åŒ…å«æ‰€æœ‰æ•°æ®é›†

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ‰€æœ‰æ•°æ®é›†
datasets = client.list_datasets()
print(f"æ€»æ•°æ®é›†æ•°: {datasets.total}")

# æœç´¢ç‰¹å®šå…³é”®è¯
tech_datasets = client.list_datasets(keyword="æŠ€æœ¯", limit=10)

# æŒ‰æ ‡ç­¾è¿‡æ»¤
tagged_datasets = client.list_datasets(tag_ids=["tag_id_1", "tag_id_2"])

# åˆ†é¡µè·å–
for page in range(1, 6):  # è·å–å‰5é¡µ
    page_datasets = client.list_datasets(page=page, limit=10)
    print(f"ç¬¬{page}é¡µ: {len(page_datasets.data)}ä¸ªæ•°æ®é›†")
```

### 4.3 è·å–æ•°æ®é›†è¯¦æƒ…

**æ–¹æ³•ç­¾åï¼š**

```python
def get_dataset(dataset_id: str) -> Dataset
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ•°æ®é›†è¯¦æƒ…
dataset = client.get_dataset("dataset_id")
print(f"æ•°æ®é›†åç§°: {dataset.name}")
print(f"æ–‡æ¡£æ•°é‡: {dataset.document_count}")
print(f"å­—ç¬¦æ•°: {dataset.character_count}")
print(f"åˆ›å»ºæ—¶é—´: {dataset.created_at}")
```

### 4.4 æ›´æ–°æ•°æ®é›†

**æ–¹æ³•ç­¾åï¼š**

```python
def update_dataset(
    dataset_id: str,
    name: Optional[str] = None,
    description: Optional[str] = None,
    indexing_technique: Optional[Literal["high_quality", "economy"]] = None,
    permission: Optional[Literal["only_me", "all_team_members", "partial_members"]] = None,
    retrieval_model: Optional[RetrievalModel] = None,
    partial_member_list: Optional[List[str]] = None,
) -> Dataset
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ›´æ–°æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
updated_dataset = client.update_dataset(
    dataset_id="dataset_id",
    name="æ›´æ–°åçš„æ•°æ®é›†åç§°",
    description="æ›´æ–°åçš„æè¿°"
)

# æ›´æ–°æƒé™è®¾ç½®
client.update_dataset(
    dataset_id="dataset_id",
    permission="all_team_members"
)

# æ›´æ–°æ£€ç´¢é…ç½®
new_retrieval_config = RetrievalModel(
    search_method="semantic_search",
    top_k=15,
    score_threshold_enabled=True,
    score_threshold=0.6
)

client.update_dataset(
    dataset_id="dataset_id",
    retrieval_model=new_retrieval_config
)
```

### 4.5 åˆ é™¤æ•°æ®é›†

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_dataset(dataset_id: str) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤æ•°æ®é›†ï¼ˆæ³¨æ„ï¼šè¿™æ˜¯ä¸å¯é€†æ“ä½œï¼‰
result = client.delete_dataset("dataset_id")
print(f"åˆ é™¤ç»“æœ: {result}")

# å®‰å…¨åˆ é™¤ï¼ˆå…ˆæ£€æŸ¥ååˆ é™¤ï¼‰
try:
    dataset = client.get_dataset("dataset_id")
    if dataset.document_count == 0:
        result = client.delete_dataset("dataset_id")
        print("ç©ºæ•°æ®é›†åˆ é™¤æˆåŠŸ")
    else:
        print(f"æ•°æ®é›†åŒ…å« {dataset.document_count} ä¸ªæ–‡æ¡£ï¼Œè¯·å…ˆæ¸…ç©º")
except Exception as e:
    print(f"åˆ é™¤å¤±è´¥: {e}")
```

## 5. æ–‡æ¡£ç®¡ç† (9 ä¸ª API)

### 5.1 ä»æ–‡æœ¬åˆ›å»ºæ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def create_document_by_text(
    dataset_id: str,                              # æ•°æ®é›†ID (å¿…éœ€)
    name: str,                                    # æ–‡æ¡£åç§° (å¿…éœ€)
    text: str,                                    # æ–‡æ¡£å†…å®¹ (å¿…éœ€)
    indexing_technique: Optional[Literal["high_quality", "economy"]] = "high_quality",  # ç´¢å¼•æŠ€æœ¯
    doc_form: Optional[Literal["text_model", "hierarchical_model", "qa_model"]] = None,  # æ–‡æ¡£å½¢å¼
    doc_language: Optional[str] = None,           # æ–‡æ¡£è¯­è¨€
    process_rule: Optional[ProcessRule] = None,   # å¤„ç†è§„åˆ™
    retrieval_model: Optional[RetrievalModel] = None,  # æ£€ç´¢æ¨¡å‹é…ç½®
    embedding_model: Optional[str] = None,        # åµŒå…¥æ¨¡å‹åç§°
    embedding_model_provider: Optional[str] = None,  # åµŒå…¥æ¨¡å‹æä¾›å•†
) -> DocumentResponse
```

**ProcessRule é…ç½®è¯¦è§£ï¼š**

```python
from dify_dataset_sdk.models import (
    ProcessRule,
    ProcessRuleConfig,
    PreProcessingRule,
    Segmentation,
    SubchunkSegmentation
)

# è‡ªå®šä¹‰é¢„å¤„ç†è§„åˆ™
pre_processing_rules = [
    PreProcessingRule(id="remove_extra_spaces", enabled=True),
    PreProcessingRule(id="remove_urls_emails", enabled=True)
]

# åˆ†æ®µé…ç½®
segmentation = Segmentation(
    separator="\n\n",    # æ®µè½åˆ†éš”ç¬¦
    max_tokens=1000        # æ¯æ®µæœ€å¤§tokenæ•°
)

# å­åˆ†æ®µé…ç½®ï¼ˆç”¨äºåˆ†å±‚æ¨¡å¼ï¼‰
subchunk_segmentation = SubchunkSegmentation(
    separator="***",
    max_tokens=300,
    chunk_overlap=50
)

# å®Œæ•´å¤„ç†è§„åˆ™é…ç½®
process_rule_config = ProcessRuleConfig(
    pre_processing_rules=pre_processing_rules,
    segmentation=segmentation,
    parent_mode="full-doc",  # æˆ– "paragraph"
    subchunk_segmentation=subchunk_segmentation
)

# è‡ªå®šä¹‰å¤„ç†è§„åˆ™
custom_process_rule = ProcessRule(
    mode="custom",
    rules=process_rule_config
)
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬æ–‡æ¡£åˆ›å»º
doc_response = client.create_document_by_text(
    dataset_id="dataset_id",
    name="APIæ–‡æ¡£",
    text="è¿™æ˜¯ä¸€ä¸ªè¯¦ç»†çš„APIä½¿ç”¨è¯´æ˜æ–‡æ¡£...",
    indexing_technique="high_quality"
)

print(f"æ–‡æ¡£ID: {doc_response.document.id}")
print(f"æ‰¹æ¬¡ID: {doc_response.batch}")

# åˆ›å»ºé—®ç­”æ¨¡å¼æ–‡æ¡£
qa_doc_response = client.create_document_by_text(
    dataset_id="dataset_id",
    name="FAQæ–‡æ¡£",
    text="é—®ï¼šä»€ä¹ˆæ˜¯Pythonï¼Ÿ\nç­”ï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€...",
    doc_form="qa_model",
    indexing_technique="high_quality"
)

# ä½¿ç”¨åµŒå…¥æ¨¡å‹å’Œæ£€ç´¢é…ç½®
from dify_dataset_sdk.models import RetrievalModel

retrieval_config = RetrievalModel(
    search_method="hybrid_search",
    top_k=10,
    score_threshold_enabled=True,
    score_threshold=0.7
)

advanced_doc_response = client.create_document_by_text(
    dataset_id="dataset_id",
    name="æŠ€æœ¯è§„èŒƒæ–‡æ¡£",
    text="é•¿ç¯‡æŠ€æœ¯æ–‡æ¡£å†…å®¹...",
    indexing_technique="high_quality",
    doc_form="text_model",
    embedding_model="text-embedding-ada-002",
    embedding_model_provider="openai",
    retrieval_model=retrieval_config,
    process_rule=custom_process_rule
)
```

### 5.2 ä»æ–‡ä»¶åˆ›å»ºæ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def create_document_by_file(
    dataset_id: str,                              # æ•°æ®é›†ID (å¿…éœ€)
    file_path: Union[str, Path],                  # æ–‡ä»¶è·¯å¾„ (å¿…éœ€)
    original_document_id: Optional[str] = None,   # åŸå§‹æ–‡æ¡£ID (å¯é€‰)
    indexing_technique: Optional[Literal["high_quality", "economy"]] = "high_quality",
    doc_form: Optional[Literal["text_model", "hierarchical_model", "qa_model"]] = None,  # æ–‡æ¡£å½¢å¼
    doc_language: Optional[str] = None,           # æ–‡æ¡£è¯­è¨€
    process_rule: Optional[ProcessRule] = None,
    retrieval_model: Optional[RetrievalModel] = None,  # æ£€ç´¢æ¨¡å‹é…ç½®
    embedding_model: Optional[str] = None,        # åµŒå…¥æ¨¡å‹åç§°
    embedding_model_provider: Optional[str] = None,  # åµŒå…¥æ¨¡å‹æä¾›å•†
) -> DocumentResponse
```

**æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š**

```python
SUPPORTED_FILE_TYPES = {
    '.txt': 'text/plain',
    '.md': 'text/markdown',
    '.pdf': 'application/pdf',
    '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    '.xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    '.csv': 'text/csv',
    '.html': 'text/html'
}
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# ä¸Šä¼ PDFæ–‡æ¡£
doc_response = client.create_document_by_file(
    dataset_id="dataset_id",
    file_path="./documents/manual.pdf",
    indexing_technique="high_quality"
)

# ä½¿ç”¨åŸå§‹æ–‡æ¡£IDæ›´æ–°æ–‡æ¡£
update_response = client.create_document_by_file(
    dataset_id="dataset_id",
    file_path="./documents/updated_manual.pdf",
    original_document_id="existing_doc_id",
    indexing_technique="high_quality"
)

# ä½¿ç”¨å®Œæ•´å‚æ•°é…ç½®
from dify_dataset_sdk.models import ProcessRule, RetrievalModel

process_rule = ProcessRule(mode="custom")
retrieval_model = RetrievalModel(
    search_method="semantic_search",
    top_k=15
)

advanced_doc = client.create_document_by_file(
    dataset_id="dataset_id",
    file_path="./documents/technical_spec.docx",
    doc_form="hierarchical_model",
    doc_language="Chinese",
    process_rule=process_rule,
    retrieval_model=retrieval_model,
    embedding_model="text-embedding-ada-002",
    embedding_model_provider="openai"
)

# æ‰¹é‡æ–‡ä»¶ä¸Šä¼ ç¤ºä¾‹
import os
from pathlib import Path

def batch_upload_documents(client, dataset_id, folder_path):
    """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£"""
    folder = Path(folder_path)
    supported_extensions = ['.txt', '.md', '.pdf', '.docx', '.xlsx', '.csv', '.html']

    for file_path in folder.iterdir():
        if file_path.suffix.lower() in supported_extensions:
            try:
                print(f"ä¸Šä¼ æ–‡ä»¶: {file_path.name}")
                doc_response = client.create_document_by_file(
                    dataset_id=dataset_id,
                    file_path=file_path,
                    indexing_technique="high_quality"
                )
                print(f"âœ… æˆåŠŸä¸Šä¼ : {doc_response.document.id}")

            except Exception as e:
                print(f"âŒ ä¸Šä¼ å¤±è´¥ {file_path.name}: {e}")

# ä½¿ç”¨ç¤ºä¾‹
batch_upload_documents(client, "dataset_id", "./documents/")
```

### 5.3 è·å–æ–‡æ¡£åˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_documents(
    dataset_id: str,
    keyword: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
) -> PaginatedResponse
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ‰€æœ‰æ–‡æ¡£
documents = client.list_documents("dataset_id")

# æœç´¢æ–‡æ¡£
search_docs = client.list_documents(
    dataset_id="dataset_id",
    keyword="API",
    limit=10
)

# åˆ†é¡µè·å–
page_docs = client.list_documents(
    dataset_id="dataset_id",
    page=2,
    limit=20
)
```

### 5.4 è·å–æ–‡æ¡£è¯¦æƒ…

**æ–¹æ³•ç­¾åï¼š**

```python
def get_document(
    dataset_id: str,
    document_id: str,
    metadata: Literal["all", "only", "without"] = "all",  # å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
) -> Document
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
document = client.get_document("dataset_id", "document_id")
print(f"æ–‡æ¡£åç§°: {document.name}")
print(f"å­—ç¬¦æ•°: {document.character_count}")
print(f"çŠ¶æ€: {document.status}")
```

### 5.5 é€šè¿‡æ–‡æœ¬æ›´æ–°æ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def update_document_by_text(
    dataset_id: str,
    document_id: str,
    name: Optional[str] = None,
    text: Optional[str] = None,
    process_rule: Optional[ProcessRule] = None,
) -> DocumentResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `name` (str, å¯é€‰): æ›´æ–°åçš„æ–‡æ¡£åç§°
- `text` (str, å¯é€‰): æ›´æ–°åçš„æ–‡æ¡£æ–‡æœ¬å†…å®¹
- `process_rule` (ProcessRule, å¯é€‰): å¤„ç†è§„åˆ™

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ›´æ–°æ–‡æ¡£åç§°å’Œå†…å®¹
updated_doc = client.update_document_by_text(
    dataset_id="dataset_id",
    document_id="document_id",
    name="æ›´æ–°åçš„æ–‡æ¡£åç§°",
    text="è¿™æ˜¯æ›´æ–°åçš„æ–‡æ¡£å†…å®¹ã€‚åŒ…å«æ›´å¤šè¯¦ç»†ä¿¡æ¯..."
)
print(f"æ›´æ–°æ‰¹æ¬¡ID: {updated_doc.batch}")

# ä»…æ›´æ–°æ–‡æ¡£åç§°
client.update_document_by_text(
    dataset_id="dataset_id",
    document_id="document_id",
    name="æ–°çš„æ–‡æ¡£æ ‡é¢˜"
)

# ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†è§„åˆ™æ›´æ–°
from dify_dataset_sdk.models import ProcessRule

process_rule = ProcessRule(mode="custom")
client.update_document_by_text(
    dataset_id="dataset_id",
    document_id="document_id",
    text="æ›´æ–°åçš„å†…å®¹",
    process_rule=process_rule
)
```

### 5.6 è·å–æ–‡æ¡£ç´¢å¼•çŠ¶æ€

**æ–¹æ³•ç­¾åï¼š**

```python
def get_document_indexing_status(
    dataset_id: str,
    batch: str
) -> IndexingStatusResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `batch` (str): æ–‡æ¡£åˆ›å»ºæ—¶è¿”å›çš„æ‰¹æ¬¡ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
import time

# åˆ›å»ºæ–‡æ¡£åç›‘æ§ç´¢å¼•è¿›åº¦
doc_response = client.create_document_by_text(
    dataset_id="dataset_id",
    name="æµ‹è¯•æ–‡æ¡£",
    text="æµ‹è¯•å†…å®¹"
)

# ç­‰å¾…ç´¢å¼•å®Œæˆ
while True:
    status = client.get_document_indexing_status(
        "dataset_id", doc_response.batch
    )

    if status.data[0].indexing_status == "completed":
        print("æ–‡æ¡£ç´¢å¼•å®Œæˆ")
        break
    elif status.data[0].indexing_status == "error":
        print(f"ç´¢å¼•å¤±è´¥: {status.data[0].error}")
        break

    print(f"ç´¢å¼•è¿›åº¦: {status.data[0].indexing_status}")
    time.sleep(2)

# æ£€æŸ¥ç´¢å¼•è¯¦æƒ…
def monitor_indexing_progress(client, dataset_id, batch_id, timeout=300):
    """ç›‘æ§æ–‡æ¡£ç´¢å¼•è¿›åº¦"""
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            status = client.get_document_indexing_status(dataset_id, batch_id)
            if status.data:
                indexing_info = status.data[0]
                print(f"çŠ¶æ€: {indexing_info.indexing_status}")
                print(f"è¿›åº¦: {indexing_info.processing_started_at}")

                if indexing_info.indexing_status == "completed":
                    print("âœ… ç´¢å¼•å®Œæˆ")
                    return True
                elif indexing_info.indexing_status in ["error", "paused"]:
                    print(f"âŒ ç´¢å¼•å¤±è´¥: {indexing_info.error}")
                    return False

            time.sleep(2)
        except Exception as e:
            print(f"æ£€æŸ¥çŠ¶æ€æ—¶å‡ºé”™: {e}")
            time.sleep(5)

    print("â° ç´¢å¼•è¶…æ—¶")
    return False

# ä½¿ç”¨ç¤ºä¾‹
monitor_indexing_progress(client, "dataset_id", doc_response.batch)
```

### 5.7 åˆ é™¤æ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_document(dataset_id: str, document_id: str) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤æ–‡æ¡£
result = client.delete_document("dataset_id", "document_id")
print(f"åˆ é™¤ç»“æœ: {result}")

# å®‰å…¨åˆ é™¤ï¼ˆå…ˆæ£€æŸ¥ååˆ é™¤ï¼‰
try:
    document = client.get_document("dataset_id", "document_id")
    if document.status == "completed":
        result = client.delete_document("dataset_id", "document_id")
        print("æ–‡æ¡£åˆ é™¤æˆåŠŸ")
    else:
        print(f"æ–‡æ¡£çŠ¶æ€ä¸º {document.status}ï¼Œè¯·ç­‰å¾…å¤„ç†å®Œæˆ")
except Exception as e:
    print(f"åˆ é™¤å¤±è´¥: {e}")
```

### 5.8 é€šè¿‡æ–‡ä»¶æ›´æ–°æ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def update_document_by_file(
    dataset_id: str,
    document_id: str,
    file_path: Union[str, Path],
    name: Optional[str] = None,
    process_rule: Optional[ProcessRule] = None,
) -> DocumentResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `file_path` (Union[str, Path]): æ–°æ–‡ä»¶è·¯å¾„ **(å¿…éœ€)**
- `name` (str, å¯é€‰): æ›´æ–°åçš„æ–‡æ¡£åç§°
- `process_rule` (ProcessRule, å¯é€‰): å¤„ç†è§„åˆ™

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# é€šè¿‡æ–‡ä»¶æ›´æ–°æ–‡æ¡£
updated_doc = client.update_document_by_file(
    dataset_id="dataset_id",
    document_id="document_id",
    file_path="./updated_document.pdf",
    name="æ›´æ–°åçš„æ–‡æ¡£åç§°"
)
print(f"æ›´æ–°æ‰¹æ¬¡ID: {updated_doc.batch}")

# ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†è§„åˆ™æ›´æ–°
from dify_dataset_sdk.models import ProcessRule

process_rule = ProcessRule(mode="automatic")
updated_doc = client.update_document_by_file(
    dataset_id="dataset_id",
    document_id="document_id",
    file_path="./new_version.docx",
    process_rule=process_rule
)
```

### 5.9 åˆ é™¤æ–‡æ¡£

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_document(dataset_id: str, document_id: str) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤æ–‡æ¡£
result = client.delete_document("dataset_id", "document_id")
print(f"åˆ é™¤ç»“æœ: {result}")

# å®‰å…¨åˆ é™¤ï¼ˆå…ˆæ£€æŸ¥ååˆ é™¤ï¼‰
try:
    document = client.get_document("dataset_id", "document_id")
    if document.status == "completed":
        result = client.delete_document("dataset_id", "document_id")
        print("æ–‡æ¡£åˆ é™¤æˆåŠŸ")
    else:
        print(f"æ–‡æ¡£çŠ¶æ€ä¸º {document.status}ï¼Œè¯·ç­‰å¾…å¤„ç†å®Œæˆ")
except Exception as e:
    print(f"åˆ é™¤å¤±è´¥: {e}")
```

## 6. æ–‡æ¡£æ‰¹é‡æ“ä½œ (1 ä¸ª API)

### 6.1 æ‰¹é‡æ›´æ–°æ–‡æ¡£çŠ¶æ€

**æ–¹æ³•ç­¾åï¼š**

```python
def batch_update_document_status(
    dataset_id: str,
    action: Literal["enable", "disable", "archive", "un_archive"],
    document_ids: List[str],
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `action` (str): æ“ä½œç±»å‹ - "enable", "disable", "archive", "un_archive"
- `document_ids` (List[str]): æ–‡æ¡£ ID åˆ—è¡¨ **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ‰¹é‡ç¦ç”¨æ–‡æ¡£
document_ids = ["doc_1", "doc_2", "doc_3"]
result = client.batch_update_document_status(
    dataset_id="dataset_id",
    action="disable",
    document_ids=document_ids
)

# æ‰¹é‡å¯ç”¨æ–‡æ¡£
client.batch_update_document_status(
    dataset_id="dataset_id",
    action="enable",
    document_ids=document_ids
)

# æ‰¹é‡å½’æ¡£æ–‡æ¡£
client.batch_update_document_status(
    dataset_id="dataset_id",
    action="archive",
    document_ids=document_ids
)
```

## 7. ç‰‡æ®µç®¡ç† (5 ä¸ª API)

### 7.1 åˆ›å»ºç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def create_segments(
    dataset_id: str,
    document_id: str,
    segments: List[Dict[str, Any]],
) -> SegmentResponse
```

**ä¸åŒæ¨¡å¼çš„ç‰‡æ®µåˆ›å»ºï¼š**

```python
# 1. æ–‡æœ¬æ¨¡å¼ç‰‡æ®µ
text_segments = [
    {
        "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œä»¥å…¶ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åº“ç”Ÿæ€ç³»ç»Ÿè€Œé—»åã€‚",
        "keywords": ["Python", "ç¼–ç¨‹è¯­è¨€", "è¯­æ³•", "åº“"]
    },
    {
        "content": "é¢å‘å¯¹è±¡ç¼–ç¨‹æ˜¯Pythonçš„æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€ï¼Œæ”¯æŒç±»ã€ç»§æ‰¿ã€å°è£…å’Œå¤šæ€ã€‚",
        "keywords": ["é¢å‘å¯¹è±¡", "ç±»", "ç»§æ‰¿", "å°è£…", "å¤šæ€"]
    }
]

# 2. é—®ç­”æ¨¡å¼ç‰‡æ®µ
qa_segments = [
    {
        "content": "ä»€ä¹ˆæ˜¯Pythonçš„ä¸»è¦ç‰¹ç‚¹ï¼Ÿ",
        "answer": "Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼šç®€æ´æ˜“è¯»çš„è¯­æ³•ã€å¼ºå¤§çš„æ ‡å‡†åº“ã€è·¨å¹³å°å…¼å®¹æ€§ã€ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ã€æ”¯æŒå¤šç§ç¼–ç¨‹èŒƒå¼ç­‰ã€‚",
        "keywords": ["Python", "ç‰¹ç‚¹", "è¯­æ³•", "æ ‡å‡†åº“", "è·¨å¹³å°"]
    },
    {
        "content": "å¦‚ä½•åœ¨Pythonä¸­å®šä¹‰ä¸€ä¸ªç±»ï¼Ÿ",
        "answer": "åœ¨Pythonä¸­ä½¿ç”¨classå…³é”®å­—å®šä¹‰ç±»ï¼ŒåŸºæœ¬è¯­æ³•ä¸ºï¼šclass ClassName: ç„¶ååœ¨ç±»ä½“ä¸­å®šä¹‰å±æ€§å’Œæ–¹æ³•ã€‚",
        "keywords": ["Python", "ç±»", "class", "å®šä¹‰", "è¯­æ³•"]
    }
]

# 3. åˆ†å±‚æ¨¡å¼ç‰‡æ®µï¼ˆåŒ…å«å­ç‰‡æ®µï¼‰
hierarchical_segments = [
    {
        "content": "Pythonæ•°æ®ç»“æ„è¯¦è§£",
        "answer": "Pythonæä¾›äº†å¤šç§å†…ç½®æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬åˆ—è¡¨ã€å…ƒç»„ã€å­—å…¸ã€é›†åˆç­‰ï¼Œæ¯ç§éƒ½æœ‰å…¶ç‰¹å®šçš„ç”¨é€”å’Œç‰¹æ€§ã€‚",
        "keywords": ["Python", "æ•°æ®ç»“æ„", "åˆ—è¡¨", "å…ƒç»„", "å­—å…¸", "é›†åˆ"],
        "child_chunks": [
            {
                "content": "åˆ—è¡¨(List)æ˜¯æœ‰åºã€å¯å˜çš„æ•°æ®é›†åˆï¼Œä½¿ç”¨æ–¹æ‹¬å·[]å®šä¹‰ã€‚"
            },
            {
                "content": "å…ƒç»„(Tuple)æ˜¯æœ‰åºã€ä¸å¯å˜çš„æ•°æ®é›†åˆï¼Œä½¿ç”¨åœ†æ‹¬å·()å®šä¹‰ã€‚"
            },
            {
                "content": "å­—å…¸(Dict)æ˜¯æ— åºçš„é”®å€¼å¯¹é›†åˆï¼Œä½¿ç”¨èŠ±æ‹¬å·{}å®šä¹‰ã€‚"
            }
        ]
    }
]
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»ºä¸åŒç±»å‹çš„ç‰‡æ®µ
text_result = client.create_segments("dataset_id", "doc_id", text_segments)
qa_result = client.create_segments("dataset_id", "doc_id", qa_segments)
hierarchical_result = client.create_segments("dataset_id", "doc_id", hierarchical_segments)

print(f"åˆ›å»ºäº† {len(text_result.data)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")
```

### 7.2 è·å–ç‰‡æ®µåˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_segments(
    dataset_id: str,
    document_id: str,
    keyword: Optional[str] = None,
    status: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
) -> SegmentResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `keyword` (str, å¯é€‰): æœç´¢å…³é”®è¯
- `status` (str, å¯é€‰): æœç´¢çŠ¶æ€ï¼Œå¦‚ 'completed'
- `page` (int): é¡µç ï¼Œé»˜è®¤ 1
- `limit` (int): æ¯é¡µæ•°é‡ï¼ŒèŒƒå›´ 1-100ï¼Œé»˜è®¤ 20

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ–‡æ¡£çš„æ‰€æœ‰ç‰‡æ®µ
segments = client.list_segments("dataset_id", "document_id")

# æœç´¢ç‰‡æ®µ
search_segments = client.list_segments(
    "dataset_id", "document_id",
    keyword="Python",
    limit=10
)

# æŒ‰çŠ¶æ€è¿‡æ»¤
completed_segments = client.list_segments(
    "dataset_id", "document_id",
    status="completed"
)

for segment in segments.data:
    print(f"ç‰‡æ®µID: {segment.id}")
    print(f"å†…å®¹: {segment.content}")
    print(f"çŠ¶æ€: {segment.status}")
```

### 7.3 è·å–ç‰‡æ®µè¯¦æƒ…

**æ–¹æ³•ç­¾åï¼š**

```python
def get_segment(
    dataset_id: str,
    document_id: str,
    segment_id: str,
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `segment_id` (str): ç‰‡æ®µ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–ç‰‡æ®µè¯¦æƒ…
segment = client.get_segment(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id"
)
print(f"ç‰‡æ®µå†…å®¹: {segment['content']}")
print(f"ç‰‡æ®µçŠ¶æ€: {segment['enabled']}")
```

### 7.4 æ›´æ–°ç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def update_segment(
    dataset_id: str,
    document_id: str,
    segment_id: str,
    segment_data: Dict[str, Any],
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `segment_id` (str): ç‰‡æ®µ ID **(å¿…éœ€)**
- `segment_data` (Dict[str, Any]): ç‰‡æ®µæ•°æ® **(å¿…éœ€)**
  - `content` (str): æ–‡æœ¬å†…å®¹/é—®é¢˜å†…å®¹ (å¿…éœ€)
  - `answer` (str): ç­”æ¡ˆå†…å®¹ (å¯é€‰, é—®ç­”æ¨¡å¼ä¸‹)
  - `keywords` (List[str]): å…³é”®è¯ (å¯é€‰)
  - `enabled` (bool): æ˜¯å¦å¯ç”¨ç‰‡æ®µ (å¯é€‰)
  - `regenerate_child_chunks` (bool): æ˜¯å¦é‡æ–°ç”Ÿæˆå­ç‰‡æ®µ (å¯é€‰)

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ›´æ–°ç‰‡æ®µå†…å®¹
segment_data = {
    "content": "æ›´æ–°åçš„ç‰‡æ®µå†…å®¹",
    "keywords": ["Python", "æ›´æ–°", "ç¼–ç¨‹"],
    "enabled": True
}

updated_segment = client.update_segment(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id",
    segment_data=segment_data
)

# æ›´æ–°é—®ç­”æ¨¡å¼ç‰‡æ®µ
qa_segment_data = {
    "content": "ä»€ä¹ˆæ˜¯Pythonçš„ä¸»è¦ç‰¹ç‚¹ï¼Ÿ",
    "answer": "Pythonçš„ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ç®€æ´æ˜“è¯»çš„è¯­æ³•ã€å¼ºå¤§çš„æ ‡å‡†åº“ã€è·¨å¹³å°å…¼å®¹æ€§ç­‰ã€‚",
    "keywords": ["Python", "ç‰¹ç‚¹", "è¯­æ³•"]
}

client.update_segment(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id",
    segment_data=qa_segment_data
)
```

### 7.5 åˆ é™¤ç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_segment(
    dataset_id: str,
    document_id: str,
    segment_id: str
) -> Dict[str, Any]
```

## 8. å­ç‰‡æ®µç®¡ç† (4 ä¸ª API)

### 8.1 åˆ›å»ºå­ç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def create_child_chunk(
    dataset_id: str,
    document_id: str,
    segment_id: str,
    content: str,
) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»ºå­ç‰‡æ®µ
child_chunk = client.create_child_chunk(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id",
    content="è¿™æ˜¯ä¸€ä¸ªå­ç‰‡æ®µçš„å†…å®¹ã€‚"
)
```

### 8.2 è·å–å­ç‰‡æ®µåˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_child_chunks(
    dataset_id: str,
    document_id: str,
    segment_id: str,
    keyword: Optional[str] = None,
    page: int = 1,
    limit: int = 20,
) -> ChildChunkResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `document_id` (str): æ–‡æ¡£ ID **(å¿…éœ€)**
- `segment_id` (str): çˆ¶ç‰‡æ®µ ID **(å¿…éœ€)**
- `keyword` (str, å¯é€‰): æœç´¢å…³é”®è¯
- `page` (int): é¡µç ï¼Œé»˜è®¤ 1
- `limit` (int): æ¯é¡µæ•°é‡ï¼Œæœ€å¤§ 100ï¼Œé»˜è®¤ 20

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ‰€æœ‰å­ç‰‡æ®µ
child_chunks = client.list_child_chunks(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id"
)

# æœç´¢å­ç‰‡æ®µ
search_chunks = client.list_child_chunks(
    dataset_id="dataset_id",
    document_id="document_id",
    segment_id="segment_id",
    keyword="Python",
    limit=10
)

for chunk in child_chunks.data:
    print(f"å­ç‰‡æ®µID: {chunk.id}")
    print(f"å†…å®¹: {chunk.content}")
```

### 8.3 æ›´æ–°å­ç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def update_child_chunk(
    dataset_id: str,
    document_id: str,
    segment_id: str,
    child_chunk_id: str,
    content: str,
) -> Dict[str, Any]
```

### 8.4 åˆ é™¤å­ç‰‡æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_child_chunk(
    dataset_id: str,
    document_id: str,
    segment_id: str,
    child_chunk_id: str,
) -> Dict[str, Any]
```

## 9. çŸ¥è¯†åº“æ£€ç´¢ (1 ä¸ª API)

### 9.1 æ£€ç´¢çŸ¥è¯†åº“å†…å®¹

**æ–¹æ³•ç­¾åï¼š**

```python
def retrieve(
    dataset_id: str,
    query: str,
    retrieval_model: Optional[RetrievalModel] = None,
    external_retrieval_model: Optional[Dict[str, Any]] = None,
) -> RetrievalResponse
```

**å®Œæ•´æ£€ç´¢é…ç½®ç¤ºä¾‹ï¼š**

```python
from dify_dataset_sdk.models import (
    RetrievalModel,
    RerankingModel,
    MetadataFilteringConditions,
    MetadataCondition
)

# 1. è¯­ä¹‰æœç´¢é…ç½®
semantic_search = RetrievalModel(
    search_method="semantic_search",
    top_k=10,
    score_threshold_enabled=True,
    score_threshold=0.7
)

# 2. å…¨æ–‡æœç´¢é…ç½®
fulltext_search = RetrievalModel(
    search_method="full_text_search",
    top_k=15,
    score_threshold_enabled=False
)

# 3. æ··åˆæœç´¢é…ç½®
hybrid_search = RetrievalModel(
    search_method="hybrid_search",
    weights=0.3,  # è¯­ä¹‰æœç´¢æƒé‡ (0.0-1.0)
    top_k=20,
    score_threshold_enabled=True,
    score_threshold=0.5,
    reranking_enable=True,
    reranking_mode="weighted_score"  # æˆ– "reranking_model"
)

# 4. å¸¦é‡æ’åºæ¨¡å‹çš„é«˜çº§æ£€ç´¢
reranking_model = RerankingModel(
    reranking_provider_name="cohere",
    reranking_model_name="rerank-multilingual-v2.0"
)

advanced_retrieval = RetrievalModel(
    search_method="hybrid_search",
    weights=0.4,
    top_k=30,
    reranking_enable=True,
    reranking_mode="reranking_model",
    reranking_model=reranking_model
)

# 5. å¸¦å…ƒæ•°æ®è¿‡æ»¤çš„æ£€ç´¢
metadata_conditions = [
    MetadataCondition(
        name="author",
        comparison_operator="is",
        value="å¼ ä¸‰"
    ),
    MetadataCondition(
        name="publish_date",
        comparison_operator="after",
        value="2024-01-01"
    ),
    MetadataCondition(
        name="version",
        comparison_operator="â‰¥",
        value=2.0
    )
]

metadata_filter = MetadataFilteringConditions(
    logical_operator="and",  # æˆ– "or"
    conditions=metadata_conditions
)

filtered_retrieval = RetrievalModel(
    search_method="semantic_search",
    top_k=10,
    metadata_filtering_conditions=metadata_filter
)

# æ‰§è¡Œä¸åŒç±»å‹çš„æ£€ç´¢
semantic_results = client.retrieve("dataset_id", "Pythonç¼–ç¨‹", semantic_search)
hybrid_results = client.retrieve("dataset_id", "æœºå™¨å­¦ä¹ ç®—æ³•", hybrid_search)
filtered_results = client.retrieve("dataset_id", "APIæ–‡æ¡£", filtered_retrieval)
```

**åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åŸºæœ¬æ£€ç´¢
results = client.retrieve(
    dataset_id="dataset_id",
    query="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
)

for result in results.data:
    print(f"å†…å®¹: {result.content}")
    print(f"ç›¸å…³åº¦: {result.score}")
    print(f"æ–‡æ¡£: {result.document_name}")

# é«˜çº§æ£€ç´¢é…ç½®
retrieval_config = RetrievalModel(
    search_method="hybrid_search",
    reranking_enable=True,
    top_k=5,
    score_threshold=0.7,
    weights=0.5  # è¯­ä¹‰æœç´¢æƒé‡
)

advanced_results = client.retrieve(
    dataset_id="dataset_id",
    query="äººå·¥æ™ºèƒ½åº”ç”¨",
    retrieval_model=retrieval_config
)
```

## 10. æ–‡ä»¶ç®¡ç† (1 ä¸ª API)

### 10.1 è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯

**æ–¹æ³•ç­¾åï¼š**

```python
def get_upload_file(dataset_id: str, document_id: str) -> Dict[str, Any]
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–ä¸Šä¼ æ–‡ä»¶ä¿¡æ¯
file_info = client.get_upload_file("dataset_id", "document_id")
print(f"æ–‡ä»¶å: {file_info['name']}")
print(f"æ–‡ä»¶å¤§å°: {file_info['size']}")
print(f"MIMEç±»å‹: {file_info['mime_type']}")
```

## 11. å…ƒæ•°æ®ç®¡ç† (6 ä¸ª API)

### 11.1 åˆ›å»ºå…ƒæ•°æ®å­—æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def create_metadata_field(
    dataset_id: str,
    field_type: str,
    name: str,
) -> Metadata
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»ºä¸åŒç±»å‹çš„å…ƒæ•°æ®å­—æ®µ
string_field = client.create_metadata_field(
    dataset_id="dataset_id",
    field_type="string",
    name="ä½œè€…"
)

number_field = client.create_metadata_field(
    dataset_id="dataset_id",
    field_type="number",
    name="ç‰ˆæœ¬å·"
)

time_field = client.create_metadata_field(
    dataset_id="dataset_id",
    field_type="time",
    name="å‘å¸ƒæ—¶é—´"
)
```

### 11.2 è·å–å…ƒæ•°æ®å­—æ®µåˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_metadata_fields(dataset_id: str) -> MetadataListResponse
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ•°æ®é›†çš„æ‰€æœ‰å…ƒæ•°æ®å­—æ®µ
metadata_fields = client.list_metadata_fields("dataset_id")

for field in metadata_fields.data:
    print(f"å­—æ®µID: {field.id}")
    print(f"å­—æ®µåç§°: {field.name}")
    print(f"å­—æ®µç±»å‹: {field.type}")
```

### 11.3 æ›´æ–°å…ƒæ•°æ®å­—æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def update_metadata_field(
    dataset_id: str,
    metadata_id: str,
    name: str,
) -> Metadata
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `metadata_id` (str): å…ƒæ•°æ®å­—æ®µ ID **(å¿…éœ€)**
- `name` (str): æ›´æ–°åçš„å­—æ®µåç§° **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ›´æ–°å…ƒæ•°æ®å­—æ®µåç§°
updated_field = client.update_metadata_field(
    dataset_id="dataset_id",
    metadata_id="metadata_id",
    name="æ›´æ–°åçš„ä½œè€…å­—æ®µ"
)
print(f"æ›´æ–°æˆåŠŸ: {updated_field.name}")
```

### 11.4 åˆ é™¤å…ƒæ•°æ®å­—æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_metadata_field(
    dataset_id: str,
    metadata_id: str,
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `metadata_id` (str): å…ƒæ•°æ®å­—æ®µ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤å…ƒæ•°æ®å­—æ®µ(æ³¨æ„ï¼šè¿™æ˜¯ä¸å¯é€†æ“ä½œ)
result = client.delete_metadata_field(
    dataset_id="dataset_id",
    metadata_id="metadata_id"
)
print(f"åˆ é™¤ç»“æœ: {result}")

# å®‰å…¨åˆ é™¤ï¼ˆå…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£ä½¿ç”¨ï¼‰
try:
    # æ£€æŸ¥è¯¥å…ƒæ•°æ®å­—æ®µæ˜¯å¦è¢«ä½¿ç”¨
    metadata_fields = client.list_metadata_fields("dataset_id")
    field_exists = any(field.id == "metadata_id" for field in metadata_fields.data)

    if field_exists:
        result = client.delete_metadata_field("dataset_id", "metadata_id")
        print("å…ƒæ•°æ®å­—æ®µåˆ é™¤æˆåŠŸ")
    else:
        print("å…ƒæ•°æ®å­—æ®µä¸å­˜åœ¨")
except Exception as e:
    print(f"åˆ é™¤å¤±è´¥: {e}")
```

### 11.5 æ›´æ–°æ–‡æ¡£å…ƒæ•°æ®

**æ–¹æ³•ç­¾åï¼š**

```python
def update_document_metadata(
    dataset_id: str,
    operation_data: List[Dict[str, Any]],
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `operation_data` (List[Dict[str, Any]]): æ“ä½œæ•°æ®åˆ—è¡¨ **(å¿…éœ€)**
  - æ¯ä¸ªæ“ä½œåŒ…å«ï¼š
    - `document_id` (str): æ–‡æ¡£ ID
    - `metadata_list` (List[Dict]): å…ƒæ•°æ®åˆ—è¡¨
      - `id` (str): å…ƒæ•°æ®å­—æ®µ ID
      - `value` (Any): å…ƒæ•°æ®å€¼
      - `name` (str): å…ƒæ•°æ®å­—æ®µåç§°

**æ‰¹é‡å…ƒæ•°æ®æ›´æ–°ç¤ºä¾‹ï¼š**

```python
# å…ˆåˆ›å»ºå…ƒæ•°æ®å­—æ®µ
author_field = client.create_metadata_field("dataset_id", "string", "ä½œè€…")
version_field = client.create_metadata_field("dataset_id", "number", "ç‰ˆæœ¬")
date_field = client.create_metadata_field("dataset_id", "time", "å‘å¸ƒæ—¥æœŸ")

# å‡†å¤‡æ‰¹é‡å…ƒæ•°æ®æ›´æ–°æ•°æ®
metadata_operations = [
    {
        "document_id": "doc_1",
        "metadata_list": [
            {"id": author_field.id, "value": "å¼ ä¸‰", "name": "ä½œè€…"},
            {"id": version_field.id, "value": "1.0", "name": "ç‰ˆæœ¬"},
            {"id": date_field.id, "value": "2024-01-15", "name": "å‘å¸ƒæ—¥æœŸ"}
        ]
    },
    {
        "document_id": "doc_2",
        "metadata_list": [
            {"id": author_field.id, "value": "æå››", "name": "ä½œè€…"},
            {"id": version_field.id, "value": "2.0", "name": "ç‰ˆæœ¬"},
            {"id": date_field.id, "value": "2024-02-20", "name": "å‘å¸ƒæ—¥æœŸ"}
        ]
    }
]

# æ‰§è¡Œæ‰¹é‡æ›´æ–°
result = client.update_document_metadata("dataset_id", metadata_operations)
print(f"æ›´æ–°ç»“æœ: {result}")

# å•ä¸ªæ–‡æ¡£å…ƒæ•°æ®æ›´æ–°
single_update = [
    {
        "document_id": "doc_3",
        "metadata_list": [
            {"id": author_field.id, "value": "ç‹äº”", "name": "ä½œè€…"},
            {"id": version_field.id, "value": "3.0", "name": "ç‰ˆæœ¬"}
        ]
    }
]

result = client.update_document_metadata("dataset_id", single_update)
```

### 11.6 åˆ‡æ¢å†…ç½®å…ƒæ•°æ®å­—æ®µ

**æ–¹æ³•ç­¾åï¼š**

```python
def toggle_built_in_metadata_field(
    dataset_id: str,
    action: Literal["disable", "enable"],
) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `action` (str): æ“ä½œç±»å‹ - "disable" æˆ– "enable" **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# å¯ç”¨å†…ç½®å…ƒæ•°æ®å­—æ®µ
result = client.toggle_built_in_metadata_field(
    dataset_id="dataset_id",
    action="enable"
)
print(f"å¯ç”¨ç»“æœ: {result}")

# ç¦ç”¨å†…ç½®å…ƒæ•°æ®å­—æ®µ
result = client.toggle_built_in_metadata_field(
    dataset_id="dataset_id",
    action="disable"
)
print(f"ç¦ç”¨ç»“æœ: {result}")
```

## 12. çŸ¥è¯†æ ‡ç­¾ç®¡ç† (7 ä¸ª API)

### 12.1 åˆ›å»ºçŸ¥è¯†æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def create_knowledge_tag(name: str) -> KnowledgeTag
```

**å‚æ•°è¯´æ˜ï¼š**

- `name` (str): æ ‡ç­¾åç§°ï¼Œæœ€å¤§ 50 ä¸ªå­—ç¬¦ **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»ºå•ä¸ªæ ‡ç­¾
tech_tag = client.create_knowledge_tag("æŠ€æœ¯æ–‡æ¡£")
print(f"åˆ›å»ºæ ‡ç­¾ID: {tech_tag.id}")
print(f"æ ‡ç­¾åç§°: {tech_tag.name}")

# æ‰¹é‡åˆ›å»ºæ ‡ç­¾
tag_names = ["äººå·¥æ™ºèƒ½", "Webå¼€å‘", "ç§»åŠ¨å¼€å‘", "æ•°æ®åº“", "äº‘è®¡ç®—"]
created_tags = []

for tag_name in tag_names:
    try:
        tag = client.create_knowledge_tag(tag_name)
        created_tags.append(tag)
        print(f"âœ… åˆ›å»ºæˆåŠŸ: {tag.name}")
    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤±è´¥ {tag_name}: {e}")

print(f"æˆåŠŸåˆ›å»º {len(created_tags)} ä¸ªæ ‡ç­¾")
```

### 12.2 ç»‘å®šæ•°æ®é›†åˆ°æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def bind_dataset_to_tag(dataset_id: str, tag_ids: List[str]) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `tag_ids` (List[str]): æ ‡ç­¾ ID åˆ—è¡¨ **(å¿…éœ€)**

**æ ‡ç­¾ç®¡ç†æœ€ä½³å®è·µï¼š**

```python
# åˆ›å»ºåˆ†ç±»æ ‡ç­¾ä½“ç³»
def setup_tag_system(client):
    """è®¾ç½®æ ‡ç­¾åˆ†ç±»ä½“ç³»"""

    # åˆ›å»ºä¸»è¦åˆ†ç±»æ ‡ç­¾
    tech_tag = client.create_knowledge_tag("æŠ€æœ¯æ–‡æ¡£")
    business_tag = client.create_knowledge_tag("ä¸šåŠ¡æ–‡æ¡£")
    process_tag = client.create_knowledge_tag("æµç¨‹è§„èŒƒ")

    # åˆ›å»ºæŠ€æœ¯å­åˆ†ç±»
    ai_tag = client.create_knowledge_tag("äººå·¥æ™ºèƒ½")
    web_tag = client.create_knowledge_tag("Webå¼€å‘")
    mobile_tag = client.create_knowledge_tag("ç§»åŠ¨å¼€å‘")

    # åˆ›å»ºä¼˜å…ˆçº§æ ‡ç­¾
    high_priority = client.create_knowledge_tag("é«˜ä¼˜å…ˆçº§")
    medium_priority = client.create_knowledge_tag("ä¸­ä¼˜å…ˆçº§")
    low_priority = client.create_knowledge_tag("ä½ä¼˜å…ˆçº§")

    return {
        "categories": [tech_tag, business_tag, process_tag],
        "tech_subcategories": [ai_tag, web_tag, mobile_tag],
        "priorities": [high_priority, medium_priority, low_priority]
    }

# ä½¿ç”¨æ ‡ç­¾ç»„ç»‡æ•°æ®é›†
tags = setup_tag_system(client)

# ä¸ºAIç›¸å…³æ•°æ®é›†ç»‘å®šæ ‡ç­¾
ai_dataset_tags = [
    tags["categories"][0].id,      # æŠ€æœ¯æ–‡æ¡£
    tags["tech_subcategories"][0].id,  # äººå·¥æ™ºèƒ½
    tags["priorities"][0].id       # é«˜ä¼˜å…ˆçº§
]

client.bind_dataset_to_tag("ai_dataset_id", ai_dataset_tags)

# æŸ¥è¯¢ç‰¹å®šæ ‡ç­¾çš„æ•°æ®é›†
ai_datasets = client.list_datasets(tag_ids=[tags["tech_subcategories"][0].id])
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ›å»ºæ ‡ç­¾
tech_tag = client.create_knowledge_tag("æŠ€æœ¯æ–‡æ¡£")
ai_tag = client.create_knowledge_tag("äººå·¥æ™ºèƒ½")

# ç»‘å®šæ•°æ®é›†åˆ°å¤šä¸ªæ ‡ç­¾
client.bind_dataset_to_tag(
    dataset_id="dataset_id",
    tag_ids=[tech_tag.id, ai_tag.id]
)

# è·å–æ•°æ®é›†çš„æ ‡ç­¾
dataset_tags = client.get_dataset_tags("dataset_id")
for tag in dataset_tags:
    print(f"æ ‡ç­¾: {tag.name}")
```

### 12.3 è·å–çŸ¥è¯†æ ‡ç­¾åˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_knowledge_tags() -> List[KnowledgeTag]
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ‰€æœ‰æ ‡ç­¾
tags = client.list_knowledge_tags()
print(f"æ€»å…± {len(tags)} ä¸ªæ ‡ç­¾")

for tag in tags:
    print(f"ID: {tag.id}, åç§°: {tag.name}")

# æŒ‰åç§°æœç´¢æ ‡ç­¾
def find_tag_by_name(client, tag_name):
    """æŒ‰åç§°æŸ¥æ‰¾æ ‡ç­¾"""
    tags = client.list_knowledge_tags()
    for tag in tags:
        if tag.name == tag_name:
            return tag
    return None

# æŸ¥æ‰¾ç‰¹å®šæ ‡ç­¾
ai_tag = find_tag_by_name(client, "äººå·¥æ™ºèƒ½")
if ai_tag:
    print(f"æ‰¾åˆ°æ ‡ç­¾: {ai_tag.name} (ID: {ai_tag.id})")
else:
    print("æœªæ‰¾åˆ°æŒ‡å®šæ ‡ç­¾")
```

### 12.4 æ›´æ–°çŸ¥è¯†æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def update_knowledge_tag(tag_id: str, name: str) -> KnowledgeTag
```

**å‚æ•°è¯´æ˜ï¼š**

- `tag_id` (str): æ ‡ç­¾ ID **(å¿…éœ€)**
- `name` (str): æ–°æ ‡ç­¾åç§°ï¼Œæœ€å¤§ 50 ä¸ªå­—ç¬¦ **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# æ›´æ–°æ ‡ç­¾åç§°
updated_tag = client.update_knowledge_tag(
    tag_id="tag_id",
    name="æ›´æ–°åçš„æ ‡ç­¾åç§°"
)
print(f"æ›´æ–°æˆåŠŸ: {updated_tag.name}")

# æ‰¹é‡æ›´æ–°æ ‡ç­¾
tag_updates = [
    {"id": "tag_1", "name": "AI & æœºå™¨å­¦ä¹ "},
    {"id": "tag_2", "name": "å‰ç«¯å¼€å‘"},
    {"id": "tag_3", "name": "åç«¯å¼€å‘"}
]

for update in tag_updates:
    try:
        updated_tag = client.update_knowledge_tag(
            tag_id=update["id"],
            name=update["name"]
        )
        print(f"âœ… æ›´æ–°æˆåŠŸ: {updated_tag.name}")
    except Exception as e:
        print(f"âŒ æ›´æ–°å¤±è´¥ {update['name']}: {e}")
```

### 12.5 åˆ é™¤çŸ¥è¯†æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def delete_knowledge_tag(tag_id: str) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `tag_id` (str): æ ‡ç­¾ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# åˆ é™¤æ ‡ç­¾(æ³¨æ„ï¼šè¿™æ˜¯ä¸å¯é€†æ“ä½œ)
result = client.delete_knowledge_tag("tag_id")
print(f"åˆ é™¤ç»“æœ: {result}")

# å®‰å…¨åˆ é™¤ï¼ˆå…ˆæ£€æŸ¥ååˆ é™¤ï¼‰
def safe_delete_tag(client, tag_id):
    """å®‰å…¨åˆ é™¤æ ‡ç­¾"""
    try:
        # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦å­˜åœ¨
        tags = client.list_knowledge_tags()
        tag_exists = any(tag.id == tag_id for tag in tags)

        if not tag_exists:
            print("æ ‡ç­¾ä¸å­˜åœ¨")
            return False

        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®é›†ä½¿ç”¨è¯¥æ ‡ç­¾
        datasets_with_tag = client.list_datasets(tag_ids=[tag_id])
        if datasets_with_tag.total > 0:
            print(f"è­¦å‘Š: æœ‰ {datasets_with_tag.total} ä¸ªæ•°æ®é›†ä½¿ç”¨è¯¥æ ‡ç­¾")
            return False

        # æ‰§è¡Œåˆ é™¤
        result = client.delete_knowledge_tag(tag_id)
        print("æ ‡ç­¾åˆ é™¤æˆåŠŸ")
        return True

    except Exception as e:
        print(f"åˆ é™¤å¤±è´¥: {e}")
        return False

# ä½¿ç”¨å®‰å…¨åˆ é™¤
safe_delete_tag(client, "tag_id")
```

### 12.6 è§£ç»‘æ•°æ®é›†æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def unbind_dataset_from_tag(dataset_id: str, tag_id: str) -> Dict[str, Any]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**
- `tag_id` (str): æ ‡ç­¾ ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è§£ç»‘å•ä¸ªæ ‡ç­¾
result = client.unbind_dataset_from_tag(
    dataset_id="dataset_id",
    tag_id="tag_id"
)
print(f"è§£ç»‘ç»“æœ: {result}")

# æ‰¹é‡è§£ç»‘æ ‡ç­¾
def unbind_multiple_tags(client, dataset_id, tag_ids):
    """æ‰¹é‡è§£ç»‘æ•°æ®é›†æ ‡ç­¾"""
    success_count = 0

    for tag_id in tag_ids:
        try:
            result = client.unbind_dataset_from_tag(dataset_id, tag_id)
            print(f"âœ… è§£ç»‘æˆåŠŸ: {tag_id}")
            success_count += 1
        except Exception as e:
            print(f"âŒ è§£ç»‘å¤±è´¥ {tag_id}: {e}")

    print(f"æˆåŠŸè§£ç»‘ {success_count}/{len(tag_ids)} ä¸ªæ ‡ç­¾")
    return success_count

# ä½¿ç”¨ç¤ºä¾‹
tag_ids_to_unbind = ["tag_1", "tag_2", "tag_3"]
unbind_multiple_tags(client, "dataset_id", tag_ids_to_unbind)
```

### 12.7 è·å–æ•°æ®é›†æ ‡ç­¾

**æ–¹æ³•ç­¾åï¼š**

```python
def get_dataset_tags(dataset_id: str) -> List[KnowledgeTag]
```

**å‚æ•°è¯´æ˜ï¼š**

- `dataset_id` (str): æ•°æ®é›† ID **(å¿…éœ€)**

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–æ•°æ®é›†çš„æ‰€æœ‰æ ‡ç­¾
dataset_tags = client.get_dataset_tags("dataset_id")
print(f"æ•°æ®é›†å…±æœ‰ {len(dataset_tags)} ä¸ªæ ‡ç­¾")

for tag in dataset_tags:
    print(f"- {tag.name} (ID: {tag.id})")

# æ‰¹é‡æŸ¥è¯¢å¤šä¸ªæ•°æ®é›†çš„æ ‡ç­¾
def get_multiple_dataset_tags(client, dataset_ids):
    """æ‰¹é‡è·å–æ•°æ®é›†æ ‡ç­¾"""
    dataset_tags_map = {}

    for dataset_id in dataset_ids:
        try:
            tags = client.get_dataset_tags(dataset_id)
            dataset_tags_map[dataset_id] = tags
            print(f"âœ… æ•°æ®é›† {dataset_id}: {len(tags)} ä¸ªæ ‡ç­¾")
        except Exception as e:
            print(f"âŒ è·å–å¤±è´¥ {dataset_id}: {e}")
            dataset_tags_map[dataset_id] = []

    return dataset_tags_map

# ä½¿ç”¨ç¤ºä¾‹
dataset_ids = ["dataset_1", "dataset_2", "dataset_3"]
tags_map = get_multiple_dataset_tags(client, dataset_ids)

# åˆ†ææ ‡ç­¾ä½¿ç”¨æƒ…å†µ
all_tags = []
for tags in tags_map.values():
    all_tags.extend(tags)

# ç»Ÿè®¡æœ€å¸¸ç”¨çš„æ ‡ç­¾
from collections import Counter
tag_usage = Counter(tag.name for tag in all_tags)
print("\næœ€å¸¸ç”¨çš„æ ‡ç­¾:")
for tag_name, count in tag_usage.most_common(5):
    print(f"- {tag_name}: {count} æ¬¡")
```

## 13. åµŒå…¥æ¨¡å‹ç®¡ç† (1 ä¸ª API)

### 13.1 è·å–å¯ç”¨çš„åµŒå…¥æ¨¡å‹åˆ—è¡¨

**æ–¹æ³•ç­¾åï¼š**

```python
def list_embedding_models() -> EmbeddingModelResponse
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```python
# è·å–å¯ç”¨çš„åµŒå…¥æ¨¡å‹
models = client.list_embedding_models()
print(f"å…±æœ‰ {len(models.data)} ä¸ªå¯ç”¨æ¨¡å‹")

for model in models.data:
    print(f"æ¨¡å‹åç§°: {model.model_name}")
    print(f"æä¾›å•†: {model.model_provider}")
    print(f"ç»´åº¦: {model.dimensions}")
    print(f"æœ€å¤§tokens: {model.max_tokens}")
    print("-" * 40)

# æŒ‰æä¾›å•†åˆ†ç±»æ¨¡å‹
def group_models_by_provider(models):
    """æŒ‰æä¾›å•†åˆ†ç±»æ¨¡å‹"""
    provider_groups = {}

    for model in models.data:
        provider = model.model_provider
        if provider not in provider_groups:
            provider_groups[provider] = []
        provider_groups[provider].append(model)

    return provider_groups

# åˆ†ç±»æ˜¾ç¤º
models = client.list_embedding_models()
provider_groups = group_models_by_provider(models)

for provider, provider_models in provider_groups.items():
    print(f"\n{provider} æä¾›å•† ({len(provider_models)} ä¸ªæ¨¡å‹):")
    for model in provider_models:
        print(f"  - {model.model_name} (ç»´åº¦: {model.dimensions})")

# é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
def recommend_model(models, requirements=None):
    """æ ¹æ®éœ€æ±‚æ¨èæ¨¡å‹"""
    if not requirements:
        requirements = {"language": "chinese", "dimension_preference": "high"}

    suitable_models = []

    for model in models.data:
        # ç®€å•çš„æ¨èé€»è¾‘
        if "chinese" in requirements.get("language", "").lower():
            if "chinese" in model.model_name.lower() or "multilingual" in model.model_name.lower():
                suitable_models.append(model)
        elif requirements.get("dimension_preference") == "high":
            if model.dimensions >= 1024:
                suitable_models.append(model)

    return suitable_models[:3]  # è¿”å›å‰3ä¸ªæ¨è

# è·å–æ¨èæ¨¡å‹
recommended = recommend_model(models, {"language": "chinese"})
print("\næ¨èçš„ä¸­æ–‡æ¨¡å‹:")
for model in recommended:
    print(f"- {model.model_name} ({model.model_provider})")
```

## 14. é”™è¯¯å¤„ç†

SDK æä¾›äº†å®Œæ•´çš„å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼š

```python
from dify_dataset_sdk.exceptions import (
    DifyAPIError,
    DifyNotFoundError,
    DifyValidationError,
    DifyAuthenticationError,
    DifyConnectionError,
    DifyTimeoutError
)

try:
    dataset = client.create_dataset(name="æµ‹è¯•æ•°æ®é›†")
except DifyValidationError as e:
    print(f"å‚æ•°éªŒè¯é”™è¯¯: {e}")
except DifyAuthenticationError as e:
    print(f"è®¤è¯å¤±è´¥: {e}")
except DifyNotFoundError as e:
    print(f"èµ„æºæœªæ‰¾åˆ°: {e}")
except DifyAPIError as e:
    print(f"APIé”™è¯¯: {e}")
```

### é«˜çº§é”™è¯¯å¤„ç†

```python
import time
import logging
from dify_dataset_sdk.exceptions import *

def robust_document_creation(client, dataset_id, documents, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ–‡æ¡£åˆ›å»º"""

    successful_docs = []
    failed_docs = []

    for doc_data in documents:
        retries = 0
        while retries < max_retries:
            try:
                doc_response = client.create_document_by_text(
                    dataset_id=dataset_id,
                    name=doc_data["name"],
                    text=doc_data["content"]
                )

                # ç­‰å¾…ç´¢å¼•å®Œæˆ
                if wait_for_indexing(client, dataset_id, doc_response.batch):
                    successful_docs.append({
                        "name": doc_data["name"],
                        "document_id": doc_response.document.id
                    })
                    break
                else:
                    raise Exception("ç´¢å¼•è¶…æ—¶")

            except DifyValidationError as e:
                logging.error(f"å‚æ•°éªŒè¯é”™è¯¯ {doc_data['name']}: {e}")
                failed_docs.append({"name": doc_data["name"], "error": str(e)})
                break  # éªŒè¯é”™è¯¯ä¸é‡è¯•

            except DifyTimeoutError as e:
                logging.warning(f"è¶…æ—¶é”™è¯¯ {doc_data['name']}: {e}")
                retries += 1
                time.sleep(2 ** retries)  # æŒ‡æ•°é€€é¿

            except DifyAPIError as e:
                if "rate limit" in str(e).lower():
                    logging.warning(f"é€Ÿç‡é™åˆ¶ {doc_data['name']}: {e}")
                    time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
                    retries += 1
                else:
                    logging.error(f"APIé”™è¯¯ {doc_data['name']}: {e}")
                    failed_docs.append({"name": doc_data["name"], "error": str(e)})
                    break

            except Exception as e:
                logging.error(f"æœªçŸ¥é”™è¯¯ {doc_data['name']}: {e}")
                retries += 1
                time.sleep(5)

        if retries >= max_retries:
            failed_docs.append({
                "name": doc_data["name"],
                "error": f"é‡è¯•{max_retries}æ¬¡åä»ç„¶å¤±è´¥"
            })

    return successful_docs, failed_docs

def wait_for_indexing(client, dataset_id, batch, timeout=300):
    """ç­‰å¾…æ–‡æ¡£ç´¢å¼•å®Œæˆ"""
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            status = client.get_document_indexing_status(dataset_id, batch)
            if status.data:
                indexing_status = status.data[0].indexing_status
                if indexing_status == "completed":
                    return True
                elif indexing_status in ["error", "paused"]:
                    return False
            time.sleep(2)
        except Exception as e:
            logging.warning(f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€æ—¶å‡ºé”™: {e}")
            time.sleep(5)

    return False  # è¶…æ—¶
```

## 15. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### å¹¶å‘å¤„ç†ç¤ºä¾‹

```python
import asyncio
import concurrent.futures
from typing import List, Dict, Any

# å¹¶å‘å¤„ç†ç¤ºä¾‹
def concurrent_document_processing(client, dataset_id, documents: List[Dict], max_workers=5):
    """å¹¶å‘å¤„ç†å¤šä¸ªæ–‡æ¡£"""

    def process_single_document(doc_data):
        try:
            return client.create_document_by_text(
                dataset_id=dataset_id,
                name=doc_data["name"],
                text=doc_data["content"]
            )
        except Exception as e:
            return {"error": str(e), "doc_name": doc_data["name"]}

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_doc = {
            executor.submit(process_single_document, doc): doc
            for doc in documents
        }

        results = []
        for future in concurrent.futures.as_completed(future_to_doc):
            doc_data = future_to_doc[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                results.append({
                    "error": str(e),
                    "doc_name": doc_data["name"]
                })

        return results

# åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æ¡£
def batch_process_large_dataset(client, dataset_id, documents: List[Dict], batch_size=10):
    """åˆ†æ‰¹å¤„ç†å¤§é‡æ–‡æ¡£"""

    total_docs = len(documents)
    processed = 0
    all_results = []

    for i in range(0, total_docs, batch_size):
        batch = documents[i:i + batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªæ–‡æ¡£")

        batch_results = concurrent_document_processing(
            client, dataset_id, batch, max_workers=min(5, len(batch))
        )

        all_results.extend(batch_results)
        processed += len(batch)

        print(f"å·²å¤„ç†: {processed}/{total_docs}")

        # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…è¿‡è½½
        if i + batch_size < total_docs:
            time.sleep(1)

    return all_results
```

## 16. æœ€ä½³å®è·µ

### 16.1 å®¢æˆ·ç«¯ç®¡ç†

```python
# ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨å…³é—­è¿æ¥
from dify_dataset_sdk import DifyDatasetClient

with DifyDatasetClient(api_key="your-api-key") as client:
    # æ‰§è¡Œæ“ä½œ
    dataset = client.create_dataset(name="æµ‹è¯•")
    # å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨å…³é—­

# æ‰‹åŠ¨ç®¡ç†è¿æ¥
client = DifyDatasetClient(api_key="your-api-key")
try:
    # æ‰§è¡Œæ“ä½œ
    dataset = client.create_dataset(name="æµ‹è¯•")
finally:
    client.close()  # ç¡®ä¿è¿æ¥å…³é—­
```

### 16.2 æ‰¹é‡æ“ä½œä¼˜åŒ–

```python
# æ‰¹é‡å¤„ç†æ–‡æ¡£
import time

def process_documents_batch(client, dataset_id, documents):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£çš„ç¤ºä¾‹"""
    for doc_data in documents:
        try:
            doc_response = client.create_document_by_text(
                dataset_id=dataset_id,
                name=doc_data["name"],
                text=doc_data["content"]
            )

            # ç›‘æ§ç´¢å¼•çŠ¶æ€
            while True:
                status = client.get_document_indexing_status(
                    dataset_id, doc_response.batch
                )

                if status.data[0].indexing_status == "completed":
                    print(f"æ–‡æ¡£ {doc_data['name']} ç´¢å¼•å®Œæˆ")
                    break
                elif status.data[0].indexing_status == "error":
                    print(f"æ–‡æ¡£ {doc_data['name']} ç´¢å¼•å¤±è´¥")
                    break

                time.sleep(2)  # ç­‰å¾…2ç§’åå†æ¬¡æ£€æŸ¥

        except Exception as e:
            print(f"å¤„ç†æ–‡æ¡£ {doc_data['name']} æ—¶å‡ºé”™: {e}")
```

### 16.3 é«˜çº§æ£€ç´¢é…ç½®

```python
from dify_dataset_sdk.models import (
    RetrievalModel,
    MetadataFilteringConditions,
    MetadataCondition
)

# é…ç½®é«˜çº§æ£€ç´¢
metadata_filter = MetadataFilteringConditions(
    logical_operator="and",
    conditions=[
        MetadataCondition(
            name="ä½œè€…",
            comparison_operator="is",
            value="å¼ ä¸‰"
        ),
        MetadataCondition(
            name="ç‰ˆæœ¬å·",
            comparison_operator="â‰¥",
            value=2.0
        )
    ]
)

advanced_retrieval = RetrievalModel(
    search_method="hybrid_search",
    reranking_enable=True,
    top_k=10,
    score_threshold_enabled=True,
    score_threshold=0.5,
    metadata_filtering_conditions=metadata_filter
)

results = client.retrieve(
    dataset_id="dataset_id",
    query="æœç´¢æŸ¥è¯¢",
    retrieval_model=advanced_retrieval
)
```

### 16.4 ç›‘æ§å’Œæ—¥å¿—

```python
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def monitor_dataset_operations(client, dataset_id):
    """ç›‘æ§æ•°æ®é›†æ“ä½œ"""

    # è·å–æ•°æ®é›†åŸºæœ¬ä¿¡æ¯
    dataset = client.get_dataset(dataset_id)
    logging.info(f"æ•°æ®é›† {dataset.name} åŒ…å« {dataset.document_count} ä¸ªæ–‡æ¡£")

    # è·å–æ–‡æ¡£åˆ—è¡¨
    documents = client.list_documents(dataset_id)
    logging.info(f"è·å–åˆ° {len(documents.data)} ä¸ªæ–‡æ¡£")

    # æ£€æŸ¥æ–‡æ¡£çŠ¶æ€
    for doc in documents.data:
        if doc.status != "completed":
            logging.warning(f"æ–‡æ¡£ {doc.name} çŠ¶æ€å¼‚å¸¸: {doc.status}")

    # æ£€æŸ¥åµŒå…¥æ¨¡å‹
    models = client.list_embedding_models()
    logging.info(f"å¯ç”¨åµŒå…¥æ¨¡å‹: {len(models.data)} ä¸ª")
```

## 17. é«˜çº§åº”ç”¨åœºæ™¯

### 17.1 ä¼ä¸šçŸ¥è¯†åº“æ„å»º

```python
import time
from pathlib import Path
from typing import Dict, List, Any

def build_enterprise_knowledge_base(client, config: Dict[str, Any]):
    """æ„å»ºä¼ä¸šçº§çŸ¥è¯†åº“"""

    # 1. åˆ›å»ºåˆ†ç±»æ•°æ®é›†
    datasets = {}
    dataset_configs = [
        {
            "name": "å…¬å¸æ”¿ç­–ä¸åˆ¶åº¦",
            "description": "åŒ…å«å…¬å¸å„ç±»æ”¿ç­–ã€åˆ¶åº¦å’Œè§„èŒƒ",
            "tags": ["æ”¿ç­–", "åˆ¶åº¦", "è§„èŒƒ"]
        },
        {
            "name": "æŠ€æœ¯æ–‡æ¡£ä¸æ•™ç¨‹",
            "description": "æŠ€æœ¯å¼€å‘ç›¸å…³æ–‡æ¡£å’Œæ•™ç¨‹",
            "tags": ["æŠ€æœ¯", "å¼€å‘", "æ•™ç¨‹"]
        },
        {
            "name": "äº§å“è¿è¥çŸ¥è¯†",
            "description": "äº§å“è®¾è®¡ã€è¿è¥ç›¸å…³çŸ¥è¯†",
            "tags": ["äº§å“", "è¿è¥", "è®¾è®¡"]
        }
    ]

    # åˆ›å»ºæ‰€æœ‰æ•°æ®é›†
    for config in dataset_configs:
        dataset = client.create_dataset(
            name=config["name"],
            description=config["description"],
            permission="all_team_members",
            indexing_technique="high_quality"
        )
        datasets[config["name"]] = dataset
        print(f"âœ… åˆ›å»ºæ•°æ®é›†: {dataset.name}")

        # åˆ›å»ºå’Œç»‘å®šæ ‡ç­¾
        tag_ids = []
        for tag_name in config["tags"]:
            try:
                tag = client.create_knowledge_tag(tag_name)
                tag_ids.append(tag.id)
            except:
                # æ ‡ç­¾å¯èƒ½å·²å­˜åœ¨
                tags = client.list_knowledge_tags()
                for existing_tag in tags:
                    if existing_tag.name == tag_name:
                        tag_ids.append(existing_tag.id)
                        break

        if tag_ids:
            client.bind_dataset_to_tag(dataset.id, tag_ids)

    return datasets

def batch_import_documents(client, dataset_id: str, doc_folder: Path):
    """æ‰¹é‡å¯¼å…¥æ–‡æ¡£"""

    supported_extensions = ['.txt', '.md', '.pdf', '.docx', '.xlsx', '.csv', '.html']
    imported_docs = []
    failed_docs = []

    for file_path in doc_folder.rglob('*'):
        if file_path.suffix.lower() in supported_extensions:
            try:
                print(f"å¯¼å…¥æ–‡ä»¶: {file_path.name}")

                # åˆ›å»ºæ–‡æ¡£
                doc_response = client.create_document_by_file(
                    dataset_id=dataset_id,
                    file_path=file_path,
                    indexing_technique="high_quality"
                )

                # ç­‰å¾…ç´¢å¼•å®Œæˆ
                if wait_for_indexing_complete(client, dataset_id, doc_response.batch):
                    imported_docs.append({
                        "file": file_path.name,
                        "document_id": doc_response.document.id
                    })
                    print(f"âœ… å¯¼å…¥æˆåŠŸ: {file_path.name}")
                else:
                    failed_docs.append({"file": file_path.name, "error": "ç´¢å¼•è¶…æ—¶"})

            except Exception as e:
                failed_docs.append({"file": file_path.name, "error": str(e)})
                print(f"âŒ å¯¼å…¥å¤±è´¥ {file_path.name}: {e}")

            # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            time.sleep(1)

    return imported_docs, failed_docs

def wait_for_indexing_complete(client, dataset_id: str, batch_id: str, timeout: int = 300) -> bool:
    """ç­‰å¾…æ–‡æ¡£ç´¢å¼•å®Œæˆ"""
    start_time = time.time()

    while time.time() - start_time < timeout:
        try:
            status = client.get_document_indexing_status(dataset_id, batch_id)
            if status.data and len(status.data) > 0:
                indexing_status = status.data[0].indexing_status
                if indexing_status == "completed":
                    return True
                elif indexing_status in ["error", "paused"]:
                    return False
            time.sleep(2)
        except Exception:
            time.sleep(5)

    return False
```

### 17.2 æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

```python
from dify_dataset_sdk.models import RetrievalModel, MetadataFilteringConditions, MetadataCondition

class IntelligentQASystem:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, client, dataset_configs: Dict[str, str]):
        self.client = client
        self.dataset_configs = dataset_configs  # {"category": "dataset_id"}

    def intelligent_search(self, query: str, category: str = None, filters: Dict = None) -> List[Dict]:
        """æ™ºèƒ½æœç´¢"""

        # é€‰æ‹©æ•°æ®é›†
        dataset_ids = [self.dataset_configs[category]] if category else list(self.dataset_configs.values())

        all_results = []

        for dataset_id in dataset_ids:
            # æ„å»ºæ£€ç´¢é…ç½®
            retrieval_config = self._build_retrieval_config(filters)

            try:
                results = self.client.retrieve(
                    dataset_id=dataset_id,
                    query=query,
                    retrieval_model=retrieval_config
                )

                for result in results.data:
                    all_results.append({
                        "content": result.content,
                        "score": result.score,
                        "source": result.document_name,
                        "dataset": dataset_id,
                        "metadata": getattr(result, 'metadata', {})
                    })

            except Exception as e:
                print(f"æœç´¢æ•°æ®é›† {dataset_id} å¤±è´¥: {e}")

        # æŒ‰ç›¸å…³åº¦æ’åº
        all_results.sort(key=lambda x: x["score"], reverse=True)
        return all_results[:10]  # è¿”å›å‰10ä¸ªç»“æœ

    def _build_retrieval_config(self, filters: Dict = None) -> RetrievalModel:
        """æ„å»ºæ£€ç´¢é…ç½®"""

        config = RetrievalModel(
            search_method="hybrid_search",
            weights=0.4,  # è¯­ä¹‰æœç´¢æƒé‡
            top_k=20,
            score_threshold_enabled=True,
            score_threshold=0.3,
            reranking_enable=True,
            reranking_mode="weighted_score"
        )

        # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
        if filters:
            conditions = []
            for key, value in filters.items():
                conditions.append(MetadataCondition(
                    name=key,
                    comparison_operator="is",
                    value=value
                ))

            if conditions:
                config.metadata_filtering_conditions = MetadataFilteringConditions(
                    logical_operator="and",
                    conditions=conditions
                )

        return config

    def answer_question(self, question: str, context_limit: int = 3) -> Dict[str, Any]:
        """å›ç­”é—®é¢˜"""

        # æœç´¢ç›¸å…³å†…å®¹
        search_results = self.intelligent_search(question)

        if not search_results:
            return {
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚",
                "confidence": 0.0,
                "sources": []
            }

        # é€‰æ‹©æœ€ç›¸å…³çš„å†…å®¹
        top_results = search_results[:context_limit]

        # ç»„åˆä¸Šä¸‹æ–‡
        context = "\n\n".join([result["content"] for result in top_results])

        # è®¡ç®—ç½®ä¿¡åº¦
        avg_score = sum(result["score"] for result in top_results) / len(top_results)
        confidence = min(avg_score, 1.0)

        return {
            "context": context,
            "confidence": confidence,
            "sources": [{
                "title": result["source"],
                "score": result["score"]
            } for result in top_results],
            "total_results": len(search_results)
        }

# ä½¿ç”¨ç¤ºä¾‹
qa_system = IntelligentQASystem(client, {
    "policy": "policy_dataset_id",
    "tech": "tech_dataset_id",
    "product": "product_dataset_id"
})

# æ™ºèƒ½é—®ç­”
result = qa_system.answer_question("å…¬å¸çš„è¯·å‡æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
print(f"å‚è€ƒæº: {[s['title'] for s in result['sources']]}")
```

### 17.3 å†…å®¹å®¡æ ¸ä¸è´¨é‡æ§åˆ¶

```python
import re
from typing import List, Dict, Tuple

class ContentQualityController:
    """å†…å®¹è´¨é‡æ§åˆ¶å™¨"""

    def __init__(self, client):
        self.client = client
        self.quality_rules = {
            "min_content_length": 50,
            "max_content_length": 10000,
            "forbidden_patterns": [
                r"\b(\w)\1{5,}\b",  # è¿ç»­é‡å¤å­—ç¬¦
                r"(\b\w+\b)\s+\1\s+\1",  # é‡å¤è¯è¯­
            ],
            "required_keywords_ratio": 0.02  # å…³é”®è¯å¯†åº¦
        }

    def validate_document(self, content: str, keywords: List[str] = None) -> Tuple[bool, List[str]]:
        """éªŒè¯æ–‡æ¡£è´¨é‡"""

        issues = []

        # 1. æ£€æŸ¥å†…å®¹é•¿åº¦
        if len(content) < self.quality_rules["min_content_length"]:
            issues.append(f"å†…å®¹è¿‡çŸ­ï¼Œå°‘äº {self.quality_rules['min_content_length']} ä¸ªå­—ç¬¦")

        if len(content) > self.quality_rules["max_content_length"]:
            issues.append(f"å†…å®¹è¿‡é•¿ï¼Œè¶…è¿‡ {self.quality_rules['max_content_length']} ä¸ªå­—ç¬¦")

        # 2. æ£€æŸ¥ç¦æ­¢æ¨¡å¼
        for pattern in self.quality_rules["forbidden_patterns"]:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append(f"å†…å®¹åŒ…å«ä¸å…è®¸çš„æ¨¡å¼: {pattern}")

        # 3. æ£€æŸ¥å…³é”®è¯å¯†åº¦
        if keywords:
            total_words = len(content.split())
            keyword_count = sum(content.lower().count(kw.lower()) for kw in keywords)
            keyword_ratio = keyword_count / total_words if total_words > 0 else 0

            if keyword_ratio < self.quality_rules["required_keywords_ratio"]:
                issues.append(f"å…³é”®è¯å¯†åº¦è¿‡ä½: {keyword_ratio:.3f}")

        return len(issues) == 0, issues

    def audit_dataset_quality(self, dataset_id: str) -> Dict[str, Any]:
        """å®¡æ ¸æ•°æ®é›†è´¨é‡"""

        documents = self.client.list_documents(dataset_id, limit=100)
        quality_report = {
            "total_documents": len(documents.data),
            "passed": 0,
            "failed": 0,
            "issues": []
        }

        for doc in documents.data:
            try:
                # è·å–æ–‡æ¡£è¯¦æƒ…
                doc_detail = self.client.get_document(dataset_id, doc.id)

                # è·å–æ–‡æ¡£ç‰‡æ®µ
                segments = self.client.list_segments(dataset_id, doc.id)

                for segment in segments.data:
                    is_valid, issues = self.validate_document(
                        segment.content,
                        getattr(segment, 'keywords', [])
                    )

                    if is_valid:
                        quality_report["passed"] += 1
                    else:
                        quality_report["failed"] += 1
                        quality_report["issues"].append({
                            "document": doc.name,
                            "segment_id": segment.id,
                            "issues": issues
                        })

            except Exception as e:
                quality_report["issues"].append({
                    "document": doc.name,
                    "error": str(e)
                })

        quality_report["pass_rate"] = quality_report["passed"] / (quality_report["passed"] + quality_report["failed"]) if (quality_report["passed"] + quality_report["failed"]) > 0 else 0

        return quality_report

    def auto_fix_common_issues(self, dataset_id: str, document_id: str) -> bool:
        """è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜"""

        try:
            segments = self.client.list_segments(dataset_id, document_id)
            fixed_count = 0

            for segment in segments.data:
                original_content = segment.content
                fixed_content = original_content

                # ç§»é™¤å¤šä½™ç©ºæ ¼
                fixed_content = re.sub(r'\s+', ' ', fixed_content)

                # ç§»é™¤é‡å¤å¥å­
                sentences = fixed_content.split('.')
                unique_sentences = []
                for sentence in sentences:
                    if sentence.strip() not in [s.strip() for s in unique_sentences]:
                        unique_sentences.append(sentence)
                fixed_content = '.'.join(unique_sentences)

                # å¦‚æœå†…å®¹æœ‰å˜åŒ–ï¼Œæ›´æ–°ç‰‡æ®µ
                if fixed_content != original_content:
                    self.client.update_segment(
                        dataset_id=dataset_id,
                        document_id=document_id,
                        segment_id=segment.id,
                        segment_data={"content": fixed_content}
                    )
                    fixed_count += 1

            return fixed_count > 0

        except Exception as e:
            print(f"è‡ªåŠ¨ä¿®å¤å¤±è´¥: {e}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
quality_controller = ContentQualityController(client)

# å®¡æ ¸æ•°æ®é›†è´¨é‡
report = quality_controller.audit_dataset_quality("dataset_id")
print(f"è´¨é‡æŠ¥å‘Š: é€šè¿‡ç‡ {report['pass_rate']:.2%}")
print(f"å‘ç° {len(report['issues'])} ä¸ªé—®é¢˜")

# è‡ªåŠ¨ä¿®å¤
for issue in report['issues']:
    if 'document' in issue:
        quality_controller.auto_fix_common_issues(
            "dataset_id",
            issue['document']
        )
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº† Dify Dataset SDK æ‰€æœ‰ 39 ä¸ª API çš„å®Œæ•´å‚è€ƒï¼ŒåŒ…æ‹¬ï¼š

- ğŸ“š **5 ä¸ªæ•°æ®é›†ç®¡ç† API**ï¼šåˆ›å»ºã€æŸ¥è¯¢ã€æ›´æ–°ã€åˆ é™¤æ•°æ®é›†
- ğŸ“„ **8 ä¸ªæ–‡æ¡£ç®¡ç† API**ï¼šæ–‡æ¡£çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸç®¡ç†
- ğŸ”„ **1 ä¸ªæ‰¹é‡æ“ä½œ API**ï¼šé«˜æ•ˆçš„æ‰¹é‡æ–‡æ¡£çŠ¶æ€ç®¡ç†
- âœ‚ï¸ **5 ä¸ªç‰‡æ®µç®¡ç† API**ï¼šç²¾ç»†åŒ–çš„å†…å®¹ç‰‡æ®µæ§åˆ¶
- ğŸŒ³ **4 ä¸ªå­ç‰‡æ®µç®¡ç† API**ï¼šåˆ†å±‚å†…å®¹ç»“æ„æ”¯æŒ
- ğŸ” **1 ä¸ªæ£€ç´¢ API**ï¼šå¼ºå¤§çš„è¯­ä¹‰å’Œæ··åˆæœç´¢
- ğŸ“ **1 ä¸ªæ–‡ä»¶ç®¡ç† API**ï¼šä¸Šä¼ æ–‡ä»¶ä¿¡æ¯æŸ¥è¯¢
- ğŸ·ï¸ **6 ä¸ªå…ƒæ•°æ®ç®¡ç† API**ï¼šè‡ªå®šä¹‰å…ƒæ•°æ®å­—æ®µå’Œå…³è”
- ğŸ”– **7 ä¸ªçŸ¥è¯†æ ‡ç­¾ç®¡ç† API**ï¼šæ ‡ç­¾åŒ–çŸ¥è¯†ç»„ç»‡
- ğŸ¤– **1 ä¸ªåµŒå…¥æ¨¡å‹ç®¡ç† API**ï¼šæ¨¡å‹ä¿¡æ¯æŸ¥è¯¢

SDK æä¾›äº†å®Œæ•´çš„ç±»å‹å®‰å…¨ã€é”™è¯¯å¤„ç†ã€æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µæŒ‡å—ï¼Œèƒ½å¤Ÿæ»¡è¶³ä»ç®€å•åˆ°å¤æ‚çš„å„ç§çŸ¥è¯†åº“ç®¡ç†éœ€æ±‚ã€‚

---

ğŸ“ **æ³¨æ„**: æœ¬æ–‡æ¡£åŸºäº Dify Dataset SDK v0.3.0 ç‰ˆæœ¬ç¼–å†™ã€‚å»ºè®®æŸ¥çœ‹æœ€æ–°çš„ [GitHub ä»“åº“](https://github.com/LeekJay/dify-dataset-sdk) è·å–æœ€æ–°åŠŸèƒ½å’Œæ›´æ–°ã€‚

ğŸ”— **ç›¸å…³é“¾æ¥**:

- [SDK GitHub ä»“åº“](https://github.com/LeekJay/dify-dataset-sdk)
- [Dify å®˜æ–¹æ–‡æ¡£](https://docs.dify.ai/)
- [PyPI åŒ…é¡µé¢](https://pypi.org/project/dify-dataset-sdk/)
- [é—®é¢˜åé¦ˆ](https://github.com/LeekJay/dify-dataset-sdk/issues)
