# Biasâ€“Variance Decomposition Toolkit

A lightweight Python toolkit for estimating **bias** and **variance** components of machine learning models.  
Supports both **regression** (via Mean Squared Error) and **classification** (via 0â€“1 loss).  
Works seamlessly with **PyTorch models** and scikit-learn style data workflows.

---

## âœ¨ Features

- ğŸ“Š **Biasâ€“variance decomposition** for:
  - **Regression** using Mean Squared Error (MSE)
  - **Classification** using 0â€“1 loss
- ğŸ”„ **Bootstrap resampling** for reliable estimates
- ğŸ§  Works with **PyTorch models** (custom architectures supported)
- â±ï¸ **Early stopping** with patience-based validation
- âš¡ Device support: CPU and CUDA (GPU)

---

## ğŸ“¦ Installation

```bash
pip install biasvariance-toolkit
```

---

## ğŸš€ Quick Start

### 1. Define your PyTorch model
```python
import torch.nn as nn

class SimpleNet(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))
```

---

### 2. Biasâ€“Variance Decomposition for Regression
```python
from biasvariance_toolkit import estimate_bias_variance_mse
import torch.nn as nn

bias_sq, variance, total_error, bias_plus_var, avg_train_loss, test_loss = estimate_bias_variance_mse(
    model_class=SimpleNet,
    model_kwargs={"input_dim": 10, "hidden_dim": 32, "output_dim": 1},
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    loss_fn=nn.MSELoss(),
    num_models=10,
    max_epochs=100,
    lr=0.001,
    device="cpu"
)
```

---

### 3. Biasâ€“Variance Decomposition for Classification
```python
from biasvariance_toolkit import estimate_bias_variance_0_1
import torch.nn as nn

avg_bias, avg_var, expected_loss, empirical_loss, avg_train_loss, test_loss = estimate_bias_variance_0_1(
    model_class=SimpleNet,
    model_kwargs={"input_dim": 20, "hidden_dim": 64, "output_dim": 3},
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    loss_fn=nn.CrossEntropyLoss(),
    num_models=10,
    max_epochs=100,
    lr=0.001,
    device="cpu"
)
```

---

## ğŸ“˜ API Overview

### `estimate_bias_variance_mse`
- **Task**: Regression (MSE)
- **Returns**:  
  - `bias_sq` â€“ Squared bias  
  - `variance` â€“ Variance  
  - `total_error` â€“ Average test error (MSE)  
  - `bias_plus_variance` â€“ BiasÂ² + Variance  
  - `avg_train_loss` â€“ Average training loss across models  
  - `test_loss` â€“ Loss of the ensembleâ€™s mean prediction  

---

### `estimate_bias_variance_0_1`
- **Task**: Classification (0â€“1 Loss)
- **Returns**:  
  - `avg_bias` â€“ Average bias  
  - `avg_var` â€“ Average variance  
  - `expected_loss` â€“ Expected 0â€“1 loss  
  - `empirical_loss` â€“ Empirical 0â€“1 loss  
  - `avg_train_loss` â€“ Average training loss across models  
  - `test_loss` â€“ Loss of the ensembleâ€™s mean prediction  

---

## ğŸ›  Requirements
- Python â‰¥ 3.8
- PyTorch â‰¥ 1.9
- NumPy
- SciPy
- scikit-learn

---

## ğŸ“Š Example Use Cases
- Analyzing model stability under resampling
- Comparing architectures (e.g., shallow vs deep networks)
- Studying underfitting vs overfitting tradeoffs
- Teaching / demonstrating biasâ€“variance decomposition concepts

---

## ğŸ“œ License
MIT License Â© 2025  
