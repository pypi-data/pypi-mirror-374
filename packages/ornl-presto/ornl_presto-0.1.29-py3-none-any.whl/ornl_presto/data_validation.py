"""
Data Validation and Preprocessing Module for PRESTO

Provides comprehensive data validation, preprocessing, and quality checks
to ensure optimal privacy algorithm performance.
"""

import numpy as np
import torch
from typing import Union, Tuple, Dict, Any, List, Optional
from scipy import stats
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import IsolationForest
import warnings


ArrayLike = Union[List, np.ndarray, torch.Tensor]


class DataValidator:
    """Comprehensive data validation for privacy analysis."""

    def __init__(self, min_size: int = 10, max_size: int = 1000000):
        self.min_size = min_size
        self.max_size = max_size
        self.validation_results: Dict[str, Any] = {}

    def validate_data(self, data: ArrayLike) -> Dict[str, Any]:
        """Perform comprehensive data validation."""

        # Convert to numpy for analysis
        if torch.is_tensor(data):
            np_data = data.cpu().numpy() if hasattr(data, "cpu") else np.array(data)
        else:
            np_data = np.array(data)

        results: Dict[str, Any] = {
            "valid": True,
            "warnings": [],
            "errors": [],
            "recommendations": [],
            "statistics": {},
        }

        # Size validation
        if len(np_data) < self.min_size:
            results["errors"].append(
                f"Data too small: {len(np_data)} < {self.min_size}"
            )
            results["valid"] = False
        elif len(np_data) > self.max_size:
            results["warnings"].append(
                f"Large dataset: {len(np_data)} > {self.max_size} (consider sampling)"
            )

        # Check for missing values
        if np.any(np.isnan(np_data)):
            nan_count = np.sum(np.isnan(np_data))
            results["errors"].append(f"Contains {nan_count} NaN values")
            results["valid"] = False
            return results  # Early return if critical errors

        if np.any(np.isinf(np_data)):
            inf_count = np.sum(np.isinf(np_data))
            results["errors"].append(f"Contains {inf_count} infinite values")
            results["valid"] = False
            return results  # Early return if critical errors

        # Basic statistics
        if len(results["errors"]) == 0:  # Only calculate if no critical errors
            results["statistics"] = self._calculate_statistics(np_data)

            # Statistical tests and recommendations
            self._analyze_distribution(np_data, results)
            self._detect_outliers(np_data, results)
            self._check_data_quality(np_data, results)

        self.validation_results = results
        return results

    def _calculate_statistics(self, data: np.ndarray) -> Dict[str, float]:
        """Calculate comprehensive statistics."""
        return {
            "size": len(data),
            "mean": float(np.mean(data)),
            "std": float(np.std(data)),
            "min": float(np.min(data)),
            "max": float(np.max(data)),
            "median": float(np.median(data)),
            "q25": float(np.percentile(data, 25)),
            "q75": float(np.percentile(data, 75)),
            "skewness": float(stats.skew(data)),
            "kurtosis": float(stats.kurtosis(data)),
            "range": float(np.max(data) - np.min(data)),
            "iqr": float(np.percentile(data, 75) - np.percentile(data, 25)),
            "coefficient_of_variation": (
                float(np.std(data) / np.abs(np.mean(data)))
                if np.mean(data) != 0
                else float("inf")
            ),
        }

    def _analyze_distribution(self, data: np.ndarray, results: Dict[str, Any]):
        """Analyze data distribution and provide recommendations."""

        # Normality test
        if len(data) >= 8:  # Minimum for Shapiro-Wilk
            try:
                stat, p_value = stats.shapiro(data[:5000])  # Limit for performance
                if p_value > 0.05:
                    if isinstance(results["recommendations"], list):
                        results["recommendations"].append(
                            "Data appears normally distributed - "
                            "Gaussian mechanism recommended"
                        )
                else:
                    if isinstance(results["recommendations"], list):
                        results["recommendations"].append(
                            "Data is not normally distributed - "
                            "consider Laplace or Exponential mechanisms"
                        )
            except Exception:
                if isinstance(results["warnings"], list):
                    results["warnings"].append("Could not perform normality test")

        # Distribution characteristics
        skew = results["statistics"]["skewness"]
        if abs(skew) > 1:
            if skew > 1:
                if isinstance(results["recommendations"], list):
                    results["recommendations"].append(
                        "Right-skewed data detected - "
                        "Exponential mechanism may work well"
                    )
            else:
                if isinstance(results["recommendations"], list):
                    results["recommendations"].append(
                        "Left-skewed data detected - consider data transformation"
                    )

        # Kurtosis analysis
        kurt = results["statistics"]["kurtosis"]
        if kurt > 3:
            if isinstance(results["warnings"], list):
                results["warnings"].append(
                    "Heavy-tailed distribution detected - "
                    "outliers may affect privacy mechanisms"
                )
        elif kurt < -1:
            if isinstance(results["warnings"], list):
                results["warnings"].append(
                    "Light-tailed distribution detected - "
                    "may have limited natural variation"
                )

        # Variance analysis
        cv = results["statistics"]["coefficient_of_variation"]
        if cv > 2:
            if isinstance(results["warnings"], list):
                results["warnings"].append(
                    "High coefficient of variation - data has high relative variability"
                )
            if isinstance(results["recommendations"], list):
                results["recommendations"].append(
                    "Consider robust scaling before privacy analysis"
                )
        elif cv < 0.1:
            if isinstance(results["warnings"], list):
                results["warnings"].append(
                    "Low coefficient of variation - "
                    "data may have limited natural variation"
                )

    def _detect_outliers(self, data: np.ndarray, results: Dict[str, Any]):
        """Detect outliers using multiple methods."""

        # IQR method
        q25, q75 = np.percentile(data, [25, 75])
        iqr = q75 - q25
        lower_bound = q25 - 1.5 * iqr
        upper_bound = q75 + 1.5 * iqr
        iqr_outliers = np.sum((data < lower_bound) | (data > upper_bound))

        # Z-score method
        z_scores = np.abs(stats.zscore(data))
        zscore_outliers = np.sum(z_scores > 3)

        # Report outliers
        outlier_percentage = max(iqr_outliers, zscore_outliers) / len(data) * 100

        if outlier_percentage > 10:
            results["warnings"].append(
                f"High outlier rate: {outlier_percentage:.1f}% of data points"
            )
            results["recommendations"].append(
                "Consider outlier removal or robust privacy mechanisms"
            )
        elif outlier_percentage > 5:
            results["warnings"].append(
                f"Moderate outlier rate: {outlier_percentage:.1f}% of data points"
            )

        results["statistics"]["outlier_percentage_iqr"] = iqr_outliers / len(data) * 100
        results["statistics"]["outlier_percentage_zscore"] = (
            zscore_outliers / len(data) * 100
        )

    def _check_data_quality(self, data: np.ndarray, results: Dict[str, Any]):
        """Check overall data quality."""

        # Check for constant values
        if np.std(data) == 0:
            results["errors"].append("Data contains only constant values")
            results["valid"] = False
        elif np.std(data) < 1e-10:
            results["warnings"].append("Data has extremely low variance")

        # Check for discrete vs continuous
        unique_values = len(np.unique(data))
        if unique_values < len(data) * 0.1:
            results["recommendations"].append(
                "Data appears highly discrete - consider categorical privacy mechanisms"
            )

        # Check data range
        data_range = np.max(data) - np.min(data)
        if data_range > 1e6:
            results["warnings"].append("Very large data range - consider normalization")
        elif data_range < 1e-6:
            results["warnings"].append("Very small data range - may need scaling")

        # Dynamic range check
        if np.max(data) / np.min(data) > 1000 and np.min(data) > 0:
            results["warnings"].append(
                "High dynamic range - logarithmic transformation may help"
            )


class DataPreprocessor:
    """Advanced data preprocessing for optimal privacy analysis."""

    def __init__(self):
        self.scalers = {}
        self.transformations = {}
        self.preprocessing_history = []

    def preprocess_data(
        self,
        data: ArrayLike,
        standardize: bool = True,
        handle_outliers: bool = True,
        outlier_method: str = "iqr",
        outlier_threshold: float = 3.0,
        transformation: Optional[str] = None,
    ) -> Tuple[torch.Tensor, Dict[str, Any]]:
        """Comprehensive data preprocessing pipeline."""

        # Convert to numpy
        if torch.is_tensor(data):
            np_data = (
                data.cpu().numpy().copy()
                if hasattr(data, "cpu")
                else np.array(data).copy()
            )
        else:
            np_data = np.array(data, dtype=np.float32).copy()

        preprocessing_info = {
            "original_shape": np_data.shape,
            "original_stats": {
                "mean": float(np.mean(np_data)),
                "std": float(np.std(np_data)),
                "min": float(np.min(np_data)),
                "max": float(np.max(np_data)),
            },
            "steps_applied": [],
            "outliers_removed": 0,
            "transformation_applied": None,
        }

        # Handle outliers
        if handle_outliers:
            np_data, outliers_removed = self._handle_outliers(
                np_data, method=outlier_method, threshold=outlier_threshold
            )
            preprocessing_info["outliers_removed"] = outliers_removed
            if outliers_removed > 0:
                preprocessing_info["steps_applied"].append(
                    f"outlier_removal_{outlier_method}"
                )

        # Apply transformation
        if transformation:
            np_data = self._apply_transformation(np_data, transformation)
            preprocessing_info["transformation_applied"] = transformation
            preprocessing_info["steps_applied"].append(
                f"transformation_{transformation}"
            )

        # Standardization
        if standardize:
            np_data = self._standardize_data(np_data)
            preprocessing_info["steps_applied"].append("standardization")

        # Final statistics
        preprocessing_info["final_stats"] = {
            "mean": float(np.mean(np_data)),
            "std": float(np.std(np_data)),
            "min": float(np.min(np_data)),
            "max": float(np.max(np_data)),
            "size": len(np_data),
        }

        # Convert back to tensor
        result_tensor = torch.tensor(np_data, dtype=torch.float32)

        self.preprocessing_history.append(preprocessing_info)

        return result_tensor, preprocessing_info

    def _handle_outliers(
        self, data: np.ndarray, method: str = "iqr", threshold: float = 3.0
    ) -> Tuple[np.ndarray, int]:
        """Remove outliers using specified method."""

        original_size = len(data)

        if method == "iqr":
            q25, q75 = np.percentile(data, [25, 75])
            iqr = q75 - q25
            lower_bound = q25 - 1.5 * iqr
            upper_bound = q75 + 1.5 * iqr
            mask = (data >= lower_bound) & (data <= upper_bound)

        elif method == "zscore":
            z_scores = np.abs(stats.zscore(data))
            mask = z_scores <= threshold

        elif method == "isolation_forest":
            if len(data) >= 10:  # Minimum for isolation forest
                iso_forest = IsolationForest(contamination=0.1, random_state=42)
                outlier_labels = iso_forest.fit_predict(data.reshape(-1, 1))
                mask = outlier_labels == 1
            else:
                # Fallback to IQR for small datasets
                return self._handle_outliers(data, method="iqr", threshold=threshold)

        else:
            raise ValueError(f"Unknown outlier method: {method}")

        cleaned_data = data[mask]
        outliers_removed = original_size - len(cleaned_data)

        return cleaned_data, outliers_removed

    def _apply_transformation(
        self, data: np.ndarray, transformation: str
    ) -> np.ndarray:
        """Apply data transformation."""

        if transformation == "log":
            # Ensure positive values
            min_val = np.min(data)
            if min_val <= 0:
                data = data - min_val + 1e-8
            return np.log(data)

        elif transformation == "sqrt":
            # Ensure non-negative values
            min_val = np.min(data)
            if min_val < 0:
                data = data - min_val
            return np.sqrt(data)

        elif transformation == "box_cox":
            try:
                transformed, _ = stats.boxcox(data + 1 - np.min(data))
                return transformed
            except Exception:
                warnings.warn("Box-Cox transformation failed, skipping")
                return data

        else:
            raise ValueError(f"Unknown transformation: {transformation}")

    def _standardize_data(
        self, data: np.ndarray, method: str = "standard"
    ) -> np.ndarray:
        """Standardize data using specified method."""

        if method == "standard":
            scaler = StandardScaler()
        elif method == "robust":
            scaler = RobustScaler()
        else:
            raise ValueError(f"Unknown standardization method: {method}")

        standardized = scaler.fit_transform(data.reshape(-1, 1)).flatten()
        self.scalers["last_used"] = scaler

        return standardized.astype(np.float32)


def recommend_preprocessing_strategy(data: ArrayLike) -> Dict[str, Any]:
    """Recommend optimal preprocessing strategy based on data characteristics."""

    validator = DataValidator()
    validation_results = validator.validate_data(data)

    recommendations = {
        "standardize": True,
        "handle_outliers": False,
        "outlier_method": "iqr",
        "transformation": None,
        "rationale": [],
    }

    if not validation_results["valid"]:
        recommendations["rationale"].append("Data validation failed - fix errors first")
        return recommendations

    stats = validation_results["statistics"]

    # Outlier handling recommendation
    if stats.get("outlier_percentage_iqr", 0) > 5:
        recommendations["handle_outliers"] = True
        if isinstance(recommendations["rationale"], list):
            recommendations["rationale"].append("High outlier rate detected")

        if stats.get("outlier_percentage_iqr", 0) > 15:
            recommendations["outlier_method"] = "isolation_forest"
            if isinstance(recommendations["rationale"], list):
                recommendations["rationale"].append(
                    "Very high outlier rate - using isolation forest"
                )

    # Transformation recommendation
    if abs(stats.get("skewness", 0)) > 1:
        if stats.get("min", 0) > 0:
            recommendations["transformation"] = "log"
            if isinstance(recommendations["rationale"], list):
                recommendations["rationale"].append(
                    "High skewness with positive values - "
                    "log transformation recommended"
                )
        else:
            recommendations["transformation"] = "sqrt"
            if isinstance(recommendations["rationale"], list):
                recommendations["rationale"].append(
                    "High skewness - sqrt transformation recommended"
                )

    # Scale recommendation
    if stats.get("coefficient_of_variation", 0) > 2:
        if isinstance(recommendations["rationale"], list):
            recommendations["rationale"].append(
                "High variability - standardization strongly recommended"
            )
    elif stats.get("range", 0) > 1000:
        if isinstance(recommendations["rationale"], list):
            recommendations["rationale"].append(
                "Large data range - standardization recommended"
            )

    return recommendations


def validate_and_preprocess(
    data: ArrayLike, auto_preprocess: bool = True, **preprocessing_kwargs
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """Combined validation and preprocessing pipeline."""

    # Validate data
    validator = DataValidator()
    validation_results = validator.validate_data(data)

    if not validation_results["valid"]:
        raise ValueError(f"Data validation failed: {validation_results['errors']}")

    # Get preprocessing recommendations
    if auto_preprocess:
        rec = recommend_preprocessing_strategy(data)
        # Use recommendations as defaults, but allow override
        for key, value in rec.items():
            if key != "rationale" and key not in preprocessing_kwargs:
                preprocessing_kwargs[key] = value

    # Preprocess data
    preprocessor = DataPreprocessor()
    processed_data, preprocessing_info = preprocessor.preprocess_data(
        data, **preprocessing_kwargs
    )

    # Combine results
    combined_info = {
        "validation": validation_results,
        "preprocessing": preprocessing_info,
        "recommendations_used": auto_preprocess,
    }

    return processed_data, combined_info


if __name__ == "__main__":
    # Example usage
    np.random.seed(42)

    # Generate test data with outliers and skewness
    normal_data = np.random.normal(50, 10, 900)
    outliers = np.random.normal(200, 5, 100)
    test_data = np.concatenate([normal_data, outliers])

    print("Data Validation and Preprocessing Example")
    print("=" * 50)

    # Validate
    validator = DataValidator()
    results = validator.validate_data(test_data)

    print("Validation Results:")
    print(f"Valid: {results['valid']}")
    print(f"Warnings: {len(results['warnings'])}")
    print(f"Recommendations: {len(results['recommendations'])}")

    for warning in results["warnings"]:
        print(f"  WARNING: {warning}")

    for rec in results["recommendations"]:
        print(f"  {rec}")

    # Preprocess
    print("\nPreprocessing...")
    processed_data, info = validate_and_preprocess(test_data)

    print(f"Original size: {info['preprocessing']['original_stats']['size']}")
    print(f"Final size: {info['preprocessing']['final_stats']['size']}")
    print(f"Outliers removed: {info['preprocessing']['outliers_removed']}")
    print(f"Steps applied: {info['preprocessing']['steps_applied']}")

    print("\n[SUCCESS] Validation and preprocessing complete!")
