# XDT (Exact Decision Tree) ğŸš€

[![PyPI version](https://badge.fury.io/py/xdt-classifier.svg)](https://badge.fury.io/py/xdt-classifier)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/release/python-380/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A high-performance decision tree classifier with exact split optimization, histogram-based candidate selection, and multi-class support. XDT combines the speed of histogram-based methods with the accuracy of exact split computation.

## ğŸ¯ Key Features

- **Exact Split Computation**: Guaranteed optimal splits with O(n) bucket optimization for integer-like features
- **Histogram-Based Candidate Selection**: Fast feature evaluation using quantile-based binning
- **Multi-Class Support**: Native support for multi-class classification with Gini impurity
- **Numba Acceleration**: JIT-compiled prediction with optional parallel processing
- **Memory Efficient**: BFS tree layout for optimal cache performance
- **Adaptive Algorithms**: Automatically chooses between O(n) bucket sort and O(n log n) sorting based on data characteristics

## ğŸš€ Performance

XDT delivers superior performance through:
- **Exact splits**: No approximation errors from histogram binning
- **Smart candidate selection**: Variance bounds to focus on promising features
- **Optimized algorithms**: O(n) bucket sort for integer-like features, O(n log n) for continuous
- **Parallel prediction**: Multi-threaded inference for large datasets
- **Memory locality**: BFS tree traversal for better cache performance

## ğŸ“¦ Installation

```bash
pip install xdtclassifier
```

## ğŸ”§ Quick Start

### Basic Usage

```python
from xdt import XDTClassifier
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate sample data
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train classifier
clf = XDTClassifier()

clf.fit(X_train, y_train)

# Make predictions
predictions = clf.predict(X_test)
probabilities = clf.predict_proba(X_test)

print(f"Accuracy: {(predictions == y_test).mean():.3f}")
```

### Advanced Configuration

```python
# Binary classification optimized settings
binary_clf = XDTClassifier(
    max_depth=10,
    min_samples_split=20,
    n_bins=192,
    min_gain_threshold=1e-8,
    max_exact_refinements_binary=96,
    use_parallel_prediction=True,
    parallel_threshold=256
)

# Multi-class optimized settings  
multiclass_clf = XDTClassifier(
    max_depth=14,
    min_samples_split=10,
    n_bins=256,
    min_gain_threshold=1e-9,
    max_exact_refinements=16,
    use_parallel_prediction=True,
    parallel_threshold=1000
)
```

### Algorithm Statistics

```python
# View detailed algorithm statistics
clf.print_algorithm_stats()
```

Output:
```
ğŸš€ XDT MULTI-CLASS STATISTICS (Total Splits: 127)
   Classes: 3 ([0 1 2])
================================================================================
âœ… Histogram-Based Splits: 89 (70.1%)
âœ… Exact refinements evaluated: 234 (184.3%)
ğŸš€ Exact Splits: 38 (29.9%)
âœ… Avg Candidates/Split: 12.4

ğŸš€ OPTIMIZATION METHOD BREAKDOWN:
  - O(n) Bucket Sort: 28 (73.7%) - Integer-like features
  - O(n log n) Sorting: 10 (26.3%) - Continuous features

ğŸ† ALGORITHM COMPONENTS:
  âœ… Quantile-based binning (XDT core)
  âœ… Histogram-based candidate selection (XDT core)
  ğŸš€ Exact split computation
  âœ… Multi-class variance bounds
  âœ… XDT

âš¡ PERFORMANCE: 73.7% O(n) optimization - Excellent for integer-like data
```

## ğŸ”§ Parameters

### Main Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `max_depth` | 10 | Maximum tree depth |
| `min_samples_split` | 20 | Minimum samples required to split |
| `min_samples_leaf` | 1 | Minimum samples required in a leaf |
| `n_bins` | 192 | Number of histogram bins for feature discretization |
| `min_gain_threshold` | 1e-8 | Minimum information gain required for splitting |
| `random_state` | 42 | Random seed for reproducibility |

### Performance Parameters  

| Parameter | Default | Description |
|-----------|---------|-------------|
| `use_parallel_prediction` | True | Enable parallel prediction |
| `parallel_threshold` | 256 | Minimum samples for parallel prediction |
| `max_exact_refinements` | 16 | Max exact refinements per node (multi-class) |
| `max_exact_refinements_binary` | 96 | Max exact refinements per node (binary) |
| `map_labels` | True | Map internal IDs back to original labels |

## ğŸ—ï¸ Algorithm Details

### Two-Phase Split Selection

1. **Phase 1 - Histogram Evaluation**: Fast candidate screening using quantile-based bins
2. **Phase 2 - Exact Refinement**: Exact split computation on top candidates

### Optimization Strategies

- **Integer-like Features**: O(n) bucket sort for discrete values
- **Continuous Features**: O(n log n) sorting for exact splits  
- **Variance Bounds**: Early pruning of unpromising features
- **Adaptive Thresholds**: Binary vs multi-class specific tuning

### Memory Layout

- **BFS Tree Storage**: Better cache locality during prediction
- **Contiguous Arrays**: Optimized for Numba JIT compilation
- **Buffer Reuse**: Minimize memory allocations during training

## ğŸ“Š Benchmarks

XDT consistently outperforms standard decision trees:

- **Accuracy**: Higher due to exact split computation
- **Speed**: Competitive training, faster prediction via Numba
- **Memory**: Efficient BFS layout reduces cache misses
- **Scalability**: Parallel prediction for large datasets

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **PyPI**: https://pypi.org/project/xdtclassifier/
- **GitHub**: https://github.com/nqmn/xdt
- **Documentation**: https://github.com/nqmn/xdt#readme
- **Issues**: https://github.com/nqmn/xdt/issues

## ğŸ“ˆ Citation

If you use XDT in your research, please cite:

```bibtex
@software{xdt_classifier,
  title={XDT: Exact Decision Tree Classifier},
  author={mohdadil},
  url={https://github.com/nqmn/xdt},
  version={1.0.0},
  year={2025}
}
```