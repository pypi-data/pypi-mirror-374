{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Event Inundation Mapping\n",
    "\n",
    "This notebook uses the 2018 Labor Day flood event at the Wildcat Creek (near Manhattan, KS) as an example to demonstrate how to map a flood event with a tiled FLDPLN library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# import DASK libraries for parallel mapping\n",
    "from dask.distributed import Client, LocalCluster\n",
    "from dask import visualize\n",
    "\n",
    "# import the mapping and gauge modules from the fldpln package\n",
    "from fldpln.mapping import *\n",
    "from fldpln.gauge import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Input Tiled Library and Output Folders\n",
    "\n",
    "Here we setup the folder under which tiled libraries (organized as sub-folders) are located. We also setup the output folder under which a map sub-folder and a 'scratch' sub-folder will be created. The map sub-folder, which is specified later, contains all inundation depth maps. The scratch folder stores temporary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiled library folder\n",
    "libFolder =  'E:/fldpln/sites/wildcat_10m_3dep' # wildcat\n",
    "\n",
    "# libraries to be mapped, i.e., gauge stages will be snapped to the FSPs in those libraries.\n",
    "libs2Map = ['tiledlib']\n",
    "\n",
    "# Set output folder\n",
    "outputFolder = 'E:/fldpln/sites/wildcat_10m_3dep/maps'\n",
    "# outputFolder = 'E:/fldpln/sites/verdigris_10m/maps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Gauge Stage and Calculate Gauge Depth of Flow (DOF)\n",
    "\n",
    "Here we obtain and process flood event stages recorded by stream gauges. The stage at a gauge typically (at least for USGS gauges) refers to difference from the gauge's datum, which is based on a certain vertical datum and is not necessary of the stream bed elevation. \n",
    "\n",
    "The stage in FLDPLN library, called Depth of Flow (DOF), refers to the height of water level from stream bed. In order to use the gauge stage in a FLDPLN library, we need to:\n",
    "* Convert gauge stage to gauge stage elevation (i.e., water surface = gauge datum + stage) and make sure that gauge stage elevation and FLDPLN filled stream bed elevation are in the same vertical datum. \n",
    "* Calculate the depth of flow (DOF) at a FSP as the difference between the two elevation (DOF = gauge stage elevation - FSP bed elevation). \n",
    "\n",
    "The Wildcat Creek DEM and FLDPLN library are based on the NAVD88 vertical datum. So gauge stage elevations need to be based on the vertical datum too to calculate the DOFs at those gauges. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauge Stage from AHPS and USGS\n",
    "\n",
    "In the United States, both USGS (through its [National Water Information System](https://waterdata.usgs.gov/nwis)) and NOAA/NWS (through its [National Water Prediction Service](https://water.noaa.gov), formerly called Advanced Hydrologic Prediction Services (AHPS)) maintain stream gauge records of past flood stages. \n",
    "\n",
    "There are three AHPS and USGS gauges ([WKCK1](https://water.noaa.gov/gauges/wkck1), [MWCK1](https://water.noaa.gov/gauges/MWCK1), [MSTK1](https://water.noaa.gov/gauges/MSTK1)) located on the Wildcat Creek that record the 2018 Labor Day flood event. Here we use the maximum flood event stage at those gauges to map the maximum inundation extent and depth of the event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flood Event Stage from AHPS Historic Crests\n",
    "\n",
    "The flood stage for the 2018 Labor Day flood event in 2018 are available on [National Water Prediction Service](https://water.noaa.gov) as the historic crests at those gauges ([WKCK1](https://water.noaa.gov/gauges/wkck1), [MWCK1](https://water.noaa.gov/gauges/MWCK1), [MSTK1](https://water.noaa.gov/gauges/MSTK1)). \n",
    "\n",
    "An Excel file, wildcat_gauges_albers_meters.xlsx, has been prepared after all the USGS and NOAA/NWS gauges available in Kansas are retrieved and cleaned up. There are several sheets in the file which store both gauge information (for example, gauge datum) and the event stages with different gauge combinations. The key fields needed for flood inundation mapping from those gauges are: stationid, x, y, and stage_elevation.\n",
    "\n",
    "Note that most USGS and AHPS gauge locations are in geographic coordinates (i.e., longitudes and latitudes) and their stages are measured in feet. So we need to make sure to convert gauge coordinates into the spatial reference used in the FLDPLN library and to convert gauge stages into the vertical unit of the library too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildcat Creek gauges\n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx' # KS LiDAR DEM in UTM with vertical unit in feet\n",
    "gaugeStageFile = 'E:/fldpln/sites/wildcat_10m_3dep/wildcat_gauges_albers_meters.xlsx' # 3DEP DEM in Albers with vertical unit in meters\n",
    "sheetName = 'ThreeGauges' # all 3 gauges\n",
    "# sheetName = 'TwoDsGauges' # 2 downstream gauges\n",
    "# sheetName = 'MSTK1' # the last downstream gauge used in HEC-RAS model\n",
    "\n",
    "# # Verdigris gauges\n",
    "# gaugeStageFileName = 'VerdigrisGauges.xlsx' # CFVK1 is not a USGS gauge and its stage is from AHPS historic crests\n",
    "# sheetName = '2019Flood' # gauges used for 2019 flood \n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFile, sheet_name=sheetName) \n",
    "# print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Event Stage from USGS NWIS\n",
    "\n",
    "We can also get event maximum stage directly from USGS NWIS to check and verify the historic crests obtained from NOAA/NWS AHPS. Note that those stages are in feet and we need to convert them to stage elevation before using it in mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wildcat Creek 3 USGS gauges (in the order from upstream to downstream)\n",
    "usgsIds = ['06879805','06879810','06879815'] # USGS IDs for Wildcat Creek gauges\n",
    "ahpsIds = ['WKCK1','MWCK1','MSTK1'] # AHPS IDs for Wildcat Creek gauges\n",
    "\n",
    "# A period between two dates: Wildcat Creek Sep.3 2018 flood event\n",
    "instStages = GetUsgsGaugeStageFromWebService(usgsIds,startDate='2018-09-02',endDate='2018-09-04')\n",
    "print(instStages)\n",
    "\n",
    "# find the max stage within the time period\n",
    "maxStages = instStages.groupby(['stationid'],as_index=False).agg({'stage_ft':'max'})\n",
    "# find the most recent time with the max stage\n",
    "tdf = pd.merge(instStages, maxStages, how='inner', on=['stationid','stage_ft'])\n",
    "gaugeStagesFromNwis = tdf.groupby(['stationid'], as_index=False).agg({'stationid':'first','stage_ft':'first','stage_time':'max'})\n",
    "print(gaugeStagesFromNwis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Gauge Stage from the National Water Model and HAND\n",
    "\n",
    "The HAND flood inundation mapping (FIM) method adopted by the NOAA/NWS National Water Center (NWC) uses the National Water Model (NWM) discharge and converts discharge into stage using the synthetic rating curve developed for each reach. The stage is then used to map flood inundation with the HAND approach.\n",
    "\n",
    "We show here how the HAND reach stage can be used to run FLDPLN for the event. Conceptually, we turn each reach into a synthetic gauge located at either the mid-point or the outlet of the reach. Selecting the reaches and locating the gauge was done by David Weiss (a graduate student) manually for the Wildcat Creek. Those synthetic gauges can be used just like the USGS/AHPS gauges. The key fields needed are: stationid, x, y, and stage_elevation.  Note that we assume the HAND reach stage elevation is in the same vertical datum, though not documented, as that of the FLDPLN library DEM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic FSP gauges from NWC reach stage\n",
    "# gaugeStageFileName = 'wildcat_gauges.xlsx'\n",
    "# sheetName = 'ReachStageAsDof' \n",
    "gaugeStageFileName = 'wildcat_gauges_albers_meters.xlsx'\n",
    "# sheetName = 'ReachMedianStage' # HAND reach median stage as DOF\n",
    "sheetName = 'ReachOutletStage' # HAND reach outlet stage as DOF\n",
    "\n",
    "# read gauge file\n",
    "gaugeStages = pd.read_excel(gaugeStageFileName, sheet_name=sheetName) # 3 gauges\n",
    "print(gaugeStages)\n",
    "\n",
    "# Need to calculate gauge stage elevation if necessary!\n",
    "\n",
    "# keep only necessary fields from gauges\n",
    "keptFields = ['stationid','x','y','stage_elevation']\n",
    "gaugeWithStageElevations = gaugeStages[keptFields]\n",
    "print(gaugeWithStageElevations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap Gauges to FSPs and Calculate Gauge DOF\n",
    "\n",
    "Here we snap the gauges to flood source pixels (FSPs), which are the stream pixels. We calculated the depth of flow/flood (DOF) at each snapped FSP as the difference between stage elevation and the stream bed elevation at the FSP. \n",
    "\n",
    "This process also identifies the FLDPLN libraries that the gauges are snapped to. This is necessary as the same gauge may be snapped to more than one library as the libraries may overlap and the FSPs in the overlay may have different locations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snap gauges to FSPs on-the-fly\n",
    "print('Snap gauges to FSPs ...')\n",
    "print(f'Number of gauges: {len(gaugeWithStageElevations.index)}')\n",
    "\n",
    "# snap the gauges to the FSPs in the selected libraries. Note that libraries may overlap in space and the same gauge may be snapped to multiple FSPs in different libraries.\n",
    "# A snapped FSP is identified by 'lib_name' and FSP's ID together.\n",
    "# Fields 'StrOrd','DsDist','SegId','FilledElev'are used for interpolating other FSP DOF\n",
    "# Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP (as there are overlapping FSPs between libraries)!\n",
    "gaugeFspDf = SnapGauges2Fsps(libFolder,libs2Map,gaugeWithStageElevations,snapDist=350,gaugeXField='x',gaugeYField='y',fspColumns=['FspX','FspY','StrOrd','DsDist','SegId','FilledElev']) \n",
    "print(gaugeFspDf)\n",
    "\n",
    "# calculate gauge FSP's DOF\n",
    "gaugeFspDf['Dof'] = gaugeFspDf['stage_elevation'] - gaugeFspDf['FilledElev']\n",
    "\n",
    "# keep only necessary columns for gauge FSPs\n",
    "gaugeFspDf = gaugeFspDf[['lib_name','FspX','FspY','StrOrd','DsDist','SegId','FilledElev','Dof']] # Note that 'lib_name','FspX', 'FspY' together uniquely identify a FSP!!!\n",
    "\n",
    "# show info\n",
    "print(f'Number of snapped gauge FSPs: {len(gaugeFspDf)}')\n",
    "# Find libs where the gauges are snapped to, and they are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "print(f'Libraries gauges snapped to: {libs2Map}')\n",
    "print(gaugeFspDf)\n",
    "\n",
    "#\n",
    "# save snapped gauges to CSV file for checking\n",
    "# gaugeFspDf.to_csv(os.path.join(outputFolder, 'SnappedGauges.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate FSP DOF Between Gauges\n",
    "\n",
    "Here we interpolate the DOF for the FSPs between any two gauge-snapped FSPs using their DOFs calculated in previous step. The interpolation uses stream orders and starts from low stream order (i.e., main streams) to high stream order (i.e., tributaries). Either horizontal or vertical (by default) interpolation can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find libs with snapped gauges. They are the actual libs to map\n",
    "libs2Map = gaugeFspDf['lib_name'].drop_duplicates().tolist()\n",
    "\n",
    "# prepare the DF for storing interpolated FSP DOF\n",
    "fspDof = pd.DataFrame(columns=['LibName','FspId','Dof'])\n",
    "\n",
    "# prepare DFs for saving interpolated FSPs and their segment IDs\n",
    "fspCols = fspInfoColumnNames + ['Dof']\n",
    "segIdCols = ['SegId','LibName']\n",
    "fsps = pd.DataFrame(columns=fspCols)\n",
    "segIds =pd.DataFrame(columns=segIdCols)\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # interpolate DOF for the gauges\n",
    "    # print('Interpolate FSP DOF using gauge DOF ...')\n",
    "    # fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf) # 'V' by default\n",
    "    fspIdDof = InterpolateFspDofFromGauge(libFolder,libName,gaugeFspDf,weightingType='H') # horizontal interpolation\n",
    "    fspIdDof['LibName'] = libName\n",
    "    # fspDof = fspDof.append(fspIdDof[['LibName','FspId','Dof']], ignore_index=True)\n",
    "    fspDof = pd.concat([fspDof,fspIdDof[['LibName','FspId','Dof']]], ignore_index=True)\n",
    "\n",
    "    # Keep interpolated FSP DOF for saving later\n",
    "    fspFile = os.path.join(libFolder, libName, fspInfoFileName)\n",
    "    fspDf = pd.read_csv(fspFile) \n",
    "    fspDf = pd.merge(fspDf,fspDof,how='inner',on=['FspId'])\n",
    "    # fsps = fsps.append(fspDf, ignore_index=True)\n",
    "    fsps = pd.concat([fsps,fspDf], ignore_index=True)\n",
    "    \n",
    "    # Keep FSP segment IDs for saving later\n",
    "    t =  pd.DataFrame(fspDf['SegId'].drop_duplicates().sort_values())\n",
    "    t['LibName'] = libName\n",
    "    # segIds = segIds.append(t, ignore_index=True)\n",
    "    segIds = pd.concat([segIds,t], ignore_index=True)\n",
    "\n",
    "# show interpolated FSPs with Dof\n",
    "print(fspDof)\n",
    "\n",
    "#\n",
    "# save interpolated FSP DOF and their segments for checking. This block of code should be commented out if no-checking needed\n",
    "#\n",
    "# Save DOF and segment IDs to CSV files\n",
    "FspDofFile = os.path.join(outputFolder, 'Interpolated_FSP_DOF.csv')\n",
    "SegIdFile = os.path.join(outputFolder, 'Interpolated_SegIds.csv')\n",
    "fsps.to_csv(FspDofFile, index=False)\n",
    "segIds.to_csv(SegIdFile, index=False)\n",
    "\n",
    "# # turn interpolated sgements into a shapefile\n",
    "# for libName in libs2Map:\n",
    "#     segShp = os.path.join(libFolder, libName, 'stream_orders.shp')\n",
    "#     segs = gpd.read_file(segShp)\n",
    "#     segs['LibName'] = libName\n",
    "#     # print(segs)\n",
    "#     # join by two fields: SegId and LibName\n",
    "#     segDf = pd.merge(segs,segIds,how='inner',on=['SegId','LibName'])\n",
    "#     # print(segDf)\n",
    "#     # write segments as a shapefile\n",
    "#     segDf.to_file(os.path.join(outputFolder, 'Interpolated_Segements.shp'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Flood Inundation Depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Mapping Parameters\n",
    "\n",
    "Setup the map folder which is under the output folder and contains all inundation depth maps. Additional settings include whether to mosaic tiles as single COG GeoTIFF file and whether using a Dask local cluster to speed up the mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up map folder\n",
    "outMapFolderName = 'wildcat_20180903' # Wildcat Labor Day flood\n",
    "\n",
    "# Create folders for storing temp and output map files\n",
    "outMapFolder, scratchFolder = CreateFolders(outputFolder,'scratch',outMapFolderName)\n",
    "\n",
    "# whether mosaci tiles as a single COG\n",
    "mosaicTiles = True #True #False\n",
    "\n",
    "# Using LocalCluster by default\n",
    "useLocalCluster = False # This doesn't work on my office desktop though it works fine on KBS server\n",
    "# numOfWorkers = round(0.8*os.cpu_count())\n",
    "# numOfWorkers = 6\n",
    "# print(f'Number of workers: {numOfWorkers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Flood Inundation Depth\n",
    "\n",
    "Here are create the flood depth map using the interpolated DOFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mapping info\n",
    "print(f'Tiled FLDPLN library folder: {libFolder}')\n",
    "print(f'Map folder: {outMapFolder}')\n",
    "# Find libs needs mapping\n",
    "libs2Map = fspDof['LibName'].drop_duplicates().tolist()\n",
    "print(f'Libraries to map: {libs2Map}')\n",
    "\n",
    "# check running time\n",
    "startTimeAllLibs = time.time()\n",
    "\n",
    "# create a local cluster to speed up the mapping. Must be run inside \"if __name__ == '__main__'\"!!!\n",
    "if useLocalCluster:\n",
    "    # cluster = LocalCluster(n_workers=4,processes=False)\n",
    "    try:\n",
    "        print('Start a LocalCluster ...')\n",
    "        # NOTE: set worker space (i.e., local_dir) to a folder that the LocalCluster can access. When run the script through a scheduled task, \n",
    "        # the system uses C:\\Windows\\system32 by default, which a typical user doesn't have the access!\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='32GB',local_dir=\"D:/projects_new/fldpln/tools\") # for KARS production server (192G RAM & 8 cores)\n",
    "        # cluster = LocalCluster(n_workers=numOfWorkers,processes=False) # for KARS production server (192G RAM & 8 cores)\n",
    "        cluster = LocalCluster(n_workers=numOfWorkers,memory_limit='8GB',local_directory=\"E:\\temp\") # for office desktop (64G RAM & 8 cores)\n",
    "        # print('Watch workers at: ',cluster.dashboard_link)\n",
    "        print(f'Number of workers: {numOfWorkers}')\n",
    "        client = Client(cluster)\n",
    "        # print scheduler info\n",
    "        # print(client.scheduler_info())\n",
    "    except:\n",
    "        print('Cannot create a LocalCLuster!')\n",
    "        useLocalCluster = False\n",
    "\n",
    "# dict to store lib processing time\n",
    "libTime={}\n",
    "\n",
    "# map each library\n",
    "for libName in libs2Map:\n",
    "    # check running time\n",
    "    startTime = time.time()\n",
    "    \n",
    "    # select the FSPs within the lib\n",
    "    fspIdDof = fspDof[fspDof['LibName']==libName][['FspId','Dof']]\n",
    "\n",
    "    # mapping flood depth\n",
    "    if useLocalCluster:\n",
    "        print(f'Map [{libName}] using LocalCLuster ...')\n",
    "        # generate a DAG\n",
    "        dag,dagRoot=MapFloodDepthWithTilesAsDag(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "        if dag is None:\n",
    "            tileTifs = None\n",
    "        else:\n",
    "            # visualize DAG\n",
    "            # visualize(dag)\n",
    "            # Compute DAG\n",
    "            tileTifs = client.get(dag, dagRoot)\n",
    "            if not tileTifs: # list is empty\n",
    "                tileTifs =  None\n",
    "    else:\n",
    "        print(f'Map {libName} ...')\n",
    "        tileTifs = MapFloodDepthWithTiles(libFolder,libName,'snappy',outMapFolder,fspIdDof,aoiExtent=None)\n",
    "    print(f'Actual mapped tiles: {tileTifs}')\n",
    "\n",
    "    # Mosaic all the tiles from a library into one tif file\n",
    "    if mosaicTiles and not(tileTifs is None):\n",
    "        print('Mosaic tile maps ...')\n",
    "        mosaicTifName = libName+'_'+outMapFolderName+'.tif'\n",
    "        # Simplest implementation, may crash with very large raster\n",
    "        MosaicGtifs(outMapFolder,tileTifs,mosaicTifName,keepTifs=False)\n",
    "    \n",
    "    # check time\n",
    "    endTime = time.time()\n",
    "    usedTime = round((endTime-startTime)/60,3)\n",
    "    libTime[libName] = usedTime\n",
    "    # print(f'{libName} processing time (minutes):', usedTime)\n",
    "\n",
    "# Show processing time\n",
    "# Individual lib processing time\n",
    "print('Individual library mapping time:', libTime)\n",
    "# total time\n",
    "endTimeAllLibs = time.time()\n",
    "print('Total processing time (minutes):', round((endTimeAllLibs-startTimeAllLibs)/60,3))\n",
    "\n",
    "#\n",
    "# Shutdown local clusters\n",
    "#\n",
    "if useLocalCluster:\n",
    "    print('Shutdown LocalCluster ...')\n",
    "    cluster.close()\n",
    "    client.shutdown()\n",
    "    client.close()\n",
    "    useLocalCluster = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Examine the Event Map\n",
    "\n",
    "Here we can visualize and exam the flood depth map generated above using GIS software."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
