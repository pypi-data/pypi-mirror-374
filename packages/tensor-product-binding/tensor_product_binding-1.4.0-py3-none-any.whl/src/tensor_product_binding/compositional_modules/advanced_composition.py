"""
ðŸ“‹ Advanced Composition
========================

ðŸŽ¯ ELI5 Summary:
This file is an important component in our AI research system! Like different organs 
in your body that work together to keep you healthy, this file has a specific job that 
helps the overall algorithm work correctly and efficiently.

ðŸ§ª Technical Details:
===================
Implementation details and technical specifications for this component.
Designed to work seamlessly within the research framework while
maintaining high performance and accuracy standards.

ðŸ“‹ Component Integration:
========================
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   This   â”‚
    â”‚Component â”‚ â†â†’ Other Components
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘â†“
    System Integration

"""
"""
ðŸŽ¯ Compositional Semantics - Composition Methods Module
======================================================


Author: Benedict Chen (benedict@benedictchen.com)
Based on: Smolensky (1990) "Tensor Product Variable Binding and the Representation of Symbolic Structures"

ðŸŽ¯ MODULE PURPOSE:
=================
Compositional semantic methods for complex linguistic structures.
Provides sentence composition, logical form processing, and semantic decomposition.

ðŸ”¬ RESEARCH FOUNDATION:
======================
Implements composition techniques based on Smolensky (1990):
- Frame-based composition for predicate-argument structures
- Sentence-level compositional processing with syntactic structure
- Logical form composition and interpretation
- Semantic decomposition and meaning vector analysis
- WordNet integration for large-scale semantic lexicons

This module contains composition methods for specialized compositional processing.
"""

import numpy as np
from typing import Dict, Any, List, Optional, Tuple, Set
import re

# Import TYPE_CHECKING to allow forward references without circular imports
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..tensor_product_binding import TensorProductBinding, TPRVector
    from ..symbolic_structures import SymbolicStructure, SymbolicStructureEncoder
    from .semantic_structures import SemanticConcept, SemanticType
    from .semantic_engine import CompositionalSemantics


class AdvancedCompositionMixin:
    """
    Advanced composition methods for CompositionalSemantics.
    
    This mixin provides complex compositional operations that extend
    the basic tensor product binding with linguistic structures.
    """
    
    def compose_predicate_argument(self, 
                                 predicate: str,
                                 argument: str,
                                 thematic_role: str = "THEME") -> np.ndarray:
        """
        Compose predicate with argument using thematic role binding
        
        Args:
            predicate: Predicate concept name
            argument: Argument concept name  
            thematic_role: Thematic role for binding
            
        Returns:
            Composed semantic representation
        """
        # Get vector representations
        pred_vector = self.encode_semantic_concept(predicate)
        arg_vector = self.encode_semantic_concept(argument)
        
        if pred_vector is None or arg_vector is None:
            raise ValueError(f"Cannot find vectors for {predicate} or {argument}")
        
        # Create semantic structure
        from ..symbolic_structures import SymbolicStructure
        structure = SymbolicStructure(
            name=f"{predicate}({argument})",
            bindings={
                "PREDICATE": pred_vector,
                thematic_role: arg_vector
            },
            structure_type="predicate_argument"
        )
        
        return self.structure_encoder.encode_structure(structure)
    
    def compose_with_frame(self,
                          frame_name: str,
                          role_bindings: Dict[str, str]) -> np.ndarray:
        """
        Compose meaning using a semantic frame
        
        Args:
            frame_name: Name of semantic frame
            role_bindings: Dictionary mapping roles to concept names
            
        Returns:
            Composed semantic representation
        """
        if frame_name not in self.semantic_frames:
            raise ValueError(f"Semantic frame {frame_name} not found")
        
        frame = self.semantic_frames[frame_name]
        
        # Check that core roles are satisfied
        for core_role in frame.core_roles:
            if core_role not in role_bindings:
                raise ValueError(f"Core role {core_role} not bound in frame {frame_name}")
        
        # Build bindings dictionary with vectors
        vector_bindings = {}
        
        for role, concept_name in role_bindings.items():
            concept_vector = self.encode_semantic_concept(concept_name)
            if concept_vector is None:
                raise ValueError(f"Cannot encode concept {concept_name}")
            vector_bindings[role] = concept_vector
        
        # Create and encode structure
        from ..symbolic_structures import SymbolicStructure
        structure = SymbolicStructure(
            name=f"{frame_name}({', '.join(f'{r}:{c}' for r,c in role_bindings.items())})",
            bindings=vector_bindings,
            structure_type="semantic_frame",
            metadata={"frame": frame_name}
        )
        
        composed_vector = self.structure_encoder.encode_structure(structure)
        
        # Store composed meaning
        self.composed_meanings[structure.name] = composed_vector
        
        return composed_vector
    
    def compose_sentence(self, words: List[str], 
                        syntax_structure: Optional[str] = None) -> np.ndarray:
        """
        Compose sentence meaning from word sequence
        
        Args:
            words: List of words in sentence
            syntax_structure: Optional syntactic structure specification
            
        Returns:
            Composed sentence meaning vector
        """
        if len(words) == 0:
            return np.zeros(self.vector_dim)
        
        if syntax_structure is not None:
            return self._compose_with_syntax(words, syntax_structure)
        else:
            return self._compose_left_to_right(words)
    
    def _compose_left_to_right(self, words: List[str]) -> np.ndarray:
        """Compose words left-to-right"""
        if not words:
            return np.zeros(self.vector_dim)
        
        # Get first word vector
        result_vector = self.encode_semantic_concept(words[0])
        if result_vector is None:
            result_vector = np.random.randn(self.vector_dim)
            result_vector = result_vector / np.linalg.norm(result_vector)
        
        # Compose with remaining words
        for word in words[1:]:
            word_vector = self.encode_semantic_concept(word)
            if word_vector is not None:
                # Use tensor product for systematic composition
                result_vector = np.outer(result_vector, word_vector).flatten()
                
                # Maintain dimensionality through compression
                if len(result_vector) != self.vector_dim:
                    # Use SVD to compress back to target dimension
                    U, s, Vt = np.linalg.svd(result_vector.reshape(-1, 1))
                    result_vector = U[:self.vector_dim, 0] * s[0]
        
        # Normalize result
        norm = np.linalg.norm(result_vector)
        return result_vector / norm if norm > 0 else result_vector
    
    def _compose_with_syntax(self, words: List[str], 
                           syntax_structure: str) -> np.ndarray:
        """
        Compose with syntactic structure guide
        
        Args:
            words: List of words
            syntax_structure: Structure like "((S VP) NP)" for tree composition
            
        Returns:
            Syntactically-guided composition result
        """
        # Simple implementation - assume left-branching for now
        # Full implementation would parse the syntax structure
        # and compose according to syntactic rules
        
        if syntax_structure.startswith("(") and syntax_structure.endswith(")"):
            # Parse structure and compose hierarchically
            # For now, use left-to-right with syntactic weights
            return self._compose_left_to_right(words)
        else:
            return self._compose_left_to_right(words)
    
    def compose_logical_form(self, logical_form: str) -> np.ndarray:
        """
        Compose logical form into semantic representation
        
        Args:
            logical_form: Logical form string like "loves(john, mary)"
            
        Returns:
            Composed semantic vector
        """
        from .utilities import parse_simple_logical_form
        predicate, arguments = parse_simple_logical_form(logical_form)
        
        if not arguments:
            # Propositional constant
            return self.encode_semantic_concept(predicate) or np.zeros(self.vector_dim)
        
        return self._compose_predicate_with_args(predicate, arguments)
    
    def _compose_predicate_with_args(self, predicate: str, arguments: List[str]) -> np.ndarray:
        """Compose predicate with its arguments"""
        pred_vector = self.encode_semantic_concept(predicate)
        if pred_vector is None:
            pred_vector = np.random.randn(self.vector_dim)
            pred_vector = pred_vector / np.linalg.norm(pred_vector)
        
        # Bind predicate with each argument using thematic roles
        thematic_roles = ["AGENT", "THEME", "GOAL", "SOURCE", "INSTRUMENT"]
        
        result_vector = pred_vector.copy()
        
        for i, arg in enumerate(arguments):
            arg_vector = self.encode_semantic_concept(arg)
            if arg_vector is not None:
                # Use appropriate thematic role
                role = thematic_roles[i] if i < len(thematic_roles) else f"ARG{i}"
                
                # Bind argument to role and compose with predicate
                bound_arg = np.outer(arg_vector, result_vector).flatten()
                
                # Compress if needed
                if len(bound_arg) > self.vector_dim:
                    bound_arg = bound_arg[:self.vector_dim]
                elif len(bound_arg) < self.vector_dim:
                    padded = np.zeros(self.vector_dim)
                    padded[:len(bound_arg)] = bound_arg
                    bound_arg = padded
                
                result_vector = bound_arg
        
        # Normalize
        norm = np.linalg.norm(result_vector)
        return result_vector / norm if norm > 0 else result_vector
    
    def decompose_meaning(self, meaning_vector: np.ndarray, 
                         possible_roles: List[str]) -> Dict[str, np.ndarray]:
        """
        Decompose meaning vector into role-filler components
        
        Args:
            meaning_vector: Composed meaning to decompose
            possible_roles: List of roles to try extracting
            
        Returns:
            Dictionary mapping roles to extracted fillers
        """
        decomposed = {}
        
        for role in possible_roles:
            role_vector = self.structure_encoder.get_role_vector(role)
            if role_vector is not None and hasattr(role_vector, 'data'):
                # Try to extract filler using pseudo-inverse unbinding
                try:
                    # Reshape meaning vector for matrix operations
                    if len(meaning_vector.shape) == 1:
                        meaning_matrix = meaning_vector.reshape(-1, len(role_vector.data))
                    else:
                        meaning_matrix = meaning_vector
                    
                    # Extract filler using matrix operations
                    role_norm = np.linalg.norm(role_vector.data)
                    if role_norm > 0:
                        normalized_role = role_vector.data / role_norm
                        extracted_filler = meaning_matrix @ normalized_role
                        decomposed[role] = extracted_filler
                except Exception:
                    # If extraction fails, skip this role
                    continue
        
        return decomposed
    
    def create_semantic_lexicon_from_wordnet(self, concepts: List[str]) -> Dict[str, "SemanticConcept"]:
        """
        Create semantic lexicon from WordNet concepts (heuristic-based implementation)
        
        Args:
            concepts: List of concept names to include
            
        Returns:
            Dictionary of semantic concepts with WordNet features
        """
        from .semantic_structures import SemanticConcept, SemanticType
        
        lexicon = {}
        
        for concept in concepts:
            # Simple heuristic-based type assignment
            if concept.endswith('_n') or concept in ['person', 'animal', 'object', 'thing']:
                concept_type = SemanticType.ENTITY
                features = {'concrete', 'countable'}
            elif concept.endswith('_v') or concept in ['run', 'walk', 'eat', 'love']:
                concept_type = SemanticType.PREDICATE
                features = {'action', 'dynamic'}
            elif concept.endswith('_a') or concept in ['red', 'big', 'happy']:
                concept_type = SemanticType.MODIFIER
                features = {'descriptive', 'attributive'}
            else:
                concept_type = SemanticType.ENTITY
                features = {'abstract'}
            
            # Create concept
            semantic_concept = SemanticConcept(
                name=concept,
                concept_type=concept_type,
                semantic_features=features
            )
            
            lexicon[concept] = semantic_concept
            
            # Add to conceptual space
            concept_vector = np.random.randn(self.concept_dim)
            concept_vector = concept_vector / np.linalg.norm(concept_vector)
            self.conceptual_space.add_concept(concept, concept_vector)
        
        return lexicon
    
    def get_semantic_field(self, concept: str, similarity_threshold: float = 0.7) -> List[str]:
        """
        Get semantic field (similar concepts) for a given concept
        
        Args:
            concept: Concept to find similar concepts for
            similarity_threshold: Minimum similarity for inclusion
            
        Returns:
            List of similar concept names
        """
        concept_vector = self.encode_semantic_concept(concept)
        if concept_vector is None:
            return []
        
        similar_concepts = []
        
        for other_concept in self.semantic_lexicon:
            if other_concept == concept:
                continue
                
            other_vector = self.encode_semantic_concept(other_concept)
            if other_vector is not None:
                similarity = self.similarity(concept_vector, other_vector)
                if similarity >= similarity_threshold:
                    similar_concepts.append(other_concept)
        
        # Sort by similarity (approximate)
        similar_concepts.sort(key=lambda c: self.similarity(
            concept_vector, 
            self.encode_semantic_concept(c) or np.zeros_like(concept_vector)
        ), reverse=True)
        
        return similar_concepts


# Export the advanced composition mixin
__all__ = ['AdvancedCompositionMixin']


if __name__ == "__main__":
    # Removed print spam: "...
    print("=" * 70)
    # Removed print spam: "...
    print("  â€¢ AdvancedCompositionMixin - Advanced composition methods")
    print("  â€¢ Frame-based composition for predicate-argument structures")
    print("  â€¢ Sentence-level compositional processing with syntax")
    print("  â€¢ Logical form composition and interpretation")
    print("  â€¢ Semantic decomposition and meaning vector analysis")
    print("  â€¢ WordNet integration for large-scale semantic lexicons")
    print("")
    # # Removed print spam: "...
    print("ðŸ”¬ Complex compositional semantics for structured representation!")