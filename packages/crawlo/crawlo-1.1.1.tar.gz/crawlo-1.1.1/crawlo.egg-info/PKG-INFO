Metadata-Version: 2.4
Name: crawlo
Version: 1.1.1
Summary: Crawlo æ˜¯ä¸€æ¬¾åŸºäºå¼‚æ­¥IOçš„é«˜æ€§èƒ½Pythonçˆ¬è™«æ¡†æ¶ï¼Œæ”¯æŒåˆ†å¸ƒå¼æŠ“å–ã€‚
Home-page: https://github.com/crawl-coder/Crawlo.git
Author: crawl-coder
Author-email: crawlo@qq.com
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: aiohttp>=3.12.14
Requires-Dist: aiomysql>=0.2.0
Requires-Dist: aioredis>=2.0.1
Requires-Dist: asyncmy>=0.2.10
Requires-Dist: cssselect>=1.2.0
Requires-Dist: dateparser>=1.2.2
Requires-Dist: httpx[http2]>=0.27.0
Requires-Dist: curl-cffi>=0.13.0
Requires-Dist: lxml>=5.2.1
Requires-Dist: motor>=3.7.0
Requires-Dist: parsel>=1.9.1
Requires-Dist: pydantic>=2.11.7
Requires-Dist: pymongo>=4.11
Requires-Dist: PyMySQL>=1.1.1
Requires-Dist: python-dateutil>=2.9.0.post0
Requires-Dist: redis>=6.2.0
Requires-Dist: requests>=2.32.4
Requires-Dist: six>=1.17.0
Requires-Dist: ujson>=5.9.0
Requires-Dist: urllib3>=2.5.0
Requires-Dist: w3lib>=2.1.2
Requires-Dist: rich>=14.1.0
Requires-Dist: astor>=0.8.1
Requires-Dist: watchdog>=6.0.0
Provides-Extra: render
Requires-Dist: webdriver-manager>=4.0.0; extra == "render"
Requires-Dist: playwright; extra == "render"
Requires-Dist: selenium>=3.141.0; extra == "render"
Provides-Extra: all
Requires-Dist: bitarray>=1.5.3; extra == "all"
Requires-Dist: PyExecJS>=1.5.1; extra == "all"
Requires-Dist: pymongo>=3.10.1; extra == "all"
Requires-Dist: redis-py-cluster>=2.1.0; extra == "all"
Requires-Dist: webdriver-manager>=4.0.0; extra == "all"
Requires-Dist: playwright; extra == "all"
Requires-Dist: selenium>=3.141.0; extra == "all"

# ğŸ•·ï¸ Crawlo - è½»é‡çº§å¼‚æ­¥çˆ¬è™«æ¡†æ¶

> ä¸€ä¸ªç®€æ´ã€æ˜“ç”¨ã€å¯æ‰©å±•çš„ Python å¼‚æ­¥çˆ¬è™«æ¡†æ¶ï¼Œçµæ„Ÿæºè‡ª Scrapyï¼Œä½†æ›´è½»é‡ã€æ›´æ˜“ä¸Šæ‰‹ã€‚

ğŸš€ æ”¯æŒå‘½ä»¤è¡Œæ“ä½œã€çˆ¬è™«ç”Ÿæˆã€åˆè§„æ£€æŸ¥ã€è¿è¡Œç›‘æ§ä¸ç»Ÿè®¡åˆ†æï¼Œé€‚åˆå¿«é€Ÿå¼€å‘ä¸­å°å‹çˆ¬è™«é¡¹ç›®ã€‚

---

## ğŸ“¦ ç‰¹æ€§

- âœ… **å‘½ä»¤è¡Œé©±åŠ¨**ï¼š`crawlo startproject`, `crawlo genspider` ç­‰
- âœ… **è‡ªåŠ¨å‘ç°çˆ¬è™«**ï¼šæ— éœ€æ‰‹åŠ¨æ³¨å†Œï¼Œè‡ªåŠ¨åŠ è½½ `spiders/` æ¨¡å—
- âœ… **å¼‚æ­¥æ ¸å¿ƒ**ï¼šåŸºäº `asyncio` å®ç°é«˜å¹¶å‘æŠ“å–
- âœ… **çµæ´»é…ç½®**ï¼šé€šè¿‡ `crawlo.cfg` å’Œ `settings.py` ç®¡ç†é¡¹ç›®
- âœ… **çˆ¬è™«æ£€æŸ¥**ï¼š`crawlo check` éªŒè¯çˆ¬è™«å®šä¹‰æ˜¯å¦åˆè§„
- âœ… **è¿è¡Œç»Ÿè®¡**ï¼š`crawlo stats` æŸ¥çœ‹å†å²è¿è¡ŒæŒ‡æ ‡ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰
- âœ… **æ‰¹é‡è¿è¡Œ**ï¼šæ”¯æŒ `crawlo run all` å¯åŠ¨æ‰€æœ‰çˆ¬è™«
- âœ… **æ—¥å¿—ä¸è°ƒè¯•**ï¼šç»“æ„åŒ–æ—¥å¿—è¾“å‡ºï¼Œä¾¿äºæ’æŸ¥é—®é¢˜

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… Crawlo

```bash
pip install crawlo
```

> âš ï¸ å½“å‰ä¸ºå¼€å‘é˜¶æ®µï¼Œå»ºè®®ä½¿ç”¨æºç å®‰è£…ï¼š
>
> ```bash
> git clone https://github.com/yourname/crawlo.git
> pip install -e crawlo
> ```

### 2. åˆ›å»ºé¡¹ç›®

```bash
crawlo startproject myproject
cd myproject
```

ç”Ÿæˆé¡¹ç›®ç»“æ„ï¼š

```
myproject/
â”œâ”€â”€ crawlo.cfg
â”œâ”€â”€ myproject/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ (ä½ çš„çˆ¬è™«å°†åœ¨è¿™é‡Œ)
```

### 3. ç”Ÿæˆçˆ¬è™«

```bash
crawlo genspider example example.com
```

ç”Ÿæˆ `spiders/example.py`ï¼š

```python
class ExampleSpider(Spider):
    name = "example"
    start_urls = ["https://example.com"]
    
    def parse(self, response):
        # è§£æé€»è¾‘
        pass
```

### 4. æ£€æŸ¥çˆ¬è™«åˆè§„æ€§

```bash
crawlo check
```

è¾“å‡ºç¤ºä¾‹ï¼š

```
ğŸ” Checking 1 spider(s)...
âœ… example              ExampleSpider (OK)
ğŸ‰ All spiders are compliant!
```

### 5. è¿è¡Œçˆ¬è™«

```bash
# è¿è¡Œå•ä¸ªçˆ¬è™«
crawlo run example

# è¿è¡Œæ‰€æœ‰çˆ¬è™«
crawlo run all
```

### 6. æŸ¥çœ‹è¿è¡Œç»Ÿè®¡

```bash
crawlo stats
```

æŸ¥çœ‹æœ€è¿‘ä¸€æ¬¡è¿è¡Œçš„è¯·æ±‚ã€å“åº”ã€é¡¹ç›®æ•°ç­‰æŒ‡æ ‡ï¼š

```
ğŸ“Š Recent Spider Statistics (last run):
ğŸ•·ï¸  example
    downloader/request_count           1
    item_scraped_count                 1
    log_count/INFO                     7
```

---

## ğŸ› ï¸ å‘½ä»¤åˆ—è¡¨

| å‘½ä»¤ | è¯´æ˜ |
|------|------|
| `crawlo startproject <name>` | åˆ›å»ºæ–°é¡¹ç›® |
| `crawlo genspider <name> <domain>` | ç”Ÿæˆçˆ¬è™«æ¨¡æ¿ |
| `crawlo list` | åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„çˆ¬è™« |
| `crawlo check` | æ£€æŸ¥çˆ¬è™«å®šä¹‰æ˜¯å¦åˆè§„ |
| `crawlo run <spider_name>` | è¿è¡ŒæŒ‡å®šçˆ¬è™« |
| `crawlo run all` | è¿è¡Œæ‰€æœ‰çˆ¬è™« |
| `crawlo stats` | æŸ¥çœ‹æœ€è¿‘è¿è¡Œçš„ç»Ÿè®¡ä¿¡æ¯ |
| `crawlo stats <spider_name>` | æŸ¥çœ‹æŒ‡å®šçˆ¬è™«çš„ç»Ÿè®¡ |

---

## ğŸ“ é¡¹ç›®ç»“æ„è¯´æ˜

```ini
# crawlo.cfg
[settings]
default = myproject.settings
```

```python
# settings.py
BOT_NAME = "myproject"
LOG_LEVEL = "DEBUG"
CONCURRENT_REQUESTS = 3
DOWNLOAD_DELAY = 1.0
# å…¶ä»–é…ç½®...
```

---

## ğŸ“Š ç»Ÿè®¡æŒä¹…åŒ–

æ¯æ¬¡çˆ¬è™«è¿è¡Œç»“æŸåï¼Œç»Ÿè®¡ä¿¡æ¯ä¼šè‡ªåŠ¨ä¿å­˜åˆ°ï¼š

```
logs/stats/<spider_name>_YYYYMMDD_HHMMSS.json
```

å¯é€šè¿‡ `crawlo stats` å‘½ä»¤è¯»å–ï¼Œæ”¯æŒè·¨è¿›ç¨‹æŸ¥çœ‹ã€‚

---

## ğŸ§ª å¼€å‘è€…æç¤º

- ç¡®ä¿ `spiders/__init__.py` ä¸­å¯¼å…¥äº†ä½ çš„çˆ¬è™«ç±»ï¼Œå¦åˆ™æ— æ³•è¢«å‘ç°
- ä½¿ç”¨ `get_project_root()` è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•ï¼ˆé€šè¿‡æŸ¥æ‰¾ `crawlo.cfg`ï¼‰
- æ‰€æœ‰å‘½ä»¤è¡Œå·¥å…·å‡æ”¯æŒç›´æ¥è¿è¡Œï¼š`python -m crawlo.commands.list`

---

